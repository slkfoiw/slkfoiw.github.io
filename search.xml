<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2025/02/17/lun-wen-yue-du/visual-prompting-for-gfss-a-multi-scale-approach-lun-wen-bi-ji/"/>
      <url>/2025/02/17/lun-wen-yue-du/visual-prompting-for-gfss-a-multi-scale-approach-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Visual-Prompting-for-Generalized-Few-shot-Segmentation-A-Multi-scale-Approach"><a href="#Visual-Prompting-for-Generalized-Few-shot-Segmentation-A-Multi-scale-Approach" class="headerlink" title="Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach"></a>Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach</h1><blockquote><p>广义少样本学习，少样本学习的一种更现实的变体引入，允许从基类或新类中查询图像。</p><p>广义少样本分割（GFSS）任务，即利用未标记的目标图像来改进新类的分割，同时通过知识蒸馏保持基类的性能。</p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在自然语言处理和视觉语言领域中使用的基础模型，是基于广泛的数据集训练得到的，具有很强的泛化性，并在多种下游任务上表现出色。这些基础模型已经通过不同的提示技术调整，以便使用在少样本领域。然而，像定位（localization）、提示（prompting）这些在密集预测和语义分割任务上有助于少样本展示的技术，仍然有待探索。</p><blockquote><p>少样本语义分割：目的是用很少的标记训练例子来分割新的（看不见的）类。</p></blockquote><p>挑战：</p><ol><li>广义少样本分割（GFSS）任务的目标是在基类和新类上都表现良好，比只关注新类上的表现更具挑战。</li><li>新类上的提示学习具有挑战。必须确保从少数样本中学习到的新提示与基提示有足够的区别，以避免新基类的错误分类。</li></ol><p>为了解决上述挑战，我们开发了一种简单而高效的transformer解码器视觉提示，用于多尺度的密集预测，它依赖于novel-to-base因果注意，而不需要元训练。我们将detr风格的架构中的查询视为一种视觉提示的形式，并设计了一种机制来初始化和学习新的提示。然后，novel-to-base的<strong>因果</strong>注意允许base提示影响novel提示表征，而不能反过来（这里的因果指的是单向性）。直观地说，这允许novel提示被排斥和/或吸引到它们的base对应物上。这种注意力是在不同尺度（转换器的层）之间共享的，因此能更稳健地学习和提高性能。提示的多尺度细化有助于在多个尺度上进行图像特征的交互和推理，从而有助于更好的密集预测。最后，我们在一个可转换设置中扩展了这种架构，在其中，novel提示和base提示都可以在测试时对无监督目标进行微调，以进一步提高性能。所提出的架构和方法如图1所示</p><p><img src="D:\mynotes\source\images\deepLearning\VP-for-GFSS1.png" alt="image-20250217104404818"></p><p>主要贡献：</p><ul><li>为GFSS设计了一个多尺度的视觉提示transformer解码器架构，具有可学习的提示，允许为novel类创建novel提示，通过support image（及其掩码）的掩码平均池化（masked average pooling）进行初始化。</li><li>在这个架构中，提出并学习了一个在novel提示和base提示之间的<strong>多尺度的（共享的）novel-to-base的交叉注意机制</strong>。</li><li>提出了一种新的<strong>转换</strong>提示调优（Transductive Prompt-tuning），它允许视觉提示在测试的（未标记的）图像上进行调优（即术语中转换的含义）。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="D:\mynotes\source\images\deepLearning\VP-for-GFSS2.png" alt="image-20250217120808951"></p><h3 id="1-视觉提示多尺度Transformer"><a href="#1-视觉提示多尺度Transformer" class="headerlink" title="1.视觉提示多尺度Transformer"></a>1.视觉提示多尺度Transformer</h3><h4 id="base视觉提示"><a href="#base视觉提示" class="headerlink" title="base视觉提示"></a>base视觉提示</h4><p>假设有$B$个基类，定义$B$个视觉提示分别代表每个基类，并随机初始化它们。这些视觉提示首先通过自我注意学习自身之间的关系，然后用来提示多个尺度的图像特征。在每个解码器层base提示的细化公式如下：<br>$$<br>V_B^{(l)} = \mathcal{A}( V_B^{(l-1)} ) + \mathcal{C}(V_B^{(l-1)}, F^{(l-1)})<br>$$<br>$\mathcal{A}$代表自注意力，$\mathcal{C}$代表交叉注意力，$V_B^{(l)} \in \mathbb{R}^{B \times C}$是第$l$层的base视觉提示，$F^{(l)} \in \mathbb{R}^{H_l W_l \times C_l}$是第$l$层的扁平（flattened）图像特征。</p><p>通过多层transformer注意细化提示后，定义一个分割头，表示为$\mathcal{H}(\cdot)$。这个分割头将最终改进的提示$V_B^{(L)}$和最高分辨率的图像特征$F^{(L)}$作为输入，生成每像素每类的预测。(具体见3)</p><h4 id="novel视觉提示"><a href="#novel视觉提示" class="headerlink" title="novel视觉提示"></a>novel视觉提示</h4><p>在对基类进行训练之后，冻结模型除了表示基类的视觉提示以外的所有层。假设有$N$个新类，我们添加$N$个novel视觉提示。进一步添加了一个因果单向注意模块，允许novel提示通过base提示增强novel提示。输出被传递给一个分割头，以对新的类进行预测。</p><p>novel视觉提示首先通过一个单向的novel-to-base因果注意模块（表示为$\mathcal{CA}$）进行细化，然后将它和base提示连接起来。总的细化公式如下：</p>$$V_N^{(l)} = \mathcal{CA}(V_B^{(l)}, V_N^{(l)}) \\V_A^{(l)} = [V_B^{(l)}, V_N^{(l)}] \\V_A^{(l)} = \mathcal{A}( V_A^{(l-1)} ) + \mathcal{C}(V_A^{(l-1)}, F^{(l-1)})$$N个novel视觉提示中的每一个都使用掩码平均全局池化进行初始化。{% raw %}$${V}_{n}^{\left( 0\right) } = \frac{1}{K}\mathop{\sum }\limits_{{k = 1}}^{K}\frac{\mathop{\sum }\limits_{{x,y}}{M}_{n}^{k}\left( {x,y}\right) {F}_{n}^{k}\left( {x,y}\right) }{\mathop{\sum }\limits_{{x,y}}{M}_{n}^{k}\left( {x,y}\right) },\forall n \in  N, (2)$${% endraw %}<p>这里，$V_n{(0)} \in \mathbb{R}^{1 \times C}$是新类n的初始novel视觉提示；$F_n^k$表示新类的第k次shot的图像特征；$M_n^k$表示对应的真实二值掩模；$x, y$是空间位置；$N$是新类的总数，$K$是shot的总数。</p><h3 id="2-novel-to-base因果注意"><a href="#2-novel-to-base因果注意" class="headerlink" title="2.novel-to-base因果注意"></a>2.novel-to-base因果注意</h3><p>直接学习新提示 或 有新提示影响基本提示 可能导致性能下降。</p><p>我们假设，novel视觉提示和base提示之间的因果注意可以帮助背景化新类嵌入，减少与base对应物的混淆。基于这种直觉，我们提出了一个novel-to-base因果注意层，它在每个尺度（也即transformer解码器的层）上重复；如图2。</p><p>该因果注意模块是一个单层交叉注意，其中解码器第$l$层的键$K^{(l)}$、值$V^{(l)}$和$Q^{(l)}$由如下公式得到：<br>$$<br>Q^{(l)} = V_N^{(l)}W^Q; K^{(l)} = V_B^{(l)}W^K; V^{(l)} = V_B^{(l)}W^V<br>$$<br>这里，$W^Q$、$W^K$、$W^V$分别为Q、K、V的权重矩阵。参数$W^Q$、$W^K$、$W^V$在该交叉注意模块的所有层上共享，以减少对新类的微调过程中可训练参数的数量，防止过拟合和不良泛化。</p><p>该单向novel-to-base因果注意通过如下公式改进每个解码器层$l$处的novel视觉提示$V_N^{(l)}$：<br>$$<br>V_N^{(l)} = softmax(Q^{(l)}(K^{(l)})^T)V^{(l)}<br>$$<br>于是就产生了一组情境化的novel视觉提示。</p><h3 id="3-分割头"><a href="#3-分割头" class="headerlink" title="3.分割头"></a>3.分割头</h3><h4 id="base分割头"><a href="#base分割头" class="headerlink" title="base分割头"></a>base分割头</h4><p>在该模型的基础训练中，base分割头$\mathcal{H}_B$相对简单，采用三层MLP的形式，将细化的提示有效地投射到类原型中；计算每个像素特征与该原型的（点积）相似度，然后沿着通道/类维度应用softmax来获得每像素每类的概率：</p>{% raw %}$${O}_{\text{base }} = {\mathcal{H}}_{B}\left( {{V}_{B}^{\left( L\right) },{F}^{\left( L\right) }}\right) \\ = \operatorname{MLP}\left( {V}_{B}^{\left( L\right) }\right)  \cdot  {\left\lbrack  {F}^{\left( L\right) }\right\rbrack  }^{\mathrm{T}} , (3)$${% endraw %}<p>这里，$O_{base} \in \mathbb{R}^{B \times H \times W}$，类分配可以通过在通道维度使用argmax实现。</p><h4 id="novel分割头"><a href="#novel分割头" class="headerlink" title="novel分割头"></a>novel分割头</h4><p>理论上可以在少样本推理时使用和公式3一样的方法，但是实践证明这并不有效。主要是由于在这个步骤中存在很少的样本和缺乏基类数据，因此对MLP进行微调会导致明显的过拟合。</p><p>因此，们诉诸于一种更简单形式的novel分割头$\mathcal{H}<em>N$，其中我们只学习在base训练中学习到的MLP的残差（保持固定）:<br>$$<br>O</em>{novel} = \mathcal{H}<em>N(V_N^{(L)}, F^{(L)}) \<br>= [MLP</em>{\textbullet}(V_N^{(L)}) + W_N] \cdot [F^{(L)}]^T<br>$$<br>这里，$_{\textbullet}$代表MLP的参数是冻结的，只有权重矩阵$W_N \in \mathbb{R}^{N \times C}$被学习。权重矩阵的初始化方法同novel提示一样（公式2）。</p><h3 id="转换提示调优"><a href="#转换提示调优" class="headerlink" title="转换提示调优"></a>转换提示调优</h3><p>最近提出的方法表明一个良好的转换设置对少样本密集预测任务的有效性。转换方法对每个未标记的测试图像进行测试时间优化，并利用测试实例的特定特征和信息来改进预测。</p><p>为了展示我们的方法的灵活性，在本节中，我们概述了调整我们的模型进行转换推理的过程，通过对视觉提示的转换微调来提高GFSS的性能。我们建立在DIaM [13]中提出的转换损耗的基础上，为了完整性，我们在这里描述它们：</p><blockquote><p>损失被设计用来最大化学习到的特征和对应测试图像的预测之间的交互信息，这是通过最大化$H(O) - H(O|I)$实现的。这里$I$、$O$分别是与像素和预测分布相关的随机变量；$H(O)$为边际熵，$H(O|I)$为条件熵。条件熵$H(O|I)$是由监督支持集的交叉熵损失和对于给定测试图像$I$的预测概率的熵之和给出。边际熵由KL散度损失给出，KL散度损失依赖于之前的估计区域比例。</p></blockquote><p>此外，与DIaM类似，我们使用知识蒸馏损失来保持基类的性能：<br>$$<br>\mathcal{L}<em>{KD} = KL(O</em>{base}^{new} || O_{base}^{old}), (4)<br>$$<br>这里，$O_{base}^{old}$是在base训练结束后，在冻结模型上基类的预测概率；$O_{base}^{new}$是使用了我们的附加组件（包括多尺度因果novel-to-base交叉注意和novel提示）来处理新类后，在模型上基类的预测概率。</p><p>整体的转换目标函数为：<br>$$<br>\mathcal{L}<em>{trans.} = \alpha H(O|I) - H(O) + \gamma \mathcal{L}</em>{KD}, (5)<br>$$<br>我们注意到第一次迭代的转换表现不佳，可能是由于边际分布的初始估计较差。为了解决这个问题，我们只对一组迭代次数应用每像素的交叉熵损失，然后合并等式5中剩余的转换损失，以实现一个更准确的边际分布估计。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/02/12/dl/janus/"/>
      <url>/2025/02/12/dl/janus/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n Janus python=3.10 -y</span><br><span class="line">conda activate Janus</span><br><span class="line">pip install -e .</span><br><span class="line">pip install gradio</span><br><span class="line"></span><br><span class="line">export HF_ENDPOINT="https://hf-mirror.com" //解决无法从hungface下载的问题</span><br><span class="line">export HF_HOME=/data/fzw/Janus/</span><br></pre></td></tr></tbody></table></figure><p>python demo/app_januspro.py</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/02/03/lun-wen-yue-du/zero-shot-rec-lun-wen-bi-ji/"/>
      <url>/2025/02/03/lun-wen-yue-du/zero-shot-rec-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Zero-shot-Referring-Expression-Comprehension-via-Structural-Similarity-Between-Images-and-Captions"><a href="#Zero-shot-Referring-Expression-Comprehension-via-Structural-Similarity-Between-Images-and-Captions" class="headerlink" title="Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions"></a>Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions</h1><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>通过图像和标题之间的结构相似性进行零样本参考表达理解</p><p>零镜头参考表达理解（zero-shot REC）的目的是根据提供的文本提示在对应的图像中定位边界框，这需要： </p><ol><li>复杂视觉场景和文本上下文的细粒度的分离</li><li>理解分离实体之间关系的能力</li></ol><p>不幸的是，现有的大型视觉语言对齐（VLA）模型，例如CLIP，在这两个方面都存在困难，因此不能直接用于此任务。为了缓解这一差距，我们利用大型基础模型将图像和文本以（主语、谓语、宾语）的格式分解为三元组。然后，用VLA模型计算视觉和文本三元组之间的结构相似矩阵，并将其传播到实例相似矩阵中。此外，为了使VLA模型具有关系理解的能力，我们设计了一个三重匹配目标，在包含丰富实体关系的管理数据集上对VLA模型进行微调。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="背景、研究现状"><a href="#背景、研究现状" class="headerlink" title="背景、研究现状"></a>背景、研究现状</h3><p>视觉定位（visual grounding）是跨计算机视觉和自然语言处理的一项基本任务，其目标是<strong>找到图像内容和文本描述之间的对应关系</strong>。然而，收集详细的接地注释来训练专业模型是很麻烦的。因此，<strong>零样本视觉定位</strong>是一个很有吸引力的替代方案。</p><p>参考表达理解（REC）作为一种视觉定位任务，其本质是<strong>将文本查询与相应的图像区域对齐</strong>。因此，在图像和标题中理解关系的能力十分重要。实体不仅是孤立的元素，而是与场景中的其他元素动态交互。在零样本学习的背景下，理解这些关系的任务会更困难，因为模型缺乏能够帮助理解的特定训练实例。</p><p>zero-shot REC的最新进展在很大程度上是由大规模视觉语言对齐（VLA）模型的集成驱动的，如CLIP和FLAVA，用它们作为连接文本和图像的桥梁。但是这些方法无法理解实体之间的关系。</p><h3 id="本文的解决方法"><a href="#本文的解决方法" class="headerlink" title="本文的解决方法"></a>本文的解决方法</h3><h4 id="一种新的零样本视觉定位模型"><a href="#一种新的零样本视觉定位模型" class="headerlink" title="一种新的零样本视觉定位模型"></a>一种新的零样本视觉定位模型</h4><p>在本文中，我们专注于REC任务，利用图像和字幕之间的结构相似性，明确地建模它们之间的实体关系，以解决零样本视觉定位问题。具体来说，我们将图像和标题分解为两组三元组，以（主语、谓词、对象）的形式出现，其中每个三元组捕获一对具有它们相互关系的潜在实体。通过综合考虑主语、宾语和谓语的相似性，我们可以找到更好的宾语建议及其引用的匹配。</p><p>与现有的工作相比，我们的方法更有原则，并消除了特殊的后处理空间关系解析器。更重要的是，为了提高标题中的关系理解，我们求助于ChatGPT，并利用其强大的上下文学习能力来进行三重分解，以找到给定一个句子的所有可能的关系三元组。与其他方法中的依赖解析器相比，我们的解析在处理长标题时工作得更好，并且不限制空间关系（例如，to the left of），这可以完全捕获动作和交互中的丰富组合语义，如walking、talking to。</p><h4 id="改善VLA模型的视觉关系理解"><a href="#改善VLA模型的视觉关系理解" class="headerlink" title="改善VLA模型的视觉关系理解"></a>改善VLA模型的视觉关系理解</h4><p>为了解决VLA模型对视觉关系理解的局限性，我们利用了一个具有丰富的关系知识的数据源集合，其中包括人-对象交互数据集和图像场景图数据集。与我们的接地管道类似，我们隔离视觉实体，并在视觉和文本方面构建三元组，然后实现一个三重级的对比学习目标来微调VLA模型。</p><p>与现有的基于规则的负提示结构相比，该设计有两个独特的优势：</p><ol><li>通过将单个图像分解为多个三元组，我们可以获得更多的训练实例，提高训练数据的多样性，而不是简单地使用整个图像进行微调。</li><li>实体的隔离消除了对图像中其他内容的干扰，为模型微调提供了更加有用的监督信息。我们使用LoRA以参数高效的方式对VLA模型进行微调，提高了其视觉关系的理解，同时保留了从大规模数据中学习到的强大的通用特征表示。</li></ol><p>我们将得到的模型称为VR-VLA（视觉关系VLA）。</p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><blockquote><p>我们报告了REC数据集上的SOTA零样本视觉定位结果，并在Who的Waldo数据集上显示了有希望的结果，其中我们的零样本方法达到了与完全监督方法相当的精度。</p></blockquote><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="D:\mynotes\source\images\deepLearning\zero-shot-REC2.png" alt="image-20250203125055175"></p><p>分为两个阶段。首先，解耦图像和文本实体，并以（主题、谓词、对象）的格式构造三元组。其次，计算三重级相似度矩阵，并将其传播到实例级，得到相似度得分最高的边界框。我们的匹配管道的主要重点是精确地建模实体之间的关系，这是通过三级结构相似性来实现的，如图2所示。我们还提供了一个新的配方，使VLA模型具有更好的组合理解能力。</p><h3 id="构建三元组"><a href="#构建三元组" class="headerlink" title="构建三元组"></a>构建三元组</h3><p>给定字幕$C$、图片$I$。假定字母、图片都由实体集组成，即$\epsilon_{T} = {e_i^T}<em>{i=1}^M$，$\epsilon_I = {e_i^I}</em>{i=1}^N$，M、N分别表示C、I的实体数。分别用$\gamma^T(\cdot)$、$\gamma^I(\cdot)$表示实体对之间的关系</p><p>在这一阶段，我们的目标是为两个模态构建实体-关系三元组。</p><p>文本三元组表示为$\tau_T = {t_{ij}^T = (e_i^T, \gamma^T(e_i^{T}, e_j^T), e_j^T)|1 \leq i, j, \leq M}$，基数为$M’$。图像三元组表示为$\tau_I = {t_{kl}^I = (e_k^I, \gamma^I(e_k^{I}, e_l^I), e_l^I)|1 \leq k, l, \leq M}$，基数为$N’$。</p><h4 id="文本三元组构建"><a href="#文本三元组构建" class="headerlink" title="文本三元组构建"></a>文本三元组构建</h4><p>利用LLM强大的上下文学习能力将字幕$C$解析为三联体$\tau_T$。具体来说，我们设计了一个提示来指示ChatGPT解析字幕文本$C$。图3提供了我们如何设计RefCOCO/+/g数据集的提示的概述，进一步的细节阐述如下。注意，提示可能会根据数据集的不同而有所不同，以适应不同的数据分布。如图3所示，提示可分为四个部分：</p><p><img src="D:\mynotes\source\images\deepLearning\zero-shot-REC3.png" alt="image-20250203153104880"></p><ol><li>通用指令：为特定的任务定义了一个明确而通用的指令</li><li>支持细节：包括预期的输出格式、基本元素以及应该或不应该包含的首选项等等</li><li>上下文学习示例：我们策划了几个上下文学习示例来指导LLM</li><li>任务指令：附加了输入字母$\tau$</li></ol><p>最后，将上述输入输入到LLM中，然后通过LLM完成生成解耦的文本三元组；还会在完成后做一个简单的格式检查。（这个部分叫LLM completion）</p><h4 id="视觉三元组构造"><a href="#视觉三元组构造" class="headerlink" title="视觉三元组构造"></a>视觉三元组构造</h4><p>在图像中，实体由边界框表示，每个边界框都包含一个单独的对象。这些方框可以由数据集预定义，也可以使用预先训练过的对象检测器进行提取。在不知道这些实体是如何关联的情况下，我们假设每对实体之间可能发生潜在的交互。因此，我们使用笛卡尔积生成视觉对，它包括所有可能的实体组合。注意，当一对实体由同一个实体（方框）组成两次时。这代表了一种自我关系，暗示了实体自己的属性，如颜色（如红色）或自我行为（如walking）。然后我们使用两个实体boxes的并区域$\gamma^T(e_i^{T}, e_j^T)$来表示实体之间的相互关系。</p><p>最后，基于类似于ReCLIP的启发式规则过滤掉冗余的三元组$t_{ij}^I \in \tau_I$。具体来说，给定一个文本三联体$t_{ij}^T$，其中它的谓词包含反映一些空间关系的关键字，如<em>to the left of</em>。在这种情况下，我们过滤掉前一个box的中心点（即主体）在后一个box（即对象）的右边这样的视觉框对。这种方法比像ReCLIP中那样构建复杂的空间语义树要简单得多，但它有效地添加了空间上下文并提高了性能。</p><h3 id="基于结构相似性的定位"><a href="#基于结构相似性的定位" class="headerlink" title="基于结构相似性的定位"></a>基于结构相似性的定位</h3><p>有两种定位思路：</p><ol><li>$text \rightarrow image$：根据文本描述确定图像区域</li><li>$image \rightarrow text$：定位给定图像区域的相关文本描述</li></ol><p>鉴于它们的对称性，本节将主要关注$text \rightarrow image$的定位场景。</p><h4 id="三元组级别的定位"><a href="#三元组级别的定位" class="headerlink" title="三元组级别的定位"></a>三元组级别的定位</h4><p>对于给定的文本三元组$t_{ij}^T = (e_i^T, \gamma^T(e_i^{T}, e_j^T), e_j^T)$，分别将$e_i^T$、$\gamma^T(e_i^{T}, e_j^T)$、$e_j^T$经过VLA文本编码器编码，获得三个文本embeddings$(\mathbf{t}<em>i, \mathbf{t}</em>{i,j}, \mathbf{t}<em>j)$。图像三元组类似，得图像embeddings$(\mathbf{v}<em>k,\mathbf{v}</em>{k,l},\mathbf{v}<em>l)$。这两个三元组的相似度为：<br>$$<br>\mathbf{S}(t</em>{ij}^T, t</em>{kl}^I) = cos(\mathbf{t}<em>i, \mathbf{v}<em>k) + cos(\mathbf{t}</em>{i, j}, \mathbf{v}</em>{k, l}) + cos(\mathbf{t}_j, \mathbf{v}_l). (3)<br>$$<br>$\mathbf{S} \in \mathbb{R}^{M’ \times N’}$是所有文本、图像三元组的相似度矩阵。之后可以得到一个二进制指标矩阵$\mathbf{B} \in {0,1}^{M’ \times N’}$，计算方式如下：</p>$$\mathbf{B}(t_{ij}^{T},t_{kl}^{I})  = \left\{  \begin{array}{ll} 1 &amp; \text{ if }k,l = \arg \mathop{\max }\limits_{{m,n}}\left( {\mathbf{S}\left( {{t}_{ij}^{T},{t}_{mn}^{I}}\right) }\right) . \\  0 &amp; \text{ otherwise. }\end{array}\right.$$<p>也就是说，对于每个文本三元组$t_{ij}^T$，二进制指标矩阵B将值1赋给最相似的图像三元组$t_{kl}^I$，将值0赋给所有其他图像。</p><h4 id="实例级别的定位"><a href="#实例级别的定位" class="headerlink" title="实例级别的定位"></a>实例级别的定位</h4><p><img src="D:\mynotes\source\images\deepLearning\zero-shot-REC4.png" alt="image-20250203164142197"></p><p>视觉定位问题中的另一个实质性挑战是，三元组中的<em>主语</em>和<em>宾语</em>都可能与其他实体有多重交互。为此，我们设计了一种新的方法来将三元组级别的定位结果传播到实例中。</p><p>具体来说，基于三元组级别的定位结果，我们可以计算实例级结构感知相似度矩阵R如下：<br>$$<br>\mathbf{R}(e_i^T, e_k^I) = \sum_{j,l} \mathbf{B}(t_{ij}^{T},t_{kl}^{I}) \mathbf{S}(t_{ij}^T, t_{kl}^I) + \sum_{j,l} \mathbf{B}(t_{ji}^{T},t_{lk}^{I}) \mathbf{S}(t_{ji}^T, t_{lk}^I). (4)<br>$$<br>等式中的两个项分别考虑了$e_i^T$和$e_k^I$以不同三元组中的<em>主语</em>和<em>宾语</em>出现的情况，如图4下半部所示。通过聚合来自多个定位三元组的相似性得分，它有助于更准确地找到实例级的对应关系。</p><p>最后，对于每个文本实体，我们计算最相关的图像实体如下：<br>$$<br>\hat{e}_i^I = {argmax}_m \mathbf{R}(e_i^T, e_m^I)<br>$$<br>其中$\hat{e}_i^I$表示$e_i^T$对应的视觉实体。如果在等式中实现一个基于阈值的选择来代替argmax函数，我们的方法可以很容易地扩展到一对多的定位场景。但这里将讨论局限于一对一的定位，以获得更清晰的理解。</p><h3 id="增强的关系理解"><a href="#增强的关系理解" class="headerlink" title="增强的关系理解"></a>增强的关系理解</h3><p>上面算式计算三元组相似度时用的是余弦相似度，试图通过余弦相似度来量化实体之间的关系，并假设VLA模型能够充分把握这些关系。但研究表明这种假设在实践中往往不足。</p><p>为了解决这个问题，我们使用了丰富的关系知识的数据集的组合来细化VLA模型。这些数据集包括HICO-det、SWiG和视觉基因组（VG）。值得注意的是，在VG数据集的情况下，我们排除了从COCO中获得的所有图像，以保持零样本协议的完整性，并与我们基于RefCOCO/+/g的实验对齐。</p><p>上述提到的数据集提供了目标的注释边界框、文本描述、与其他对象的关系。所以我们可以很容易地遵循我们在三元组级别定位阶段所做的事情来创建视觉-文本的三元组，然后对这些三元组利用对比学习损失。为了表示清晰，我们在等式中使用了相同的符号来计算两个三胞胎之间的相似性。假设$(t_{ij}^{T}$和$t_{kl}^{I}$是两个对应的三元组，我们定义对比损失如下：</p>$$\mathcal{L} = \mathop{\sum }\limits_{\left( {t}_{ij}^{T},{t}_{kl}^{I}\right) }\left\lbrack  {\log \left( \frac{\mathbf{S}\left( {{t}_{ij}^{T},{t}_{kl}^{I}}\right) }{\mathop{\sum }\limits_{{m,n}}\mathbf{S}\left( {{t}_{ij}^{T},{t}_{mn}^{I}}\right) }\right) }\right.\left. {+\log \left( \frac{\mathbf{S}\left( {{t}_{ij}^{T},{t}_{kl}^{I}}\right) }{\mathop{\sum }\limits_{{m,n}}\mathbf{S}\left( {{t}_{mn}^{T},{t}_{kl}^{I}}\right) }\right) }\right\rbrack$$<p>通过这种改进的方法，可以提高VLA模型对实体间关系的理解和准确评分的能力，从而提高零样本定位能力。（具体优势见概要部分）</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="建立"><a href="#建立" class="headerlink" title="建立"></a>建立</h3><p><strong>RefCOCO/RefCOCO+/RefCOCOg数据</strong>来自MS-COCO。RefCOCO包括19,994张图像，其中有142,210个参考表达式。RefCOCO+有19,992张图像和141,564个表达式。RefCOCOg包含26,771张图像，包含104,560个表达式。在RefCOCO和RefCOCO+中，表达式更短，平均有1.6个名词和3.6个单词。在RefCOCOg中，表达式更长，平均有2.8个名词和8.4个单词。</p><p><strong>Who‘s Waldo</strong>引入了一个以人为中心的视觉定位任务，其中字幕中的所有名称都被屏蔽，迫使模型通过属性和视觉实体之间的交互来链接框和隐藏的[NAME]令牌。字幕很长并且包含了复杂的场景描述。我们使用它的测试分割来进行评估，其中包含6741张图像。每个字幕包含大约30个单词。</p><p><strong>评估指标</strong>：在RefCOCO/+/g上，我们遵循之前的工作，使用accuracy作为接地结果（grounding results），即如果预测盒与真实区域之间的IoU值大于0.5，则是正确的预测。对于Who‘s Waldo，根据之前的工作，对于给定的文本描述中的人和图像中的边界框之间的对应关系的接地结果，我们报告了在测试集上的真实标签链接的accuracy。</p><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p>RefCOCO/+/g我们将我们的方法与各种零样本视觉定位模型进行了基准测试，包括彩色提示调优（CPT）、GradCAM 和ReCLIP [54]。ReCLIP代表了零射击REC方法中最新的SOTA。</p><p><img src="D:\mynotes\source\images\deepLearning\zero-shot-REC-t1.png" alt="image-20250204102809470"></p><p>如表1所示，与使用相同CLIP架构的其他模型相比，我们的方法在所有的拆分中都优于其他方法。具体来说，我们的模型比ReCLIP的性能提高了19.53%，平均提高了9.74%。值得注意的是，即使不对主干CLIP模型进行微调，我们的方法也可以超过ReCLIP3.91%，平均为1.05%，这表明基于ChatGPT解析的结构相似性也有助于理解关系。</p><p>此外，我们还将我们的方法扩展到另一个VLA模型——FLAVA ，以验证我们的方法的可推广性。毫不奇怪，当集成到我们的匹配管道中时，FLAVA比CLIP模型显示出更优越的性能。这可以归因于FLAVA天生更健壮的架构。在对FLAVA进行微调后，所得到的VR-FLAVA持续地提高了所有数据集分割的性能，加强了我们的方法在增强对各种VLA模型的关系理解方面的有效性。</p><p><strong>Who’s Waldo</strong> 我们将我们的方法与在接地数据集上训练的模型进行了比较，包括Gupta，SL-CCRF ，MAttNet和UNITER 。Who‘s Waldo方法作为监督基线，如其原论文。此外，我们为我们的数据集适配了ReCLIP，利用它们的语言解析器来识别潜在的参考表达式，然后使用它们的原始方法进行接地。</p><p><img src="D:\mynotes\source\images\deepLearning\zero-shot-REC-t2.png" alt="image-20250204103320804"></p><p>如表2所示，我们的方法优于所有在接地数据集上训练的模型，具有显著的边际。</p><h2 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h2><p>在本节中，我们将对RefCOCOg进行消融研究。这个数据集特别适合于我们的评估，因为它有更长的字幕和丰富的实体交互，使其成为评估每个组件的理想测试平台。</p><h3 id="接地管道内组件的有效性"><a href="#接地管道内组件的有效性" class="headerlink" title="接地管道内组件的有效性"></a>接地管道内组件的有效性</h3><p>我们探索了两个关键的变化：三元组和VR-CLIP。三重态变量检验了使用三重实例匹配的影响，而不是基本的评分和排序方法，即使用CLIP对每个孤立的盒子进行评分，而不是选择一个相似性最高的盒子。VRCLIP变体评估了微调后的VR-CLIP和原始CLIP模型之间的性能差异。</p><h3 id="三元组组件的有效性"><a href="#三元组组件的有效性" class="headerlink" title="三元组组件的有效性"></a>三元组组件的有效性</h3><p>分别删除等式(3)中的主语、宾语和谓词术语以探讨其在定位性能方面的有效性</p><h3 id="三元组级和实例级定位的有效性"><a href="#三元组级和实例级定位的有效性" class="headerlink" title="三元组级和实例级定位的有效性"></a>三元组级和实例级定位的有效性</h3><p>分别删除了等式(4)中的第一项和第二项来验证我们的三元组级定位到实例级定位的设计。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/01/26/lun-wen-yue-du/fm-fsod-lun-wen-bi-ji/"/>
      <url>/2025/01/26/lun-wen-yue-du/fm-fsod-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="Few-Shot-Object-Detection-with-Foundation-Models"><a href="#Few-Shot-Object-Detection-with-Foundation-Models" class="headerlink" title="Few-Shot Object Detection with Foundation Models"></a>Few-Shot Object Detection with Foundation Models</h1><blockquote><p>小样本目标检测（FSOD）的目的是通过一些训练样本（也叫做support image）来检测没见过的新目标。</p></blockquote><p>视觉特征提取和支持查询的相似度学习是小样本目标检测（FSOD）的两个关键组成部分。现有的工作通常是基于ImageNet预训练的视觉骨干，并设计复杂的度量学习网络用于小样本学习，但其准确性仍然较低。</p><blockquote><p>度量学习也叫做相似度学习。在数学中，一个度量（或距离函数）是一个<strong>定义集合中元素之间距离的函数</strong>。一个具有度量的集合被称为<strong>度量空间</strong>。度量学习旨在学习一个度量空间，在该空间中，相同类别的样本距离更近，不同类别的样本距离更远。</p></blockquote><p>我们研究了使用现代基础模型的小样本目标检测。首先，视觉主干采用只有视觉的对比预训练的DINOv2模型，在不调整参数的情况下显示出很强的可转移性能。其次，采用大语言模型进行上下文小样本学习，输入所有类和查询图像proposal。语言指令经过精心设计，以提示LLM在上下文中对每个提案进行分类。语境信息包括proposal-propasal关系、proposal-class关系和class-class关系，它们可以在很大程度上促进小样本学习。我们在多个FSOD基准测试中全面评估了所提出的模型（FM-FSOD），从而实现了最先进的性能。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="视觉主干"><a href="#视觉主干" class="headerlink" title="视觉主干"></a>视觉主干</h3><p>首先，预先训练的视觉主干不仅要具有对各种语义概念的辨别能力，而且要具有强大的块级空间定位能力，使其成为下游定位敏感任务的理想选择。</p><p>基于这一动机，我们综合评估了多个预先训练好的视觉基础模型，包括MAE、CLIP、SAM和DINOv2，以及不同的检测架构、基于RCNN的框架ViTDet和基于Transformer的框架可变形DETR 。我们的结论是，DINOv2同时经过图像级和块级自监督目标的预训练，并配备了基于Transformer的检测框架，取得了最好的性能。此外，之前的一些工作需要在训练期间更新视觉特征主干，因此在一次前馈传递中只能支持很少的类，我们的模型不需要对视觉主干进行微调。这使得可以使用更大的类集来进行情境化的少镜头学习。</p><h3 id="LLM"><a href="#LLM" class="headerlink" title="LLM"></a>LLM</h3><p>其次，对object proposals的小样本分类是FSOD的另一个关键组件。我们建议通过在查询图像特征和类原型之间应用交叉注意来生成支持类感知的proposals。但这些proposals仍然有噪声。<strong>FSOD的关键挑战是带有噪声的proposals的小样本学习</strong>。之前的工作针对这个问题提出了几种方法，从简单的点积到更复杂的深度神经网络，目的是改进有噪声proposal的少镜头分类。</p><p>在本文中，我们建议利用预先训练好的LLMs的强大的上下文学习能力来进行FSOD中上下文化的小样本proposal分类。受最近的多模态LLMs的启发，我们仔细设计了语言指令，以提示LLM对每个proposal进行分类，并提供类别和它们的视觉原型之间的映射作为输入指令的一部分。我们的模型可以通过LLM自动利用proposal和类之间的各种上下文信息，包括proposal-propasal关系、proposal-class关系和class-class关系。提取的上下文信息可以在很大程度上促进对同一查询图像的小样本proposal分类。对于模型训练，我们使用元学习对LLM进行了微调。在每个训练过程中，我们为每个类别随机抽取一些视觉样本来计算原型，这在模型训练过程中作为强大的数据增强。</p><h2 id="提出的方法"><a href="#提出的方法" class="headerlink" title="提出的方法"></a>提出的方法</h2><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><p>在FSDOD中，我们有两个类集合$C = C_{base} \cup C_{novel}, C_{base} \cap C_{novel} = \emptyset$。$C_{novel}$<strong>也叫做support classess，其中的样本叫support images</strong>。K-shot目标检测就是每个新类都有K个边界框注释作为训练数据。FSOD的目标是在数据丰富的基类训练数据的帮助下，使用少量的视觉示例来检测新的类，并在基类上保持良好的性能。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="D:\mynotes\source\images\deepLearning\FM-FSOD2.png" alt="2"> </p><p>如图2所示，我们的模型主要由以下三个子模块：</p><h4 id="Visual-Feature-Extraction"><a href="#Visual-Feature-Extraction" class="headerlink" title="Visual Feature Extraction"></a>Visual Feature Extraction</h4><p>为query images和support images提取特征表示。</p><p>在FSOD中，query image $I_q \in \mathbb{R}^{H_{I_q} * W_{I_q} * 3}$和N-way K-shot support set $S = { { {I_{S}^{j,i}} } <em>{i=1}^{K}}</em>{j=1}^{N}$作为输入，$I_S^{j,i} \in \mathbb{R}^{H_{I_q} * W_{I_q} * 3}$。</p><p>用ViT提取query image的特征表示$f_q = \mathcal{F}(I_q)$，并为后面的目标定位保留所有位置块表示。对于support set，同样用ViT提取每张support image的特征表示$f_S^{j,i} = \mathcal{F}(I_S^{j,i})$。实际上，support image会在目标对象附近被裁剪。我们使用RoIAlign根据对象的边界框注释计算对象表示$\overline{f}_S^{j,i} = RoI Align(f_S^{j,i}, box_S^{j,i})$。</p><p>那么每个类的原型就是K-shot support特征的平均值${\hat{f}<em>S^j = \frac{\sum</em>{i=1}^{K} \overline{f}<em>S^{j,i}}{K}}</em>{j=1}^{N}$</p><p>在本文中，我们使用预先训练过的冻结DINOv2作为我们的特征主干，原因如下：</p><ol><li><p>DINOv2是一种视觉专用的自监督学习模型，在大规模的curated image数据集上进行训练。全局图像级和局部块级自监督目标共同用于训练特征主干。局部块损失可以使模型对局部化敏感，这对下游检测任务很友好。</p></li><li><p>DINOv2模型在一个大规模的图像数据集上进行了预训练。为了保持DINOv2中的原始知识，我们在训练过程中冻结了特征主干。我们的实验表明，对DINOv2的某些层进行微调并不能提高性能。此外，冻结主干允许我们预先计算每个类的支持特性，这使得在上下文中使用更广泛的类集进行更大的few-shot分类。</p></li></ol><p>使用DINOv2的一个潜在问题是，用于测试的few-shot新类可能在DINOv2预训练中看到。我们认为，预训练只学习图像表示。而如何有效地将基础模型转移到下游任务中仍有待探索，尤其是当训练前的自我监督学习任务和下游检测任务存在较大的差异时这个问题更加突出。</p><h4 id="proposal-Generation"><a href="#proposal-Generation" class="headerlink" title="proposal Generation"></a>proposal Generation</h4><p>在使用DINOv2提取视觉特征后，我们使用Transformer编码器-解码器架构来生成proposal。</p><p>首先在DINOv2提取的query image的块tokens上使用多层Transformer编码器。通过Transformer编码器模块，每个块token特征都被全局上下文信息丰富。采用了多尺度变形注意方法，可以加快收敛。</p><p>为了生成能感知support class的proposals，我们计算类原型${\hat{f}<em>S^j}</em>{j=1}^{N}$和来自Transformer编码器的query image特征的交叉注意力，以生成能感知support class的query image特征。</p><p>然后，能感知support class的query image特征和一系列随机初始化的对象查询$Q = {q_i}<em>{i=1}^{M}$作为Transformer解码器的输入。接着采用几种自注意层和交叉注意层来细化对象查询的表示$\hat{Q} = {\hat{q_i}}</em>{i=1}^{M}$，让它们逐渐收敛到相应的物体上。在Transformer解码器的顶部使用简单的线性层计算每个对象查询的边界框位置$B = {b_i}_{i=1}^M, b_i = [x,y,w,h]$。这样，我们可以为下面的few-shot proposal classification模块生成少量（在我们模型中M=300）支持类感知的对象查询（或者叫proposals）。</p><h4 id="few-shot-proposal-classification"><a href="#few-shot-proposal-classification" class="headerlink" title="few-shot proposal classification"></a>few-shot proposal classification</h4><p>根据映射类别和它们的视觉原型为每个proposal进行分类</p><p>在从查询图像中获得proposals以及每个类的原型表示之后，few-shot classification将proposals分类为新类之一或“空”类。我们提出利用LLM强大的上下文学习能力进行上下文化的few-shot proposal classification，通过引入上下文信息提高few-shot分类的准确性，简化设计复杂度量学习网络的工作。</p><p>具体步骤：</p><ol><li>在LLM tokenizer加上class tokens（e.g.: <class_1>, …,<class_80>）和background class token（<class_bg>）</class_bg></class_80></class_1></li><li>为LLM设计以下语言指令<em>“Please classify each of the proposals in <proposal_1>…<proposal_300>. Categories Containing,&lt;class_ 1&gt;: &lt;visual prototype&gt;… <class_80>: &lt;visual prototype&gt;. If the proposal does not belong to any of these classes, it will be classified as <class_bg>.”</class_bg></class_80></proposal_300></proposal_1></em>，以对每个proposal执行分类。</li><li>用相应的proposal特征替换占位符<proposal_1>，然后用一个可训练的投影层，将维数转换为LLM中单词嵌入的维数；将&lt;visual prototype&gt;替换为相应的类原型，然后后面跟另一个投影层。其余的语言指令由LLM tokenizer使用新引入的类标记进行标记化。我们使用Vicuna作为我们的默认语言模型，这是一个仅有解码器的LLM，从Llama进行指令调整得到的。LLM将上述指令的编码后特征作为输入，并通过隐式查找输入指令中定义的类别映射表，为每个proposal生成<class_id> token。</class_id></proposal_1></li><li>proposal classification的输出标记与输入指令中的proposals保持相同的顺序。通过LLM对生成的标记进行解码后，我们将LLM的分类结果与proposal generation模块中的预测相结合，得到最终的检测结果。</li></ol><p>通过提供上述的语言说明，并提示LLM对每个proposal进行分类，我们的方法设计得很简单。更重要的是，我们的模型将所有proposal的输入与所有类的类别映射表一起使用，它可以自动利用proposal和类之间的多重关系，包括proposal-proposal关系、proposal-class关系和class-class关系。提取的上下文信息可以在很大程度上促进对同一查询图像的few-shot proposal分类。</p><h3 id="训练框架"><a href="#训练框架" class="headerlink" title="训练框架"></a>训练框架</h3><p>分三步：</p><ol><li>预训练proposal generation模块。在基类上使用可变形的DETR 与冻结的DINOv2主干。遵循DETR中定义的原始损失函数，首先找到预测对象集和真实对象集之间的最优二部匹配，然后将模型优化到这个最优分配。</li><li>在基类上训练整个模型。proposal generation模块用第一步的训练的结果初始化，LLM用Vicuna模型初始化。为了获得LLM训练proposals的真实标签，我们使用DETR中的二部匹配来为proposals分配标签。然后，我们可以使用next-token预测损失端到端训练LLM，这个损失是在真实proposal标签上计算。在这一步中，proposal generation模块也通过DETR损失进行了微调。</li><li>根据新类来微调模型。类似第一步和第二步，首先使用下采样的基类和新类对proposal generation模块进行微调。然后，我们使用基类和上采样的新类来微调LLM（因为微调LLM需要更多的训练数据）。</li></ol><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>PASCAL VOC</li><li>MSCOCO</li></ul><h3 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h3><p>MSCOCO数据集上的：</p><p><img src="D:\mynotes\source\images\deepLearning\FM-FSOD-t1.png" alt="t1"></p><p>PASCAL VOC数据集上的</p><p><img src="D:\mynotes\source\images\deepLearning\FM-FSOD-t3.png" alt="t3"></p><p>从表1中可以发现，本文的方法明显优于传统的基于Faster R-CNN的FSOD模型，特别是对于shot数量相对较多的设置。这验证了对FSOD使用大型基础模型的有效性。基于DETR的方法通常比基于Faster R-CNN的方法有更好的结果。同样地，我们在更大数量的shot中观察到较大的性能提高，但是也有两种方法（FS-DETR、Meta-DETR）在1次和2次shot上AP50都优于我们的模型。我们认为，这是因为LLM很难用如此少的训练数据进行调整，而且我们不像FS-DETR那样使用外部数据集来进行FSOD训练。未来可能的工作是引入外部数据集用于检测训练，并探索在小数据下使用LLM的有效训练。类似的性能比较见表3。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/01/19/lun-wen-yue-du/id-like-prompt-learning-for-few-shot-out-of-distribution-detection-lun-wen-bi-ji/"/>
      <url>/2025/01/19/lun-wen-yue-du/id-like-prompt-learning-for-few-shot-out-of-distribution-detection-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="ID-like-Prompt-Learning-for-Few-Shot-Out-of-Distribution-Detection"><a href="#ID-like-Prompt-Learning-for-Few-Shot-Out-of-Distribution-Detection" class="headerlink" title="ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection"></a>ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>分布外（Out-of-Distribution, OOD）检测方法通常利用<strong>辅助离群值</strong>来训练识别分布外样本的模型，特别是从辅助离群值数据集中发现具有挑战性的离群值，以提高OOD 检测能力。但是在有效区分<strong>与分布内（In-Distribution, ID）数据很相似的分布外样本</strong>时仍面临着限制。这种样本叫做类ID样本（ID-like）。</p><blockquote><p>分布外（Out-of-Distribution, OOD）是指数据样本不属于模型训练时所使用的数据分布的情况‌。</p></blockquote><p>为此，这篇文章提出了一种新的OOD检测框架，利用<strong>CLIP</strong>从ID样本的附近空间发现ID-like离群值，从而帮助识别这些最具挑战性的OOD样本。</p><p>然后提出了一个提示学习框架，利用识别出的ID-like离群值来进一步发挥CLIP在OOD检测方面的能力。得益于强大的CLIP，我们只需要少量的 ID 样本就能学习模型的提示，而无需暴露其他辅助离群值数据集。</p><ol><li><p>首先构建与 ID 数据高度相关的离群值</p><p>具有挑战性的OOD 样本往往与 ID 数据表现出高度相关性，具有高度相似的视觉或语义，这就会导致错误的预测，因此一个自然的想法就产生了：从 ID 样本中提取相关特征来构建具有挑战性的 OOD 样本。</p><p>直接从原始图像中构建离群值。对 ID 样本的邻近空间进行了多次采样。在这些样本中，与 ID 提示相似度较低的样本即使它们包含与ID 类别相关的特征，也不会被归为 ID 类别。因此，这些样本自然被选为具有挑战性的 OOD 样本。</p></li><li><p>并为 OOD 检测引入了新颖的类 ID 提示</p><blockquote><p>我们认为，仅仅依靠 ID 提示不足以解决这一问题。因此，我们引入了额外的提示来加强 OOD 识别</p></blockquote><p>例如，对于dog和wolf，二者很相似，假设开发了一个额外的prompt，称为”dog-like”，与”dog” prompt相似。如果我们能提高 “dog-like” prompt和与 “dog”高度相关的OOD 样本之间的相似度，模型就能通过”dog”提示识别出狗并通过“dog-like”提示识别出具有挑战性的 OOD 样本（如”wolf“）。</p><p>具体来说，我们将额外的提示与<code>1</code>中构建的具有挑战性的OOD 相匹配，创造出与 ID 提示类似的 OOD 提示，从而有效识别具有挑战性的 OOD 样本。</p></li></ol><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="D:\mynotes\source\images\deepLearning\ID-likePromptLearning3" alt="ID-like3"></p><h3 id="从ID样本中构建离群值"><a href="#从ID样本中构建离群值" class="headerlink" title="从ID样本中构建离群值"></a>从ID样本中构建离群值</h3><p>定义训练集$D = {(x_1, y_1), (x_2, y_2), …, (x_N, y_N)}$，N表示样本数。</p><p>为了充分探索训练样本的临近空间，我们对每个 ID 样本$x_i$进行多次随机裁剪，得到集合$X_{i}^{crop} = {x_{i,1}^{crop}, x_{i,2}^{crop}, …, x_{i,M}^{crop}}$​，M表示裁剪次数。同时，我们使用预先定义的模板（如a photo of a &lt;$y_k$&gt;， $y_k$表示类名）创建相应的类别描述文本$t_k$。</p><p>然后利用预先训练好的 CLIP 模型来计算样本集$X_{i}^{crop}$与描述$t_k$之间的余弦相似度。根据余弦相似度的强弱，从高、低相似度段分别提取ID、OOD样本，记做$X_{i}^{in} = {x_{i,1}^{in}, x_{i,2}^{in}, …,x_{i,Q}^{in}}$和$X_{i}^{out} = {x_{i,1}^{out}, x_{i,2}^{out}, …,x_{i,Q}^{out}}$。于是得到$D^{in} = {(x_{1,1}^{in}, y_1), (x_{1,2}^{in}, y_1), …, (x_{N,Q}^{in}, y_N)}$、$D^{out} = {(x_{1,1}^{out}, y_1), (x_{1,2}^{out}, y_1), …, (x_{N,Q}^{out}, y_N)}$</p><h3 id="提示学习"><a href="#提示学习" class="headerlink" title="提示学习"></a>提示学习</h3><p>为每个类别初始化一个可学习的提示，形成 ID 提示集$T^{in} = {t_1^{in}, t_2^{in}, …, t_K^{in}}$，并初始化一个额外的OOD 提示集合$T^{out} = {t_1^{out}, t_2^{out}, …, t_C^{out}}$。鉴于个别描述所覆盖的范围有限，我们引入了多个OOD描述来增强覆盖范围。与CoOp类似，我们为这些文本描述随机初始化嵌入，然后使用下文中提出的损失函数对它们进行优化。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><h4 id="In-distribution-loss"><a href="#In-distribution-loss" class="headerlink" title="In-distribution loss"></a>In-distribution loss</h4><p>为了确保分布内数据的分类性能，我们使用了一个标准的交叉熵损失函数，它度量ID样本的预测标签概率和真实标签之间的差异。在形式上，ID交叉熵损失被定义为：</p>${\mathcal{L}}_{in} = {\mathbb{E}}_{\left( {x,y}\right)  \sim  {D}^{in}}\left\lbrack  {-\log \frac{{e}^{{s}_{ * }/\tau }}{\mathop{\sum }\limits_{{k = 1}}^{K}{e}^{{s}_{k}^{in}/\tau } + \mathop{\sum }\limits_{{c = 1}}^{C}{e}^{{s}_{c}^{out}/\tau }}}\right\rbrack$<p>其中，${s}<em>{ * } = \operatorname{sim}\left( {\mathcal{T}\left( {t}</em>{ * }\right) ,\mathcal{I}\left( x\right) }\right) ,{s}<em>{k}^{in} = \operatorname{sim}\left( {\mathcal{T}\left( {t}</em>{k}^{in}\right) ,\mathcal{I}\left( x\right) }\right) ,{s}<em>{k}^{out} = \operatorname{sim}\left( {\mathcal{T}\left( {t}</em>{k}^{out}\right) ,\mathcal{I}\left( x\right) }\right)$</p><h4 id="Out-of-distribution-loss"><a href="#Out-of-distribution-loss" class="headerlink" title="Out-of-distribution loss"></a>Out-of-distribution loss</h4><p>为了将OOD提示与离群值对齐，我们引入了OOD损失。需要注意的是，在理想的场景中，每个类别都会有一个ID提示和一个OOD提示。然而，为了节省计算资源和提高训练效率，我们将OOD提示的数量固定在100个。因此，当没有足够的OOD提示与ID类别建立一一对应关系时，我们会最大化OOD提示和异常值之间的整体相似性。为了实现这一点，我们建议有以下损失：</p>$${\mathcal{L}}_{\text{out }} = {\mathbb{E}}_{x \sim  {D}^{\text{out }}}\left\lbrack  {-\log \frac{\mathop{\sum }\limits_{{c = 1}}^{C}{e}^{{s}_{c}^{\text{out }}/\tau }}{\mathop{\sum }\limits_{{k = 1}}^{K}{e}^{{s}_{k}^{in}/\tau } + \mathop{\sum }\limits_{{c = 1}}^{C}{e}^{{s}_{c}^{\text{out }}/\tau }}}\right\rbrack$$<p>此外，我们观察到在训练中使用以下形式的Lout更有利于优化提示：</p>$${\mathcal{L}}_{\text{out }} = {\mathbb{E}}_{x \sim  {D}^{\text{out }}}\left\lbrack  {\log \frac{\mathop{\sum }\limits_{{k = 1}}^{K}{e}^{{s}_{k}^{in}/\tau }}{\mathop{\sum }\limits_{{k = 1}}^{K}{e}^{{s}_{k}^{in}/\tau } + \mathop{\sum }\limits_{{c = 1}}^{C}{e}^{{s}_{c}^{\text{out }}/\tau }}}\right\rbrack$$<h4 id="Diversity-regularization"><a href="#Diversity-regularization" class="headerlink" title="Diversity regularization"></a>Diversity regularization</h4><p>由于所有的OOD提示都是在同一目标下随机初始化和优化的，在OOD提示之间存在过度相似性的风险。类似的OOD提示可能会导致可检测的OOD类数量的减少。为了减轻这个问题并确保OOD提示的多样性，我们引入了一个额外的损失${\mathcal{L}}_{\text{div }}$，它显式地最大化了提示之间的差异：</p>$${\mathcal{L}}_{\text{div }} = \frac{\mathop{\sum }\limits_{{c = 1}}^{{C - 1}}\mathop{\sum }\limits_{{j = c + 1}}^{C}\operatorname{sim}\left( {{h}_{c}^{\text{out }},{h}_{j}^{\text{out }}}\right) }{C\left( {C - 1}\right) /2}$$<p>这里$h_c^{out} = \mathcal{T}(t_c^{out})$，$h_j^{out} = \mathcal{T}(t_j^{out})$。$t_c^{out}, t_j^{out} \in T^{out}$表示OOD提示中的第c、j个提示</p><p>总损失：<br>$$<br>\mathcal{L} = \mathcal{L}<em>{in} + \lambda</em>{out}\mathcal{L}<em>{out} + \lambda</em>{div}\mathcal{L}_{div}<br>$$</p><h4 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h4><p>在执行分类任务时，我们使用与CLIP相同的分类方法，仅依赖于ID提示进行分类。对于OOD检测，我们将评分函数定义为：</p>$$S(x) = \frac{\mathop{\sum }\limits_{{k = 1}}^{K}{e}^{{s}_{k}^{in}/\tau }}{\mathop{\sum }\limits_{{k = 1}}^{K}{e}^{{s}_{k}^{in}/\tau } + \mathop{\sum }\limits_{{c = 1}}^{C}{e}^{{s}_{c}^{\text{out }}/\tau }}$$<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>与以往的OOD检测任务不同，我们的主要目标是在开放世界环境中实现OOD检测，所以我们不选择一些玩具（如低分辨率）数据集，如CIFAR [16]和MNIST [17]。在我们的工作中，我们遵循MOS [13]和MCM [28]的设置，它们使用ImageNet-1k [3]作为ID数据，以及自然学家[11]的一个子集，将[42]和纹理[2]作为OOD数据。SUN [39]作为一个特定的OOD数据集进行了独立的测试。在MOS [13]之后，这些OOD数据从与ImageNet-1k [3]不重叠的类别中随机选择。此外，一些消融实验使用ImageNet-100进行ID数据。该数据集遵循MCM [28]的配置，该配置从ImageNet-1k中选择100个类作为ID数据。</p><h3 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h3><p>使用CLIP-B/16作为OOD提示学习的预训练模型。具体地说,，CLIP-B/16，由一个ViT-B/16 Transformer作为图像编码器，一个自注意Transformer作为文本编码器组成。在我们的实验中，我们保持了CLIP的所有网络参数的固定，包括图像编码器和文本编码器，只更新了文本输入端的嵌入层，遵循提示学习的方法。</p><h3 id="实施细节"><a href="#实施细节" class="headerlink" title="实施细节"></a>实施细节</h3><p>对于少样本训练，需要在完整的训练数据中的每个类中随机选择一定数量的样本来形成训练集。例如，我们从ImageNet-1k中的每个类中随机选择一个（一次）或四个样本（四次）。在构建ID和OOD数据时，我们对每个样本进行M（我们实验中256）次随机裁剪，并根据与手工提示的相似度选择最高的Q（我们实验中32）个和最低的Q个样本。对于ID提示，每个类只有一个可学习的提示，并且保留了类名信息。对于OOD提示，我们将它们的总数设置为C（在我们的实验中为100），并且不保留类名信息。我们将λ1设置为0.3，λ2设置为0.2，并使用AdamW作为优化器。其他超参数设置如下：训练epoch = 3，学习率= 0.005，批量大小=1，token长度L = 16。</p><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>我们采用以下OOD检测常用的评价指标：(1)分布样本真阳性率为95%时OOD样本的假阳性率（FPR95）；(2)受试者工作特征曲线下面积（AUROC）；(3) ID分类精度（ID ACC）。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>表1是比较结果，表明使用我们的方法可以获得更好的 OOD 检测性能，优于大多数比较结果。更重要的是，即使与那些需要完整数据的方法相比，我们的方法在one shot中仍然有很好的效果。具体地说，在four shot设置中，我们平均获得了 26.08% 的 FPR95 和94.36% 的 AUROC，与相同设置下表现最好的方法相比，分别降低了 12.16% 和提高了 2.76%。</p><p><img src="D:\mynotes\source\images\deepLearning\ID-likePromptLearning-t1.png" alt="t1"></p><p>图 4 显示了我们的方法与 MCM 在 iNaturalist 数据集上的比较。我们的方法性能更优，ID 和 OOD 之间的差异明显更大。这表明，在区分 ID 和 OOD 时，MCM 对阈值更敏感，而我们的方法能更直观地区分 ID 和 OOD。</p><p><img src="D:\mynotes\source\images\deepLearning\ID-likePromptLearning4" alt="image-20250123155828480"></p><p>如表2所示，我们的方法优于其他少样本方法，在ID数据上获得了更好的分类结果，为68.28%。</p><p><img src="D:\mynotes\source\images\deepLearning\ID-likePromptLearning-t2.png" alt="image-20250122161453748"></p><h2 id="消融研究"><a href="#消融研究" class="headerlink" title="消融研究"></a>消融研究</h2><p>离群值的有效性</p><p>即时学习的有效性</p><p>不同数量的ID-like提示（OOD 提示）的有效性</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/01/12/lun-wen-yue-du/efficient-teacher/"/>
      <url>/2025/01/12/lun-wen-yue-du/efficient-teacher/</url>
      
        <content type="html"><![CDATA[<p>基于单阶段锚框的检测器缺乏生成高质量或灵活伪标签的结构，导致半监督目标检测（SSOD）出现严重的不一致性问题。</p><p>在本文中，我们提出了 Efficient Teacher（高效教师）框架，用于可扩展且有效的基于锚点的单阶段 SSOD 训练。由如下部分组成：</p><ul><li>Dense Detector：是一个baseline模型，它利用受 YOLOv5 启发的<strong>密集采样技术</strong>扩展了 RetinaNet。</li><li>Pseudo Label Assigner：能更精细地利用密集检测器（Dense Detector）中的伪标签，<strong>解决pseudo label inconsistency问题</strong>。防止了大量低质量伪标签在师生互学机制中对 Dense Detector 造成干扰而导致偏差的发生。</li><li>Epoch Adaptor：是一种能为 Dense Detector 提供稳定、高效的端到端 SSOD 训练计划的方法。利用<strong>域和分布自适应</strong>让 Dense Detector 学习全局分布一致的特征，使训练与标记数据的比例无关。</li></ul><p>本文的主要贡献如下：</p><ul><li>我们设计了 Dense Detector 作为基线模型来比较 YOLOv5 和 RetinaNet 之间的差异，通过使用密集采样，性能提高了 5.36AP50:95。</li><li>我们提出了一种有效的 SSOD 训练框架，称为 Efficient Teacher，其中包括一种新颖的伪标签分配机制–伪标签分配器（Pseudo Label Assigner），可减少伪标签的不一致性；以及 Epoch Adaptor，可实现快速高效的端到端 SSOD 训练计划。</li><li>我们的实验证明，在 YOLOv5 上使用 Efficient Teacher 可以在 VOC、COCO 标准和 COCO 附加数据集上产生一流的结果，同时消耗的 FLOPs 明显少于以前的方法。</li></ul><p><img src="D:\mynotes\source\images\image-20250113184359799.png" alt="image-20250113184359799"></p><ul><li>objectness score表示预测框的位置质量，密集检测器通过计算预测框和GT框之间的完全IoU来获得客观性得分。</li><li>X(h, w)是学生模型的输出</li><li>Y(h, w)是Dense Detector的label Assigner产生的采样结果</li></ul><p>obj(h,w)是谁产生的？</p><p>GRL有啥用？</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/01/10/lun-wen-yue-du/mesed-lun-wen-bi-ji/"/>
      <url>/2025/01/10/lun-wen-yue-du/mesed-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="MESED"><a href="#MESED" class="headerlink" title="MESED"></a>MESED</h1><p>A Multi-modal Entity Set Expansion Dataset with Fine-grained Semantic Classes and Hard Negative Entities</p><p>具有细粒度语义类和硬否定实体的多模态实体集扩展数据集</p><h2 id="实体集扩展介绍"><a href="#实体集扩展介绍" class="headerlink" title="实体集扩展介绍"></a>实体集扩展介绍</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>实体集扩展（Entity Set Expansion，ESE）任务旨在用属于同一语义类别的新实体扩展少量种子实体。例如，给定{Washington D.C., Chicago, Los Angeles}，ESE 会尝试检索具有目标语义类别 US 的其他实体城市，如New York, NYC, Boston。</p><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><p><img src="D:\mynotes\source\images\deepLearning\MESED1.png" alt="image-20250110101845737"></p><p>传统的ESE 方法基于单模态（即字面模态），难以处理现实世界中的复杂实体，例如：</p><ol><li>具有细粒度语义差异的否定实体：指属于同一粗粒度语义类作为目标类的实体。这些实体共享文本内涵的语义，因此很难进行详细区分。例如，在扩展US Cities时，不可避免地要扩展具有相同父类（即US Location）的实体，如同样位于美国的<em>佛罗里达州</em>和<em>德克萨斯州</em>（US states）。</li><li>同义实体：实体有各种别名，ESE 模型可以很容易地理解常见的别名，而无法理解对上下文敏感的别名，如缩写和昵称。由于确定缩略语和昵称的含义需要明确的文本提示，因为确定它们的含义需要明确的文本提示。例如，SEA 在某些上下文中只表示西雅图，这可能会导致检索的遗漏。</li><li>多义实体：表示文本中提及的多个实体可能存在歧义。由于预训练的语言模型是通过word co-occurrence来学习语义的，因此由相同标记组成的实体在本质上更接近。例如，Washington, D.C.（<strong>城市</strong>）到Washington State（<strong>州</strong>）的 L2 距离反而小于到Austin等其他许多<strong>城市</strong>的距离（8.89 对 10.02）。因此，仅仅具有相同文本标记的实体可能会被错误地检索出来。</li><li>长尾实体：语料库中的低频实体，如生僻地名。由于文本描述不充分，这些实体的表示常常过于稀疏，这给它们的检索带来了挑战</li></ol><h2 id="多模态实体集扩展"><a href="#多模态实体集扩展" class="headerlink" title="多模态实体集扩展"></a>多模态实体集扩展</h2><p>因此，提出了多模态实体集扩展（MESE），即模型整合多种模态的信息来表示实体。直观地说，多模态信息对 ESE 有三方面的好处：</p><ol><li>不同模态可以提供互补信息：多模态信息可以补充文本（尤其是短文本）提供的信息，从而增强模型对实体的全面理解</li><li>对于同一语义类别或实体，多模态信息通过共同的视觉属性提供统一的信号：多模态信息可以作为一种内聚信号，将基于共同视觉属性或特征的语义类别联合起来。例如，在处理漫画人物时，图像的背景和风格可以作为漫画人物的统一特征，将其与硬性的负面语义类别 “电影人物 “区分开来。</li><li>多模态信息可为同义实体提供稳健的匹配信号</li></ol><p>遗憾的是，尽管有多种多模态数据类型，但目前还没有基于细粒度语义类别构建的多模态数据集可用来评估 MESE 的功效。为了填补这一空白，我们构建了一个<strong>名为MESED 的大规模人工标注 MESE 数据集</strong>，其中包括来自维基百科的 14,489 个实体和 434,675 对图像-句子。据我们所知，MESED 是首个进行了大规模、精细人工校准的 ESE 多模态数据集。MESED 的几个特点突出了 ESE 所面临的挑战。首先，我们精心设计了一个由 26 个粗粒度类别和 70 个细粒度类别组成的语义类别模式，其中相互模糊的细粒度类别（如<em>中国演员</em>与<em>美国演员</em>）被指定为彼此的硬否定类别。同义词和多义词实体也加入其中，以扩大实体之间的混淆。为了衡量模型理解稀疏实体的能力，还特意加入了不常见的语义类别。</p><p>我们提出了一个功能强大的多模态模型 MultiExpan，该模型在四个多模态预训练任务上进行了预训练。在 MESED 上进行的大量实验和分析表明了该数据集的高质量和我们的 MultiExpan 的有效性，同时也为未来的研究指明了方向。</p><p>主要贡献：</p><ul><li>提出了一项新颖的多模态实体集扩展（MESE）任务，该任务可扩展多种模态的实体。</li><li>首先发布了一个名为 MESED 的大规模人工标注 MESE 数据集，可以微调语义类和模棱两可的候选实体。</li><li>提供了强大的多模态baseline模型 MultiExpan，并探索了多样化的自监督预训练目标，以实现多模态实体的感知学习。</li></ul><h2 id="数据集构建"><a href="#数据集构建" class="headerlink" title="数据集构建"></a>数据集构建</h2><p>MESE 的输入是一个小集合$𝑆={𝑒_1,𝑒_2,…,𝑒_𝑘}$，其中包含几个描述某个语义类别的种子实体和一个候选实体库V。语料库D包含多模态文本${e_i, (t_1^i, v_1^i),…,(t_n^i, v_n^i)}$，给定的实体$e_i$，$t_n^i$是一个$e_i$构成的句子，$(t_n^i, v_n^i)$形成一个图像-句子对。注意在特定语境中可能缺乏任意模态。</p><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>采用了更通用的自上而下的方法来构建 MESED。即首先构建语义类别和相应的实体，然后依次收集与实体相对应的文本和视觉语境。</p><ol><li>语义类和实体收集</li><li>收集实体标注的句子</li><li>收集相关图片</li><li>图像重新排序</li></ol><h2 id="MultiExpan-方法"><a href="#MultiExpan-方法" class="headerlink" title="MultiExpan 方法"></a>MultiExpan 方法</h2><p>将 Multi-Expan 分成两个步骤：多模态实体表示阶段和实体扩展阶段。</p><p>在第一阶段，我们设计了一个多模态实体级编码器，其输出是候选实体的掩蔽跨度概率分布。实体表示为包含该实体的所有句子的预测实体分布的平均值。我们提出了一个多模态掩蔽实体预测任务和三个辅助自学习预训练任务，以重新完善实体表示。在第二阶段，MultiExpan 会根据实体概率表征的相似性获取目标实体。提出 MultiExpan 的目的是为了提供一个稳健的多模态baseline，并探索不同预训练任务的有效性</p><h3 id="多模态实体表示阶段"><a href="#多模态实体表示阶段" class="headerlink" title="多模态实体表示阶段"></a>多模态实体表示阶段</h3><p><img src="D:\mynotes\source\images\deepLearning\MESED3" alt="多模态实体表示"></p><p>多模态编码器首先利用自注意Transformer分别处理文本和图像，然后将它们结合起来，进行深度跨模态交互。</p><ol><li><p>文本：上下文文本用$BERT_{base}$初始化的12层Transformer编码，获得文本嵌入$\hat{W} = {\hat{w_1}, \hat{w_2}, …, \hat{w_{L_1}} = BERT_{BASE}(T)}$。$L_1$ 是句子中词块的最大长度</p></li><li><p>图像：将每幅图像转换成固定的形状，并确定每个patch的大小，将每幅图像分为 36个patch，然后用ResNet提取patch特征：${v_1, v_2, …, v_{L_2}} = Flat(Resnet(I))$。$L_2$是patch数。 </p><p>由于patch特征会导致分割过程中位置信息的丢失，因此添加了一个可学习的位置嵌入$P = {p_1, p_2, …, p_{L_2}}$来标记每个patch的位置信息。patch特征和位置嵌入都通过成对相加的方式结合起来。</p><p>最后，建立一个 3 层Transformer架构，作为视觉信息处理中的图像编码器。</p></li><li><p>跨模态融合：通过文本特征和视觉特征的组合$concat(\hat{W}, \hat{V})$得到隐藏状态${h_1,h_2,…,h_L}$。然后，我们将其输入一个 3 层Transformer进行交互和融合，使图像-文本对完全对齐：${\hat{h_1},\hat{h_2},…,\hat{h_L}} = Encoder_{cross}({h_1,h_2,…,h_L})$，这里$L=L_1+L_2$</p></li></ol><p>多模态嵌入编码器后面有一个分类 f。在得到掩码位置的隐藏状态后，嵌入向量通过 MLP 和 Softmax 函数转换为掩码实体在可能的候选实体中的概率分布：<br>$$<br>\hat{y}=f(\hat{h}<em>{[MASK]}) = Softmax(MLP(\hat{h}</em>{[MASK]})), \hat{y} \in R^{V_e}<br>$$<br>$V_e$表示候选实体词汇量的大小。</p><h3 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h3><p>在多模态编码器的训练中，四种自监督预训练目标包括：Masked实体预测损失、对比学习损失、聚类学习损失和动量蒸馏损失。最终，多模态编码器对这四个目标进行迭代优化。</p><h4 id="Masked实体预测损失"><a href="#Masked实体预测损失" class="headerlink" title="Masked实体预测损失"></a>Masked实体预测损失</h4><p>$$<br>L_{mask} = - \frac{1}{N} \sum_{i}^{N} \sum_{j}^{V_e} y_i[j] \cdot (1 - \eta) \cdot \log (\hat{y_i}[j]) + (1 - y_i[j]) \cdot \eta \cdot \log(1 - \hat{y_i}[j])<br>$$</p><p>用交叉熵损失计算，$y$表示one-hot向量，$N$表示batch size，$\eta$表示平滑因子，用于防止与目标实体共享序列的实体被过度压制。</p><h4 id="对比学习损失"><a href="#对比学习损失" class="headerlink" title="对比学习损失"></a>对比学习损失</h4><p>对比学习通过拉近相同语义类别实体的代表和拉远不同语义类别实体的</p><p>代表，使语义类别的语义边界更加清晰。</p><p>从上一次迭代中获得的扩展列表中为每个语义类生成正反两方面的实体。排在前$K_{pos}$ 位的实体被定义为正实体，排在$L_{neg}$至$U_{neg}$之间的实体则被视为负实体。来自正/负实体的样本配对形成正/负样本对。对于大小为𝑁 的迷你批次，每个样本$x_{2𝑖− 1}$ 与其他样本形成 $2𝑁−1$对，其中，我们将$x_{2𝑖− 1}，x_{2i}$对视为正，并定义其他 $2𝑁−2$对为负。</p><p>由于直接对隐藏特征$\hat{h}<em>{MASK}$使用对比学习可能导致信息丢失，所以在多模态编码器后面插入了一个双层MLP（用$p</em>{con}(\cdot)$表示）来把隐藏特征映射到正则子空间：$z_i= p_{con}(\hat{h}<em>{MASK})$。相似度计算用点积：<br>$$<br>s(z_i,z_j) = z_i \cdot z_j^T,i,j \in [1, 2N]<br>$$<br>对比学习的损失集中在硬性的负面实体，对于一个样本$z_i$（假设它与$z_j$形成正样本对），则损失为：<br>$$<br>l_i = -\log \frac{e^{s(z_i,z_j)/t}}{e^{s(z_i,z_j)/t} + R_i^-} \<br>R_i^- = max(\frac{-(2N-2) \cdot \iota \cdot e^{s(z_i,z_j)/t} + \tilde{R_i^-}}{1 - \iota^+}, e^{-\frac{1}{t}}) \<br>\tilde{R_i^-} = \frac{(2N-2)\sum</em>{k:k \not= i \not= j} e^{(1+\beta) \cdot s(z_i,z_k)/t}}{sum_{k:k \not= i \not= j} e^{\beta \cdot s(z_i,z_k)/t}} \<br>L_{con} = \sum_{i=1}^{2N} l_i<br>$$</p><h4 id="聚类学习损失"><a href="#聚类学习损失" class="headerlink" title="聚类学习损失"></a>聚类学习损失</h4><p>采用另一种投影头（表示为$p_{clu}$ ），将输入样本$x_i$ 映射到一个语义类子空间，结果是$c_i = p_{clu}(\hat{h}[MASK])$。$c_i$的维数$M$与聚类的数量（即目标语义类别的数量）有关。我们认为，一个语义类别可以通过一批实体对它的概率反应来表征。让$C = [c_1,…,c_{2i-1},…,c_{2N-1}] \in R^{N \times M}$表示样本${x_1,…,x_{2i-1},…,x_{2N-1}}$的类可能分布，$C’ = [c_2,…,c_{2i},…,c_{2N}]$表示样本${x_2,…,x_{2i},…,x_{2N}}$的类可能分布。正聚类对是由矩阵$C$和$C’$的相同列所代表的语义类别形成的，这是因为与这些列向量的每个元素相对应的实体$x_{2𝑖− 1}$ 和$x_{2i}$ 是源自同一语义类别的正样本对。<br>$$<br>\hat{s}(\hat{c_i}, \hat{c_j}) = \hat{c_i^T} \cdot \hat{c_j} \<br>…\text{其他公式和对比损失计算类似} \<br>L_{clu} = \sum_{i=1}^{2M} \hat{l_i}<br>$$</p><h4 id="动量蒸馏损失"><a href="#动量蒸馏损失" class="headerlink" title="动量蒸馏损失"></a>动量蒸馏损失</h4><p>MESED 中的图像-句子对是从网络中收集的，通常伴有噪声，这就导致收集到的图像可能与句子关系不强，或者属于语义类的扩展实体没有包含在地面实况中。为了缓解上述问题，我们引入了动量蒸馏学习。在训练过程中，模型的动量版本会以指数方式缓慢更新。</p><p>移动动量系数$m:\theta_t m \leftarrow \theta_t + (1-m) \theta_s$</p><p>动量模型用于生成伪标签，作为额外的监督，防止学生模型过度拟合噪音。</p><p>动量蒸馏损失用动量模型生成的伪实体概率分布$\tilde{y}$ 和当前一轮多模态编码器的预测结果$\hat{y}$之间的 KL 发散来表示：<br>$$<br>L_mod= - \sum_{i=1}^{m} \tilde{y_i}log(\tilde{y_i}) - \tilde{y_i}log(\hat{y_i})<br>$$</p><h3 id="实体扩展阶段"><a href="#实体扩展阶段" class="headerlink" title="实体扩展阶段"></a>实体扩展阶段</h3><p>实体被表示为包含该实体的所有句子的预测实体分布的平均值。语义类别由当前扩展集实体的加权平均值表示，权重由窗口搜索算法动态保持。这样，具有相似分布的候选实体就会被放置在当前集合中，并以 KL 分歧来衡量。当前集合中的实体数量达到目标规模时，就会执行实体重新排序算法，以完善最终的排序列表。</p><blockquote><p>由于扩展过程不是本文的重点，我们使用了 ProbExpan 中的窗口搜索和实体重新排序算法，在此不再赘述。</p></blockquote><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>比较了三类模型</p><ol><li>传统的基于文本的 ESE 方法：包括SetEx**-** pan、CaSE、CGExpan、ProbExpan和 GPT-3.5。SetExpan 和 CaSE 是传统的基于统计概率的方法，而 CGExpan 和ProbExpan 则是基于预训练语言模型 BERT 的最先进方法。</li><li>基于视觉的模型：VIT、BEIT、ICLIP的图像编码器</li><li>具有不同结构的多模态模型：CLIP、ALBEF</li></ol><p><strong>评价指标</strong> ESE 的目标是根据实体与给定种子实体的相似度，按降序扩展排序实体列表。采用了两种广泛使用的评价指标，即MAP@𝐾 和 P@𝐾。MAP@K计算方式如下：<br>$$<br>MAP@K = \frac{1}{|Q|} \sum_{q \in Q}AP_K(R_q, G_q)<br>$$<br>𝑄是每个查询q的集合，$AP_K(R_q, G_q)$是𝐾与排名列表$R_q$ 和groud truth列表$G_q$ 的平均精度。P@K是前K个实体的精度。</p><h3 id="主要实验"><a href="#主要实验" class="headerlink" title="主要实验"></a>主要实验</h3><p><img src="D:\mynotes\source\images\deepLearning\MESED-table3" alt="image-20250112101526651"></p><ol><li>多模态方法上优于单模态方法。值得注意的是，我们的 MultiExpan（MEP）仅通过使用屏蔽实体预测任务就取得了优异的性能。完整的 MultiExpan 方法取得了最佳的整体性能。此外，完整版的 Multi-Expan也达到了最佳性能。</li><li>就多模态模型的结构而言，ALBEF 和我们的 MultiExpan 通过Transformer实现了深度模态交互，与 CLIP 通过点积相似性计算实现的浅度模态交互相比，后者更适合 ESE 任务。这些结果表明，深度模态交互和融合是未来可以探索的一个方向。</li><li>就基于视觉的模型而言，BEIT 通过对遮蔽图像建模进行预训练，在利用更精细的图像语义如物体和背景形成）方面表现出色。VIT 模型通过对 Image Net 数据集中的图像进行分类来学习整体图像语义，相比之下，BEIT 在实体理解方面取得了更好的效果。同时，CLIP 的图像编码器由于与文本模态相联系，也捕捉到比 VIT 模型更丰富的语义。然而，仅仅依靠图像模式并不足以产生令人满意的结果，文本模式仍然占据主导地位。</li><li>种子数的增加并不一定会带来整体性能的提高。更多的种子可以更精确地描述语义类别，并更安全地检索到一些 “必须正确 “的实体，因此当 K 较小时（=10,20），MAP/P 会有所提高。然而，更多的种子实体意味着更大的语义类搜索空间，这就需要对共同实体属性进行比当前模型更细致的分析。这个问题代表了 ESE 模型一直面临的语义漂移挑战，因此当 K 越大时，MAP/P 就越小。当然，增加 |Seed| 也有助于消除查询中属于多个类别的实体的歧义。例如，在语义类别<em>Light Novel</em>（轻小说）中，一些种子实体也属于 <em>Manga</em>（漫画），增加 |Seed| 在所有指标上平均增益 17.5%。</li><li>GPT-3.5 没有取得令人满意的结果，甚至不如无监督 CGExpan。通过细致GPT-3.5 在特定语义类别上的表现，我们发现该模型在处理复杂类别（如 108 位二战烈士）时表现吃力。我们明确指示 GPT-3.5 首先推理类名，然后根据类名进行扩展。这一修改被命名为 GPT+Name ，与 GPT-3.5 相比有了大幅提升。这种方法与大型语言模型的新兴思维链推理理念一致，即一步一步地思考。我们建议未来的研究探索思维链与 ESE 任务的结合。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/01/09/dl/yolov7-xue-xi-bi-ji/"/>
      <url>/2025/01/09/dl/yolov7-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="yolov7"><a href="#yolov7" class="headerlink" title="yolov7"></a>yolov7</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p>输入input、骨干网络backbone、颈部neck、头部head</p><p><img src="D:\mynotes\source\images\deepLearning\yolov7" alt="yolov7"></p><ol><li>图片经过input部分数据增强等一系列操作进行预处理后，被送入backbone</li><li>backbone对处理后的图片提取特征</li><li>提取到的特征经过 Neck 模块特征融合处理得到大、中、小三种尺寸的特征</li><li>最终，融合后的特征被送入检测头，经过检测之后输出得到结果</li></ol><h2 id="prototype"><a href="#prototype" class="headerlink" title="prototype"></a>prototype</h2><p><a href="https://docs.pingcode.com/ask/187609.html">prototype learning讲解</a></p><p>K均值聚类是原型学习的一个典型示例</p><p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.pdf">https://openaccess.thecvf.com/content/CVPR2021/papers/Zhu_Prototype_Augmentation_and_Self-Supervision_for_Incremental_Learning_CVPR_2021_paper.pdf</a></p><h2 id="trick"><a href="#trick" class="headerlink" title="trick"></a>trick</h2><p>CBAM加一个，在yolov7_backbone处</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">        x0 = self.stage0(x) </span><br><span class="line">        x1 = self.stage1(x0) </span><br><span class="line"></span><br><span class="line">        x2 = self.elan_0(x1)</span><br><span class="line">        x2 = self.cbam_0(x2) #! 添加 CBAM</span><br><span class="line">        </span><br><span class="line">        x3 = self.elan_1(x2)</span><br><span class="line">        # x3 = self.cbam_1(x3) #! 添加 CBAM</span><br><span class="line">        </span><br><span class="line">        x4 = self.elan_2(x3)</span><br><span class="line">        # x4 = self.cbam_2(x4) #! 添加 CBAM</span><br><span class="line">        </span><br><span class="line">        x5 = self.elan_3(x4)</span><br><span class="line">        # x5 = self.cbam_3(x5) #! 添加 CBAM </span><br><span class="line">       </span><br><span class="line">        return x3, x4, x5</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/18/dl/taming-self-training/"/>
      <url>/2024/12/18/dl/taming-self-training/</url>
      
        <content type="html"><![CDATA[<p>环境配置</p><p>18的nvcc –version显示的CUDA版本（10.0）和nvidia-smi版本（12.4）不一样</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n sas_det python=3.8</span><br><span class="line">conda activate sas_det</span><br><span class="line">conda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.3 -c pytorch</span><br><span class="line">conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=10.2 -c pytorch</span><br><span class="line">conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch</span><br><span class="line">git clone https://github.com/facebookresearch/detectron2.git</span><br><span class="line">conda install ninja</span><br><span class="line"></span><br><span class="line">python -m pip install -e detectron2 # 没法改cpp_extension.py来跳过CUDA版本检查（因为代码在/tmp下），采用下面的</span><br><span class="line"></span><br><span class="line">cd detectron2</span><br><span class="line">python setup.py build --force develop</span><br></pre></td></tr></tbody></table></figure><p>报错：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">File "/tmp/pip-build-env-xsd6d80w/overlay/lib/python3.8/site-packages/torch/utils/cpp_extension.py", line 413, in _check_cuda_version</span><br><span class="line">      raise RuntimeError(CUDA_MISMATCH_MESSAGE.format(cuda_str_version, torch.version.cuda))</span><br><span class="line">  RuntimeError:</span><br><span class="line">  The detected CUDA version (10.0) mismatches the version that was used to compile</span><br><span class="line">  PyTorch (12.1). Please make sure to use the same CUDA versions.</span><br></pre></td></tr></tbody></table></figure><p>detection2中添加pyproject.toml文件，内容如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">[build-system]</span><br><span class="line">requires = ["setuptools&gt;=64", "torch"]</span><br><span class="line">build-backend = "setuptools.build_meta"</span><br></pre></td></tr></tbody></table></figure><p>如果你使用的是像pip这样的包管理器，可以使用 <code>pip cache purge</code> 清理缓存。</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pip cache purge</span><br></pre></td></tr></tbody></table></figure><p>编译问题<a href="https://blog.51cto.com/u_15353042/3752099">https://blog.51cto.com/u_15353042/3752099</a></p><p>配置文件修改<a href="https://developer.aliyun.com/article/1626064">https://developer.aliyun.com/article/1626064</a></p><p>跳过CUDA版本检查<a href="https://blog.csdn.net/m0_51516317/article/details/139423784">https://blog.csdn.net/m0_51516317/article/details/139423784</a></p><p>报错：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">In file included from /usr/local/cuda-10.0/include/cuda_runtime.h:83,</span><br><span class="line">                 from &lt;command-line&gt;:</span><br><span class="line">/usr/local/cuda-10.0/include/crt/host_config.h:129:2: error: #error -- unsupported GNU version! gcc versions later than 7 are not supported!</span><br><span class="line">  129 | #error -- unsupported GNU version! gcc versions later than 7 are not supported!</span><br><span class="line">      |  ^~~~~</span><br><span class="line">error: command '/usr/local/cuda-10.0/bin/nvcc' failed with exit code 1</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 10</span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 10</span><br></pre></td></tr></tbody></table></figure><p>可以使用 <code>update-alternatives --config g++</code> 和 <code>update-alternatives --config gcc</code> 来查看和选择你想要使用的 <code>g++</code> 和 <code>gcc</code> 版本</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">sudo yum update</span><br><span class="line">sudo yum install gcc-7 g++-7</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/13/lun-wen-yue-du/instrucblip-lun-wen-bi-ji/"/>
      <url>/2024/12/13/lun-wen-yue-du/instrucblip-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>大规模的预训练和指令调整已经成功地创建了具有广泛能力的通用<strong>语言模型</strong>。然而 ，由于额外的视觉输入带来了丰富的输入分布和任务多样性，建立通用视觉语言模型具有挑战性。尽管视觉语言预训练已被广泛研究，但<strong>视觉语言指令调整仍未得到充分探索</strong>。</p><p>为了应对上述挑战，本文提出了一个视觉语言指令调整框架–InstructBLIP，使通用模型能够通过统一的自然语言接口解决各种视觉语言任务。InstructBLIP 使用不同的指令数据集来训练多模态 LLM。具体来说，使用预先训练好的 BLIP-2 模型进行初始化训练，该模型由一个图像编码器、一个 LLM  和一个查询转换器（Q-Former）组成，是二者的桥梁。在指令调整过程中，保持图像编码器和  LLM 不变，对 Q-Former 进行微调。</p><p>关键：</p><ol><li>将 26 个数据集转换为指令调整格式，并将其分为 11 个任务类别。</li><li>使用 13 个held in数据集进行指令调整，并使用 13 个held out数据集进行零样本评估。</li><li>提出了 “指令感知视觉特征提取”，具体来说，文本指令不仅提供给冻结的 LLM，也提供给 Q-Former，这样它就能从冻结 的图像编码器中提取指令感知的视觉特征。</li></ol><blockquote><p>held-in evaluation：内部评估，使用同一数据集进行训练和评估，通常采用交叉验证技术将数据集划分为训练集和验证集。模型会在训练集上进行训练，并使用验证集上的数据进行评估和调优。可能导致过度拟合。<br>held-out evaluation：外部评估，使用独立的、未参与训练的数据集进行评估的。测试泛化能力。</p></blockquote><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>收集了26个公开的视觉语言数据集，，并将它们转换成指令调整格式，涵盖11个任务分类</p><p><img src="D:\mynotes\source\images\image-20241213105002407.png" alt="image-20241213105002407"></p><p>对于每项任务，都用自然语言精心制作 10 到 15 个不同的指令模板。这些模板是构建指令调整数据的基础，其中阐明了任务和目标。对于本质上倾向于<strong>简短回答</strong>的公共数据集，在一些相应的指令模板中使用了<code>short</code>和<code>briefly</code>等术语，以<strong>降低模型过度拟合</strong>到总是生成简短输出的风险。</p><h3 id="训练与评估"><a href="#训练与评估" class="headerlink" title="训练与评估"></a>训练与评估</h3><p>26个数据集分为13个held in（黄），13个held out（白）。</p><p>使用held in数据集的训练集进行指令调整，并使用其验证集或测试集进行held in评估。</p><p>held out数据集分两种：</p><ol><li>模型在训练过程中未接触到的数据集，但其任务存在于held in cluster中；</li><li>在训练过程中完全未见的数据集及其相关任务。</li></ol><p>由于held in数据集和held out数据集之间的数据分布会发生变化， 因此处理第一种类型的held out评估并非易事。对于第二种类型，我们完全保留了几项任务，包括 visual reasoning, video question answering, visual conversational QA和image classification</p><h3 id="指令感知视觉特征提取"><a href="#指令感知视觉特征提取" class="headerlink" title="指令感知视觉特征提取"></a>指令感知视觉特征提取</h3><p>包括 BLIP-2 在内的现有零样本 图像到文本 生成方法在提取视觉特征时采用了与指令<strong>无关</strong>的方法。这 就导致无论任务是什么，都会有一组静态的视觉表征被输入到 LLM 中。相比之下，指令感知视觉模型可以<strong>适应</strong>任务指令，生成最有利于手头任务的视觉表征。如果我们预计同一输入图像的任务指令会有很大变化，那么这显然是有利的。</p><p><img src="D:\mynotes\source\images\deepLearning\InstructBLIP.png" alt="image-20241213110132976"></p><p>InstructBLIP 利用 Q Former从冻结的图像编码器中提取视觉特征。Q-Former 的输入包含一组 K 个可学习的查询embeddings， 通过交叉注意与图像编码器的输出进行交互。Q-Former 的输出由 K 个编码的视觉向量组成，每个查询embedding一个，然后经过线性投影，送到冻结的 LLM。</p><p>与 BLIP-2 一样，Q-Former在指令调整前分两个阶段使用image-caption数据进行预训练（第一阶段使用冻结图像编码器对 Q-Former 进行预训练，以进行视觉语言表征学习 ；第二阶段利用冻结的 LLM 将 Q-Former 的输出调整为文本生成的软视觉提示）在预训练之后，我们通过指令调整对 Q-Former 进行微调，其中 LLM 接收来自 Q-Former 的视觉编码和任务指令作为输入。</p><p>在扩展 BLIP-2 的基础上，InstructBLIP 提出了一个指令感知 Q-former模块，该模块将指令文本标记作为额外输入。指令通过 Q-Former的自注意力层与查询embedding进行交互，并鼓励提取与任务相关的图特征。因此，LLM 接收到了有利于指令跟踪的视觉信息。</p><blockquote><p>我们通过经验证明（表 2），指令感知的视觉特征提取可以大幅提高 held in和 held out评估的性能</p></blockquote><h3 id="平衡训练数据集"><a href="#平衡训练数据集" class="headerlink" title="平衡训练数据集"></a>平衡训练数据集</h3><p>问题：由于训练数据集数量庞大，且每个数据集的大小差异显著，将它们均匀混合可能会导致模型对较小数据集的拟合过度，而对较大数据集的拟合不足。</p><p>解决办法：对数据集进行采样，采样概率与数据集大小或训练样本数量的平方根成正比。一般来说，给定 D 个数据集，其大小为 ${S1 , S2 , … ., SD }$，即在训练过程中从数据集 d 中选择数据样本的概率 为 $p_d = \frac{\sqrt{S_d}}{\sum_{i=1}^{D} \sqrt{S_i}}$。在此公式的基础上，我们对某些参数的权重进行了手动调整。 这是因为数据集和任务之间存在固有的差异，尽管数据集和任务的规模相似，但 却需要不同程度的训练强度。</p><p>具体来说，降低了以多选题为特色的 A-OKVQA 的权重，增加了需要生成开放式文本的 OKVQA 的权重。表 2 显示，平衡数据集抽样策略提高了保持评估和保持归纳的 整体性能。</p><h3 id="推理方法"><a href="#推理方法" class="headerlink" title="推理方法"></a>推理方法</h3><p>采用了两种略有不同的生成方法对不同的数据集进行评估。</p><ol><li>对于大多数数据集， 如 image captioning，open-ended VQA ，<strong>直接使用经过指令调整的模型生成回答</strong>，然后将其与ground truth进行比较以计算指标。</li><li>对于 classification 和 multi-choice VQA 任务，沿用了以前的研究成果 (如ALBEF）， 采用了词汇排序法：仍然促使模型生成答案，但将其词汇量限制在候选列表中；然后计算每个候选词的对数似然，并<strong>选择数值最大的一个作为最终预测结果</strong>。这种排序方法适用于   ScienceQA、IconQA、A-OKVQA（多选）、HatefulMemes、Visual Dialog、MSVD 和 MSRVTT 数据集 。</li></ol><h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><ol><li>结构：得益于 BLIP-2 模块化架构设计所带来的灵活性，我们可以快速调整模型以适应各种 LLM。在 实验中，我们采用了 BLIP-2 的<strong>四个变体</strong>，它们具有<strong>相同的图像编码器</strong>（ViT-g/14 ），但<strong>冻结的 LLM 不同</strong>，包括 FlanT5- XL (3B)、FlanT5-XXL (11B)、Vicuna-7B 和 Vicuna-13B。FlanT5 [7] 是基于 编码器-解码器变换器 T5 [34] 的指令调整模型。另一方面，Vicuna[2] 是最近从 LLaMA[41] 发布的纯 解码器变换器指令调整模型。在视觉语言指令调整过程中，从预先训练好的 BLIP-2 checkpoint初始化模型，只对 Q-Former 的参数进行微调，同时冻结图像编码器和 LLM。由于原始 BLIP-2 模型不包括 Vicuna 的检查点，因此我们使用与 BLIP-2 相同的程序对 Vicuna 进行预训练。 </li><li>训练和超参数。使用 LAVIS 库 进行实施、训练和评估。所有模型都经过最多 60K 步的指令调整，每 3K 步验证一次模型的性能。对于每个模型，我们都会选择一个最佳checkpoint，并在所有数据 集上进行评估。对 3B、7B 和 11/13B 模型分别采用了 192、128 和 64 的批量大小。使用 Adam优化器，$\beta_1 =  0.9，\beta_2 =  0.999$，权重衰减为 0.05。此外，在最初的 1,000 步中对学习率进行线性预热，从 $10^{-8}$增加到 $10^{-5}$，然后进行余弦衰减，最低学习率为 0。所有模型均使用 16 个 Nvidia  A100 (40G) GPU 进行训练，并在 1.5 天内完成。</li></ol><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="零样本评估"><a href="#零样本评估" class="headerlink" title="零样本评估"></a>零样本评估</h3><p><img src="D:\mynotes\source\images\image-20241213181719964.png" alt="image-20241213181719964"></p><h3 id="指令调整vs多任务学习"><a href="#指令调整vs多任务学习" class="headerlink" title="指令调整vs多任务学习"></a>指令调整vs多任务学习</h3><p>多任务学习与指令调整类似，是一种广泛使用的方法，涉及同时训练多个数据集，目的是提高每个数据集的性能。为了研究在指令调整中观察到的zero-shot泛化的改善是主要来自于指令的格式化 还是仅仅来自于多任务学习，我们在相同的训练设置下对这两种方法进行了比较分析。</p><p><img src="D:\mynotes\source\images\image-20241213191314798.png" alt="image-20241213191314798"></p><p>指令调整和多任务学习在保留数据集上表现出相似的性能。这表明，只要模型经过此类数据的训练，就能很好地适应这两种不同的输入模式。另一方面，在不可见的保留数据集上，指令调整比多任务学习有显著提高，而多任务学习的性能仍与原始 BLIP-2 相当。这表明，指令调整是提高模型zero-shot泛化能力的关键。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/11/dl/xiang-mu-bian-yi-cheng-pyc-yun-xing-liu-cheng/"/>
      <url>/2024/12/11/dl/xiang-mu-bian-yi-cheng-pyc-yun-xing-liu-cheng/</url>
      
        <content type="html"><![CDATA[<ol><li><p>使用<code>comileall</code>编译<code>oadp/dp</code>下的所有文件</p><p>例如：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> compileall <span class="keyword">as</span> ca</span><br><span class="line"></span><br><span class="line">ca.compile_dir(<span class="string">r'oadp/dp'</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>需要删掉/移除到原本的py文件，并将<code>dp/__pycache__</code>下的所有文件移到dp/下，并重新命名，去掉文件名中的<code>cpython-310</code></p></li></ol><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rename_files_in_directory</span>(<span class="params">directory_path</span>):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(directory_path):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">".cpython-310"</span> <span class="keyword">in</span> filename:</span><br><span class="line">            new_filename = filename.replace(<span class="string">".cpython-310"</span>, <span class="string">""</span>)</span><br><span class="line">            old_file_path = os.path.join(directory_path, filename)</span><br><span class="line">            new_file_path = os.path.join(directory_path, new_filename)</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                os.rename(old_file_path, new_file_path)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f"Renamed: <span class="subst">{filename}</span> -&gt; <span class="subst">{new_filename}</span>"</span>)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f"Error renaming <span class="subst">{filename}</span>: <span class="subst">{e}</span>"</span>)</span><br><span class="line"></span><br><span class="line">directory_path = <span class="string">"oadp/dp/"</span></span><br><span class="line"></span><br><span class="line">rename_files_in_directory(directory_path)</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li>运行命令和之前一样，例如<code>torchrun --nproc_per_node=1 -m oadp.dp.train oadp_ori_ov_coco configs/dp/oadp_ov_coco.py</code></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/11/compiler/zong-jie-gan-xiang/"/>
      <url>/2024/12/11/compiler/zong-jie-gan-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="总结感想"><a href="#总结感想" class="headerlink" title="总结感想"></a>总结感想</h1><p>经过一学期编译实验的磨练，深刻领悟了第一次实验课上老师反复提起的“很难”。</p><p>最开始的词法分析、语法分析还算平和，语义分析也能够应付。等开始中间代码生成的时候简直痛不欲生，生成llvm我就花了整整一周时间，最开始理解llvm的指令都费劲（尤其是getelementptr的用法），然后去翻了很多资料，看了讲座、教程，甚至去翻了翻llvm的官方文档，终于有了点眉目，逐渐理清了llvm的结构，不得不说“一切皆Value”的设计很巧妙，但是代码量也十分恐怖。后来想尝试着生成mips再做做优化，但是生成目标代码又花了快一周的时间（嗯感觉自己效率真的很低），也de了很久的bug，加上快学期末了各种大作业，很多事情堆到了一起，所以即便感觉自己还比较早开始了代码生成的作业，但是留给代码优化的时间仍然很少，所以我只做了一些简单的优化。</p><p>总得来说，自己搭建一个编译器加深了我对编译的理解，看着自己几千行的代码真的感概良多，编译真是一件复杂的事情，完成编译实验的道路困难重重，但也算坚持到了最后，比较遗憾的是感觉时间真的很紧，这学期的事情也真的很多，没有办法去细细琢磨优化的事情了。最后，感谢助教们的答疑还有录制的讲座、ppt，对我的帮助很大！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/08/db/shuang-token/"/>
      <url>/2024/12/08/db/shuang-token/</url>
      
        <content type="html"><![CDATA[<h2 id="用户安全"><a href="#用户安全" class="headerlink" title="用户安全"></a>用户安全</h2><h3 id="密码加密存储"><a href="#密码加密存储" class="headerlink" title="密码加密存储"></a>密码加密存储</h3><p>用户的密码经过哈希加密后存储在数据库中，关键代码如下</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">String</span> <span class="variable">gensalt</span> <span class="operator">=</span> BCrypt.gensalt();<span class="comment">// 生成盐值，用于加密密码</span></span><br><span class="line"><span class="type">String</span> <span class="variable">hashpw</span> <span class="operator">=</span> BCrypt.hashpw(bUser.getPassword(), gensalt); <span class="comment">// 使用生成的盐值对用户输入的密码进行哈希加密</span></span><br><span class="line">bUser.setPassword(hashpw);</span><br></pre></td></tr></tbody></table></figure><h3 id="使用双token机制"><a href="#使用双token机制" class="headerlink" title="使用双token机制"></a>使用双token机制</h3><p>双Token机制使用两种令牌（即Token）来管理用户的身份验证和授权过程： assess token 和 refresh token。assess token是短期有效的令牌，用于授权用户访问受保护的资源。每次用户请求时，assess token会携带在请求的头部（<code>Authorization</code>头）进行身份验证。它的有效期通常比较短，因此能降低令牌被盗用后的风险。refresh token长期有效，用于获取新的assess token。当assess token过期时，客户端可以使用refresh token向认证服务器请求新的assess token。refresh token通常会有较长的有效期（例如几天、几周或几个月），并且只在用户的登录会话中使用。它不会频繁地发送到服务器，只在需要刷新assess token时使用。</p><p>优点：</p><ul><li>提高安全性：：assess token即使被盗用，攻击者能够使用它的时间也非常有限。过期后，用户需要使用refresh token刷新assess token，从而限制了assess token的暴露风险。</li><li>改善用户体验：在assess token过期时，客户端可以使用refresh token自动刷新assess token，而无需用户重新登录，提供了无缝的体验。同时，只要refresh token有效，用户就可以在不干扰操作的情况下保持登录状态，减少登录频率。</li><li>提高系统性能、减少身份验证负担</li><li>灵活、可扩展性</li></ul><h3 id="关键代码"><a href="#关键代码" class="headerlink" title="关键代码"></a>关键代码</h3><p>利用了Spring Security做安全访问控制，相关配置在<code>config/SecurityConfig</code>下：</p><p><img src="D:\mynotes\source\images\image-20241210113901969.png" alt="image-20241210113901969"></p><p>具体流程如下：</p><h4 id="注册请求"><a href="#注册请求" class="headerlink" title="注册请求"></a>注册请求</h4><ol><li>用户提交注册请求。</li><li><code>BUserController</code>的<code>register</code>方法处理请求，保存用户信息到数据库。</li></ol><h4 id="登录请求"><a href="#登录请求" class="headerlink" title="登录请求"></a>登录请求</h4><ol><li>用户提交登录表单，表单数据会被发送到 <code>SecurityConfig</code> 中配置的 <code>loginProcessingUrl</code>。</li><li>Spring Security 会使用配置的 <code>UserDetailsService</code> 加载用户信息。</li><li>Spring Security 会使用配置的 <code>BCryptPasswordEncoder</code> 验证用户密码。</li><li>如果验证成功，Spring Security 调用 <code>LoginSuccessHandler</code> 中的 <code>onAuthenticationSuccess</code> 方法，生成<code>accessToken</code>和<code>refreshToken</code>，并返回给前端。</li><li>前端保存<code>accessToken</code>和<code>refreshToken</code>。</li></ol><h4 id="其他请求"><a href="#其他请求" class="headerlink" title="其他请求"></a>其他请求</h4><ol><li>用户提交请求，携带<code>accessToken</code>。</li><li><code>RequestAuthenticationFilter</code>拦截请求，验证<code>accessToken</code>。</li><li>如果<code>accessToken</code>有效，解析token并设置用户的认证信息。</li><li>如果<code>accessToken</code>过期，使用<code>refreshToken</code>生成新的<code>accessToken</code>和<code>refreshToken</code>，并保存到Redis中。</li><li>请求继续执行，返回结果。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/05/db/bug-ji-lu/"/>
      <url>/2024/12/05/db/bug-ji-lu/</url>
      
        <content type="html"><![CDATA[<p>git经常出现fatal error的问题：参考这个<a href="https://blog.csdn.net/Greenhand_BN/article/details/135881590">https://blog.csdn.net/Greenhand_BN/article/details/135881590</a></p><h2 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h2><ol><li><p>controller层某个api方法里service类为null：该方法被设置成了private，应改为public</p></li><li><p>IPage功能不生效</p><p>原因：没有配置文件</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="meta">@EnableTransactionManagement</span></span><br><span class="line"><span class="meta">@MapperScan( { "com.example.dorm.entity" })</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MybatisPlusConfig</span> {</span><br><span class="line"> </span><br><span class="line">   <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> MybatisPlusInterceptor <span class="title function_">mybatisPlusInterceptor</span><span class="params">()</span> {</span><br><span class="line">        <span class="type">MybatisPlusInterceptor</span> <span class="variable">mybatisPlusInterceptor</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MybatisPlusInterceptor</span>();</span><br><span class="line">        mybatisPlusInterceptor.addInnerInterceptor(<span class="keyword">new</span> <span class="title class_">PaginationInnerInterceptor</span>(DbType.MYSQL));</span><br><span class="line">        <span class="keyword">return</span> mybatisPlusInterceptor;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li><li><p>Result&lt;?&gt;：?不要写出其他的!!</p></li></ol><h2 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h2><ol><li>导入了奇怪的依赖包，导致出现如下报错：</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">vue-router.js?v=1cf80410:44 [Vue Router warn]: uncaught error during route navigation:...</span><br><span class="line">warn @ vue-router.js?v=1cf80410:44</span><br><span class="line">vue-router.js?v=1cf80410:2581 SyntaxError: Identifier '__vite__injectQuery' has already been declared (at ${mod.id}:40653:1)...</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li><p>el-upload onchange里的函数被调用两次</p><p>原因：未关闭auto-upload，前面要有”:”！</p></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">&lt;el-upload class="upload-demo" drag accept=".xls, .xlsx, .xml"</span><br><span class="line">    :on-change="handleFileUpload"</span><br><span class="line">    :auto-upload="false"&gt;</span><br><span class="line">    &lt;i class="el-icon-upload"&gt;&lt;/i&gt;</span><br><span class="line">    &lt;div class="el-upload__text"&gt;重新导入宿管信息，将文件拖到此处，或&lt;em&gt;点击上传&lt;/em&gt;&lt;/div&gt;</span><br><span class="line">&lt;/el-upload&gt;</span><br></pre></td></tr></tbody></table></figure><blockquote><p>在使用 el-upload上传文件时，on-change会触发二次事件，一次是ready状态，一次是success状态</p></blockquote><ol start="3"><li><p>GET类型的api传参要用params而不是data</p><p>原因：<code>GET</code> 请求参数应该通过 URL 查询字符串传递，而不是放在请求体（<code>data</code>）中。应该使用 <code>params</code> 来传递 URL 查询参数，而不是使用 <code>data</code></p><figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">const</span> <span class="title function_">getDormManagers</span> = (<span class="params">{pageNum, pageSize, search}</span>) =&gt; {</span><br><span class="line">    <span class="keyword">return</span> <span class="title function_">http</span>({</span><br><span class="line">        <span class="attr">url</span>: <span class="string">'/manager/get-managers'</span>,</span><br><span class="line">        <span class="attr">method</span>: <span class="string">'GET'</span>,</span><br><span class="line">        <span class="attr">headers</span>: {</span><br><span class="line">            <span class="string">'Content-Type'</span>: <span class="string">'application/json'</span></span><br><span class="line">        },</span><br><span class="line">        <span class="attr">params</span>: {</span><br><span class="line">            pageNum,</span><br><span class="line">            pageSize,</span><br><span class="line">            search</span><br><span class="line">        }</span><br><span class="line">    })</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure></li><li><p>网页显示 [object Promise]</p><p>原因：同步异步问题，如下方式不对，这是异步调用</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">getStudentById(id).then(res =&gt; {</span><br><span class="line">...</span><br><span class="line">})</span><br><span class="line">//改成：</span><br><span class="line">const res = await getStudentById(id);</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p>mysql的触发器游标使用问题</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/03/db/xue-xi-ji-lu/"/>
      <url>/2024/12/03/db/xue-xi-ji-lu/</url>
      
        <content type="html"><![CDATA[<p>请帮我在后端实现如下api， 要求：  </p><ol><li>只在Controller层完成</li><li>最好使用IService里的方法</li><li>用中文回答</li></ol><p>请把上述事物流程全部改掉，写出两个事务，要求如下：</p><ol><li>当插入新的公告、报修、调宿申请时，notify表中增加一条消息</li><li>当插入一条新的notify时，根据notify的type：<ul><li>若为ANNOUNCEMENT，则找到notify的targetId对应的公告，根据公告的notifyObject字段：如果为0，则通知每个学生；如果为其他正整数，则通知对应公寓的所有学生</li><li>若为REPAIR或者ADJUST_ROOM，则根据notify的senderUsername在students表中找到对应学生，然后根据该学生的dormId字段，通知该学生对应dormId的所有宿管</li></ul></li></ol><h2 id="双token机制"><a href="#双token机制" class="headerlink" title="双token机制"></a>双token机制</h2><p> <a href="https://zhuanlan.zhihu.com/p/699850219">https://zhuanlan.zhihu.com/p/699850219</a></p><p><a href="https://blog.csdn.net/tzyaaaaaa/article/details/140934877">https://blog.csdn.net/tzyaaaaaa/article/details/140934877</a> 简答易懂的springsecurity认证流程！</p><p>该项目通过以下步骤实现了双token机制：</p><h3 id="1-详细解释每个文件的代码含义"><a href="#1-详细解释每个文件的代码含义" class="headerlink" title="1. 详细解释每个文件的代码含义"></a>1. 详细解释每个文件的代码含义</h3><h4 id="LoginSuccessHandler-java"><a href="#LoginSuccessHandler-java" class="headerlink" title="LoginSuccessHandler.java"></a><code>LoginSuccessHandler.java</code></h4><ul><li><strong>功能</strong>：处理用户登录成功后的操作。</li><li><strong>代码含义</strong>：<ul><li>获取登录用户信息。</li><li>生成<code>accessToken</code>和<code>refreshToken</code>。</li><li>将生成的token通过<code>ResponseResult.write</code>方法返回给前端。</li></ul></li></ul><h4 id="RequestAuthenticationFilter-java"><a href="#RequestAuthenticationFilter-java" class="headerlink" title="RequestAuthenticationFilter.java"></a><code>RequestAuthenticationFilter.java</code></h4><ul><li><strong>功能</strong>：过滤和验证每个请求的token。</li><li><strong>代码含义</strong>：<ul><li>从请求头或请求参数中获取<code>accessToken</code>。</li><li>如果<code>accessToken</code>存在且有效，解析token并设置用户的认证信息。</li><li>如果<code>accessToken</code>过期，尝试从请求头中获取<code>refreshToken</code>，并使用<code>refreshToken</code>生成新的<code>accessToken</code>和<code>refreshToken</code>，然后将新的token保存到Redis中。</li></ul></li></ul><h4 id="JwtUtils-java"><a href="#JwtUtils-java" class="headerlink" title="JwtUtils.java"></a><code>JwtUtils.java</code></h4><ul><li><strong>功能</strong>：生成和解析JWT（JSON Web Token）。</li><li><strong>代码含义</strong>：<ul><li><code>generateToken</code>方法：使用私钥生成JWT。</li><li><code>getUsernameFromToken</code>方法：使用公钥解析JWT并获取用户名。</li></ul></li></ul><h4 id="RsaUtils-java"><a href="#RsaUtils-java" class="headerlink" title="RsaUtils.java"></a><code>RsaUtils.java</code></h4><ul><li><strong>功能</strong>：处理RSA加密和解密。</li><li><strong>代码含义</strong>：<ul><li>生成公钥和私钥文件。</li><li>从文件中读取公钥和私钥。</li></ul></li></ul><h4 id="ResponseResult-java"><a href="#ResponseResult-java" class="headerlink" title="ResponseResult.java"></a><code>ResponseResult.java</code></h4><ul><li><strong>功能</strong>：统一返回结果的工具类。</li><li><strong>代码含义</strong>：<ul><li>提供了多种静态方法，用于生成不同类型的响应结果。</li><li><code>write</code>方法：将响应结果写入<code>HttpServletResponse</code>。</li></ul></li></ul><h4 id="SecurityConfig-java"><a href="#SecurityConfig-java" class="headerlink" title="SecurityConfig.java"></a><code>SecurityConfig.java</code></h4><ul><li><strong>功能</strong>：Spring Security的核心配置。</li><li><strong>代码含义</strong>：<ul><li>配置了自定义的登录成功处理器<code>LoginSuccessHandler</code>。</li><li>配置了自定义的请求过滤器<code>RequestAuthenticationFilter</code>。</li></ul></li></ul><h4 id="BUserController-java"><a href="#BUserController-java" class="headerlink" title="BUserController.java"></a><code>BUserController.java</code></h4><ul><li><strong>功能</strong>：用户相关的前端控制器。</li><li><strong>代码含义</strong>：<ul><li><code>addUser</code>方法：处理用户注册请求，保存用户信息。</li></ul></li></ul><h4 id="UserDetailServiceImpl-java"><a href="#UserDetailServiceImpl-java" class="headerlink" title="UserDetailServiceImpl.java"></a><code>UserDetailServiceImpl.java</code></h4><ul><li><strong>功能</strong>：从数据库查询登录用户信息及其权限。</li><li><strong>代码含义</strong>：<ul><li><code>loadUserByUsername</code>方法：根据用户名查询用户信息，并返回<code>UserDetails</code>对象。</li></ul></li></ul><p>在 Spring Security 中，用户提交登录表单后，身份验证的过程是由 Spring Security 自动处理的。具体来说，Spring Security 会使用配置的 <code>UserDetailsService</code> 和 <code>BCryptPasswordEncoder</code> 来验证用户的用户名和密码。如果验证成功，Spring Security 会调用 <code>LoginSuccessHandler</code> 中的 <code>onAuthenticationSuccess</code> 方法。</p><p>以下是身份验证的关键步骤：</p><ol><li>用户提交登录表单，表单数据会被发送到 <code>SecurityConfig</code> 中配置的 <code>loginProcessingUrl</code>。</li><li>Spring Security 会使用配置的 <code>UserDetailsService</code> 加载用户信息。</li><li>Spring Security 会使用配置的 <code>BCryptPasswordEncoder</code> 验证用户密码。</li><li>如果验证成功，Spring Security 会调用 <code>LoginSuccessHandler</code> 中的 <code>onAuthenticationSuccess</code> 方法。</li></ol><p>在这个配置中，<code>/login</code> URL 会被 Spring Security 拦截并处理，<code>UserDetailsServiceImpl</code> 会加载用户信息，<code>BCryptPasswordEncoder</code> 会验证密码。如果验证成功，<code>LoginSuccessHandler</code> 会被调用。</p><p>AuthorityUtils.commaSeparatedStringToAuthorityList(auths)方法将权限字符串”echoes”转换为GrantedAuthority对象的列表，供Spring Security使用</p><h4 id="RefreshTokenAspect-java"><a href="#RefreshTokenAspect-java" class="headerlink" title="RefreshTokenAspect.java"></a><code>RefreshTokenAspect.java</code></h4><ul><li><strong>功能</strong>：AOP切面，处理<strong>token的刷新</strong>。</li><li><strong>代码含义</strong>：<ul><li><code>addToken</code>方法：在控制器方法执行前后，检查并刷新token。</li></ul></li></ul><h3 id="RefreshTokenAspect-java-和-RequestAuthenticationFilter-java的区别"><a href="#RefreshTokenAspect-java-和-RequestAuthenticationFilter-java的区别" class="headerlink" title="RefreshTokenAspect.java 和 RequestAuthenticationFilter.java的区别"></a><code>RefreshTokenAspect.java</code> 和 <code>RequestAuthenticationFilter.java</code>的区别</h3><ol><li><p>**<code>RefreshTokenAspect.java</code>**：</p><ul><li><strong>作用</strong>：这是一个AOP切面，用于在控制器方法执行后，检查并添加新的访问令牌（access token）和刷新令牌（refresh token）。它会从Redis或请求头中读取token，并将其添加到响应结果中。</li><li><strong>应用场景</strong>：当用户成功登录或刷新token时，系统会在响应中返回新的access token和refresh token。</li><li><strong>请求例子</strong>：用户登录成功后，系统会返回一个包含新的access token和refresh token的响应。</li></ul></li><li><p>**<code>RequestAuthenticationFilter.java</code>**：</p><ul><li><strong>作用</strong>：这是一个过滤器，用于在每个请求到达服务器时，验证请求中的access token。如果access token过期，则尝试使用refresh token生成新的access token和refresh token，并将其保存到Redis中。</li><li><strong>应用场景</strong>：当用户发起需要身份验证的请求时，系统会检查请求中的access token，如果过期则使用refresh token重新生成token。</li><li><strong>请求例子</strong>：用户访问受保护的资源时，系统会检查请求头中的access token，如果过期则使用refresh token重新生成并返回新的token。</li></ul></li></ol><p>通过这两个文件的配合，系统能够在每次请求时验证用户身份，并在必要时刷新token，确保用户的会话持续有效。</p><h3 id="2-详细解释各文件之间的关系"><a href="#2-详细解释各文件之间的关系" class="headerlink" title="2. 详细解释各文件之间的关系"></a>2. 详细解释各文件之间的关系</h3><ul><li><code>SecurityConfig</code>配置了<code>LoginSuccessHandler</code>和<code>RequestAuthenticationFilter</code>，确保登录成功后生成token，并在每个请求中验证token。</li><li><code>LoginSuccessHandler</code>在用户登录成功后生成<code>accessToken</code>和<code>refreshToken</code>。</li><li><code>RequestAuthenticationFilter</code>在每个请求中验证<code>accessToken</code>，如果过期则使用<code>refreshToken</code>生成新的token。</li><li><code>JwtUtils</code>和<code>RsaUtils</code>提供了生成和解析JWT的工具方法。</li><li><code>ResponseResult</code>统一了返回结果的格式。</li><li><code>BUserController</code>处理用户注册请求。</li><li><code>UserDetailServiceImpl</code>从数据库查询用户信息。</li><li><code>RefreshTokenAspect</code>在控制器方法执行前后，检查并刷新token。</li></ul><h3 id="3-解释在登录、注册等请求发生时，该项目的工作流是怎么样的"><a href="#3-解释在登录、注册等请求发生时，该项目的工作流是怎么样的" class="headerlink" title="3. 解释在登录、注册等请求发生时，该项目的工作流是怎么样的"></a>3. 解释在登录、注册等请求发生时，该项目的工作流是怎么样的</h3><h4 id="登录请求"><a href="#登录请求" class="headerlink" title="登录请求"></a>登录请求</h4><ol><li>用户提交登录请求。</li><li><code>SecurityConfig</code>配置的登录处理器拦截请求。</li><li>用户认证成功后，<code>LoginSuccessHandler</code>生成<code>accessToken</code>和<code>refreshToken</code>，并返回给前端。</li><li>前端保存<code>accessToken</code>和<code>refreshToken</code>。</li></ol><h4 id="注册请求"><a href="#注册请求" class="headerlink" title="注册请求"></a>注册请求</h4><ol><li>用户提交注册请求。</li><li><code>BUserController</code>的<code>addUser</code>方法处理请求，保存用户信息到数据库。</li></ol><h4 id="其他请求"><a href="#其他请求" class="headerlink" title="其他请求"></a>其他请求</h4><ol><li>用户提交请求，携带<code>accessToken</code>。</li><li><code>RequestAuthenticationFilter</code>拦截请求，验证<code>accessToken</code>。</li><li>如果<code>accessToken</code>有效，解析token并设置用户的认证信息。</li><li>如果<code>accessToken</code>过期，使用<code>refreshToken</code>生成新的<code>accessToken</code>和<code>refreshToken</code>，并保存到Redis中。</li><li>请求继续执行，返回结果。</li></ol><h3 id="4-扩展其他相关解释"><a href="#4-扩展其他相关解释" class="headerlink" title="4. 扩展其他相关解释"></a>4. 扩展其他相关解释</h3><h4 id="Redis的使用"><a href="#Redis的使用" class="headerlink" title="Redis的使用"></a>Redis的使用</h4><ul><li>Redis用于存储和管理<code>accessToken</code>和<code>refreshToken</code>，确保在<code>accessToken</code>过期后可以通过<code>refreshToken</code>重新生成新的<code>accessToken</code>。</li></ul><h4 id="AOP切面的使用"><a href="#AOP切面的使用" class="headerlink" title="AOP切面的使用"></a>AOP切面的使用</h4><ul><li><code>RefreshTokenAspect</code>通过AOP切面，在控制器方法执行前后，检查并刷新token，确保用户在使用过程中不会因为token过期而中断。</li></ul><p>通过上述机制，该项目实现了双token机制，确保在<code>accessToken</code>过期后可以通过<code>refreshToken</code>重新生成新的<code>accessToken</code>，从而提高了系统的安全性和用户体验。</p><h3 id="Column和-TableField"><a href="#Column和-TableField" class="headerlink" title="@Column和@TableField"></a>@Column和@TableField</h3><p>@Column用在jpa框架</p><p>@TableField用在MyBatis-Plus</p><h2 id="Slf4j"><a href="#Slf4j" class="headerlink" title="@Slf4j"></a>@Slf4j</h2><p><a href="https://www.jb51.net/program/322645laz.htm">https://www.jb51.net/program/322645laz.htm</a></p><blockquote><p>我一般使用的是<code>@SLf4j</code>作为日志输出。</p><p>使用logback.xml，需要引入下面两个依赖。</p><p>如果你使用的是springboot，那么springboot的starter会自带这两个依赖，不用再重复导入。</p></blockquote><h2 id="IService"><a href="#IService" class="headerlink" title="IService"></a>IService</h2><p><code>Service</code> 中的方法通常是业务逻辑层的方法，用于处理具体的业务需求。以下是一些常见的 <code>Service</code> 方法：</p><ol><li><p><strong>保存数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">save</span><span class="params">(T entity)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>批量保存数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">saveBatch</span><span class="params">(Collection&lt;T&gt; entityList)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>保存或更新数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">saveOrUpdate</span><span class="params">(T entity)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>批量保存或更新数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">saveOrUpdateBatch</span><span class="params">(Collection&lt;T&gt; entityList)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据ID删除数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">removeById</span><span class="params">(Serializable id)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据条件删除数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">remove</span><span class="params">(Wrapper&lt;T&gt; queryWrapper)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>批量删除数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">removeByIds</span><span class="params">(Collection&lt;? extends Serializable&gt; idList)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据ID更新数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">updateById</span><span class="params">(T entity)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据条件更新数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">update</span><span class="params">(T entity, Wrapper&lt;T&gt; updateWrapper)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>批量更新数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="type">boolean</span> <span class="title function_">updateBatchById</span><span class="params">(Collection&lt;T&gt; entityList)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据ID查询数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">T <span class="title function_">getById</span><span class="params">(Serializable id)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据条件查询单条数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">T <span class="title function_">getOne</span><span class="params">(Wrapper&lt;T&gt; queryWrapper)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>查询所有数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">List&lt;T&gt; <span class="title function_">list</span><span class="params">()</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据条件查询数据列表</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">List&lt;T&gt; <span class="title function_">list</span><span class="params">(Wrapper&lt;T&gt; queryWrapper)</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>分页查询数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">Page&lt;T&gt; <span class="title function_">page</span><span class="params">(Page&lt;T&gt; page)</span>;</span><br><span class="line"><span class="comment">// 假设要进行无条件的分页查询，每页显示10条记录，查询第1页</span></span><br><span class="line">IPage&lt;User&gt; page = <span class="keyword">new</span> <span class="title class_">Page</span>&lt;&gt;(<span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">IPage&lt;User&gt; userPage = userService.page(page); <span class="comment">// 调用 page 方法</span></span><br><span class="line">List&lt;User&gt; userList = userPage.getRecords();</span><br><span class="line"><span class="type">long</span> <span class="variable">total</span> <span class="operator">=</span> userPage.getTotal();</span><br><span class="line">System.out.println(<span class="string">"Total users: "</span> + total);</span><br><span class="line"><span class="keyword">for</span> (User user : userList) {</span><br><span class="line">    System.out.println(<span class="string">"User: "</span> + user);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>根据条件分页查询数据</strong>：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line">Page&lt;T&gt; <span class="title function_">page</span><span class="params">(Page&lt;T&gt; page, Wrapper&lt;T&gt; queryWrapper)</span>;</span><br></pre></td></tr></tbody></table></figure></li></ol><p>这些方法提供了对数据库进行增删改查的基本操作。</p><h2 id="sql映射"><a href="#sql映射" class="headerlink" title="sql映射"></a>sql映射</h2><p>在 MyBatis 中，可以通过在映射文件中定义 SQL 语句，并将其与 Java 接口方法关联起来。以下是一个简单的例子，展示了如何在 <code>BUserMapper.xml</code> 文件中定义 SQL 语句，并将其与 <code>BUserMapper</code> 接口中的方法进行映射。</p><p>首先，定义 <code>BUserMapper</code> 接口：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.mysecurity.dao;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.example.mysecurity.model.BUser;</span><br><span class="line"><span class="keyword">import</span> org.apache.ibatis.annotations.Select;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">BUserMapper</span> {</span><br><span class="line">    <span class="meta">@Select("SELECT * FROM b_user WHERE username = #{username}")</span></span><br><span class="line">    BUser <span class="title function_">findByUsername</span><span class="params">(String username)</span>;</span><br><span class="line"></span><br><span class="line">    List&lt;BUser&gt; <span class="title function_">findAll</span><span class="params">()</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>然后，在 <code>BUserMapper.xml</code> 文件中定义 SQL 语句：</p><figure class="highlight xml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">"1.0"</span> encoding=<span class="string">"UTF-8"</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">mapper</span> <span class="keyword">PUBLIC</span> <span class="string">"-//mybatis.org//DTD Mapper 3.0//EN"</span> <span class="string">"http://mybatis.org/dtd/mybatis-3-mapper.dtd"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">"com.example.mysecurity.dao.BUserMapper"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 定义 findAll 方法的 SQL 语句 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"findAll"</span> <span class="attr">resultType</span>=<span class="string">"com.example.mysecurity.model.BUser"</span>&gt;</span></span><br><span class="line">        SELECT * FROM b_user</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></tbody></table></figure><p>在这个例子中：</p><ol><li><code>BUserMapper</code> 接口中定义了两个方法：<code>findByUsername</code> 和 <code>findAll</code>。</li><li><code>findByUsername</code> 方法使用了注解 <code>@Select</code> 来直接在接口中定义 SQL 语句。</li><li><code>findAll</code> 方法在 <code>BUserMapper.xml</code> 文件中定义了 SQL 语句，并通过 <code>id</code> 属性与接口中的方法名进行关联。</li></ol><p>通过这种方式，MyBatis 可以根据接口方法调用相应的 SQL 语句。</p><h2 id="logaspect"><a href="#logaspect" class="headerlink" title="logaspect"></a>logaspect</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">package com.example.dorm.aspect;</span><br><span class="line"></span><br><span class="line">import com.example.dorm.entity.AccessLog;</span><br><span class="line">import com.example.dorm.mapper.LogMapper;</span><br><span class="line">import org.aspectj.lang.ProceedingJoinPoint;</span><br><span class="line">import org.aspectj.lang.annotation.Around;</span><br><span class="line">import org.aspectj.lang.annotation.Aspect;</span><br><span class="line">import org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line">import org.springframework.security.core.Authentication;</span><br><span class="line">import org.springframework.security.core.context.SecurityContextHolder;</span><br><span class="line">import org.springframework.stereotype.Component;</span><br><span class="line">import org.springframework.web.context.request.RequestContextHolder;</span><br><span class="line">import org.springframework.web.context.request.ServletRequestAttributes;</span><br><span class="line"></span><br><span class="line">import javax.servlet.http.HttpServletRequest;</span><br><span class="line">import java.time.LocalDateTime;</span><br><span class="line"></span><br><span class="line">@Aspect</span><br><span class="line">@Component</span><br><span class="line">public class LogAspect {</span><br><span class="line"></span><br><span class="line">    @Autowired</span><br><span class="line">    private LogMapper LogMapper;</span><br><span class="line"></span><br><span class="line">    @Around("execution(* com.example.dorm.controller.*.*(..))")  // 拦截 controller 包下的所有方法</span><br><span class="line">    public Object logAccess(ProceedingJoinPoint joinPoint) throws Throwable {</span><br><span class="line">        // 获取用户信息，这里假设用户ID和IP可以通过 Security Context 获取</span><br><span class="line">        String username = getUserIdFromContext();</span><br><span class="line">        String userIp = getUserIp();</span><br><span class="line"></span><br><span class="line">        // 获取访问的页面</span><br><span class="line">        String accessedPage = joinPoint.getSignature().toString();</span><br><span class="line"></span><br><span class="line">        // 获取操作类型（假设可以通过请求方法获取）</span><br><span class="line">        String actionType = getRequestActionType();  // 比如 GET, POST 等</span><br><span class="line"></span><br><span class="line">        // 创建并保存访问日志</span><br><span class="line">        AccessLog accessLog = new AccessLog();</span><br><span class="line">        accessLog.setUsername(username);</span><br><span class="line">        accessLog.setUserIp(userIp);</span><br><span class="line">        accessLog.setAccessedPage(accessedPage);</span><br><span class="line">        accessLog.setDate(LocalDateTime.now());</span><br><span class="line">        accessLog.setType(actionType);</span><br><span class="line"></span><br><span class="line">        // 保存日志</span><br><span class="line">        LogMapper.insert(accessLog);</span><br><span class="line"></span><br><span class="line">        return joinPoint.proceed();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    private String getUserIdFromContext() {</span><br><span class="line">        // 从 Spring Security 获取当前用户的 ID</span><br><span class="line">        Authentication authentication = SecurityContextHolder.getContext().getAuthentication();</span><br><span class="line">        return authentication.getName(); // 假设用户名是用户 ID</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    private String getUserIp() {</span><br><span class="line">        // 获取用户的 IP 地址，通常通过 HTTP 请求头获取</span><br><span class="line">        HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();</span><br><span class="line">        return request.getRemoteAddr();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    private String getRequestActionType() {</span><br><span class="line">        // 通过 HTTP 请求方法获取操作类型，例如 GET、POST 等</span><br><span class="line">        HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest();</span><br><span class="line">        return request.getMethod();</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/30/compiler/dai-ma-you-hua/"/>
      <url>/2024/11/30/compiler/dai-ma-you-hua/</url>
      
        <content type="html"><![CDATA[<h1 id="代码优化文档"><a href="#代码优化文档" class="headerlink" title="代码优化文档"></a>代码优化文档</h1><h2 id="乘法优化"><a href="#乘法优化" class="headerlink" title="乘法优化"></a>乘法优化</h2><p>有如下几种情况：</p><ul><li>乘以0 rd寄存器直接赋0</li><li>乘以1 把rs寄存器move到rd寄存器</li><li>乘以2的幂：改为sll语句</li><li>其他按普通乘法处理</li></ul><p>关键代码：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (m == <span class="number">1</span>) {</span><br><span class="line">    <span class="keyword">if</span> (rd != reg1) {</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MoveText</span>(rd, reg1);</span><br><span class="line">    }</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> (m == <span class="number">0</span>) {</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">LiText</span>(rd, <span class="number">0</span>);</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> ((m &amp; (m - <span class="number">1</span>)) == <span class="number">0</span>) { <span class="comment">// 2的幂</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> ((m &amp; (<span class="number">1</span> &lt;&lt; k)) == <span class="number">0</span>) {</span><br><span class="line">        k++;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SLL, rd, reg1, k);</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">LiText</span>(reg2, m); <span class="comment">// 重新加载operand2</span></span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">MDText</span>(MDText.Op.MULT, reg1, reg2);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">MFText</span>(MFText.MFType.MFLO, rd);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="除法优化"><a href="#除法优化" class="headerlink" title="除法优化"></a>除法优化</h2><p>不能简单地把2的幂翻译成srl（会有溢出问题），因此我按照讲座里给出的公式来算</p><p><img src="D:\mynotes\source\images\image-20241130102745452.png" alt="image-20241130102745452"></p><p>关键代码：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (d == <span class="number">1</span>) {</span><br><span class="line">    <span class="keyword">if</span> (rd != reg1) {</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MoveText</span>(rd, reg1);</span><br><span class="line">    }</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> ((d &amp; (d - <span class="number">1</span>)) == <span class="number">0</span>){ <span class="comment">// 2的幂</span></span><br><span class="line">    <span class="comment">// 使用公式：reg1/d= SRA(reg1 + SRL(SRA(reg1, k − 1), 32 − k), k)</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> ((d &amp; (<span class="number">1</span> &lt;&lt; k)) == <span class="number">0</span>) {</span><br><span class="line">        k++;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, reg2, reg1, k - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRL, reg2, reg2, <span class="number">32</span> - k);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, reg2, reg2, reg1);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, rd, reg2, k);</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">    <span class="comment">// 处理一般情况 d &gt; 0 且非 2 的幂</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">N</span> <span class="operator">=</span> <span class="number">31</span>; <span class="comment">// 假设常量位数为 32 位</span></span><br><span class="line">    <span class="type">long</span> <span class="variable">m</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 确定 m 和 l</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">        l++;</span><br><span class="line">        m = (<span class="type">long</span>) Math.ceil((<span class="type">double</span>) (<span class="number">1L</span> &lt;&lt; (N + l)) / d);</span><br><span class="line">        <span class="keyword">if</span> (m * d &lt;= (<span class="number">1L</span> &lt;&lt; (N + l)) + (<span class="number">1L</span> &lt;&lt; l)) {</span><br><span class="line">            <span class="keyword">break</span>; <span class="comment">// 满足条件，退出循环</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (m &lt; (<span class="number">1L</span> &lt;&lt; N)) {</span><br><span class="line">        <span class="comment">// 使用公式: SRA(MULSH(m, n), l − 1) + SRL(n, 31)</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">LiText</span>(reg2, (<span class="type">int</span>) m);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MDText</span>(MDText.Op.MULT, reg1, reg2);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MFText</span>(MFText.MFType.MFHI, reg2);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, reg2, reg2, l - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRL, reg1, reg1, <span class="number">31</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, rd, reg2, reg1);</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 使用公式: SRA(n + MULSH(m − 2^32, n), l − 1) + SRL(n, 31)</span></span><br><span class="line">        m -= (<span class="number">1L</span> &lt;&lt; <span class="number">32</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">LiText</span>(reg2, (<span class="type">int</span>) m);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MDText</span>(MDText.Op.MULT, reg1, reg2);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MFText</span>(MFText.MFType.MFHI,reg2);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, reg2, reg2, reg1);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, reg2, reg2, l - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRL, reg1, reg1, <span class="number">31</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, rd, reg2, reg1);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="基本块合并"><a href="#基本块合并" class="headerlink" title="基本块合并"></a>基本块合并</h2><p>若一个基本块无条件跳转到它的下一个基本块，且下一个基本块只有一个前驱，那么可以直接合并这两个基本块，省去跳转指令。</p><p>关键代码：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (bbIter.hasNext()) {</span><br><span class="line">    <span class="type">BasicBlock</span> <span class="variable">curBB</span> <span class="operator">=</span> bbIter.next();</span><br><span class="line">    <span class="keyword">if</span> (prevBB.getInstrs().getLast() <span class="keyword">instanceof</span> BR br &amp;&amp; br.isUncondition() &amp;&amp; br.getDest() == curBB</span><br><span class="line">            &amp;&amp; curBB.getUsers().size() == <span class="number">1</span>) { <span class="comment">// curBB只有preBB一个前驱</span></span><br><span class="line">        prevBB.removeInstr(br);</span><br><span class="line">        <span class="keyword">for</span> (Instruction instr : curBB.getInstrs()) {</span><br><span class="line">            prevBB.addInstr(instr);</span><br><span class="line">            instr.setItsBB(prevBB);</span><br><span class="line">        }</span><br><span class="line">        bbIter.remove();</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        prevBB = curBB;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/26/lun-wen-yue-du/coca-lun-wen-bi-ji/"/>
      <url>/2024/11/26/lun-wen-yue-du/coca-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="CoCa：Contrastive-Captioner"><a href="#CoCa：Contrastive-Captioner" class="headerlink" title="CoCa：Contrastive Captioner"></a>CoCa：Contrastive Captioner</h1><p>探索<strong>大规模的预训练base模型</strong>在计算机视觉领域有重要意义，因为这些模型可以快速转移到许多下游任务中。</p><h2 id="现有的三种基础模型训练方法"><a href="#现有的三种基础模型训练方法" class="headerlink" title="现有的三种基础模型训练方法"></a>现有的三种基础模型训练方法</h2><h3 id="1-单编码模型single-encoder-models"><a href="#1-单编码模型single-encoder-models" class="headerlink" title="1. 单编码模型single-encoder models"></a>1. 单编码模型single-encoder models</h3><p>只有图像编码器，生成视觉表示。使用人工标注的干净数据集</p><p>用途：可以被用到图像和视频理解的下游任务。</p><p>局限：这种模型<strong>严重依赖图像标注</strong>作为标签向量，而不吸收自然语言知识，阻碍了它们在涉及视觉和语言模式的下游任务中的应用。</p><h2 id="2-双编码模型dual-encoder-models"><a href="#2-双编码模型dual-encoder-models" class="headerlink" title="2. 双编码模型dual-encoder models"></a>2. 双编码模型dual-encoder models</h2><p>可以使用有噪声的网络数据，可以在有噪声的图像-文本对上用对比损失预训练两个编码器。</p><p>用途：除了用于视觉任务的视觉嵌入外，由此产生的双编码器模型还可以将文本embedding<strong>编码到相同的潜在空间</strong>，从而实现新的跨模态对齐功能，如零样本图像分类和图像-文本检索。</p><p>局限：这种模型并不直接适用于联合视觉-语言理解任务，如视觉问题回答（VQA），因为缺少联合组件来学习融合的图像和文本表示。</p><h2 id="3-编码-解码模型encoder-decoder-models"><a href="#3-编码-解码模型encoder-decoder-models" class="headerlink" title="3. 编码-解码模型encoder-decoder models"></a>3. 编码-解码模型encoder-decoder models</h2><p>是生成式预训练模型，可以<strong>联合学习视觉和多模态的特征</strong>。在预训练期间，该模型在编码器端编码图像，在解码器输出端计算LM(Language Model)损失。</p><p>用途：对于下游任务，解码器的输出可以用作多模态理解任务的联合表示。</p><p>局限：虽然通过预先训练的编码器-解码器模型获得了优越的视觉语言结果，但它们不产生与图像embedding对齐的纯文本表示，因此在跨模态对齐任务中不可行和效率较低。</p><h2 id="CoCa基本特点"><a href="#CoCa基本特点" class="headerlink" title="CoCa基本特点"></a>CoCa基本特点</h2><p>CoCa统一了上面三种方法的范式，并训练了一个包含这三种方法的模型能力的图像-文本基础模型。</p><ul><li>是一个结合了<strong>对比损失</strong>和<strong>字幕损失</strong>的图像-文本 编码-解码基础模型，因此具备使用<strong>对比</strong>方法的模型能力和<strong>生成</strong>方法的模型能力。</li><li>两个训练目标共享相同的计算图（computational graph），开销小</li><li>在人工标注图像和噪声图像文本数据上都进行训练，训练是<strong>端到端和从头开始的预训练</strong>。通过将所有标签简单地视为文本，无缝地统一了表示学习的自然语言监督。</li><li>在广泛的下游任务上通过零样本迁移或minimal task adaptation有很好的表现。</li></ul><h2 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h2><p>作者首先回顾了三个不同方式使用自然语言监督的基础模型：单编码器分类预训练、双编码器对比学习和编码器解码器图像字幕。然后引入了CoCa，它们在一个简单的架构下同时具有对比学习和图像到字幕生成的优点。进一步讨论了CoCa模型如何通过zero-shot transfer或minimal task adaptation快速转移到下游任务。</p><h3 id="三种用自然语言监督的基础模型"><a href="#三种用自然语言监督的基础模型" class="headerlink" title="三种用自然语言监督的基础模型"></a>三种用自然语言监督的基础模型</h3><h4 id="1-Single-Encoder-Classification"><a href="#1-Single-Encoder-Classification" class="headerlink" title="1. Single-Encoder Classification"></a>1. Single-Encoder Classification</h4><p>用交叉熵损失计算：<br>$$<br>\mathcal{L}<em>{cls} = -p(y)\log{q</em>{\theta}(x)}<br>$$</p><p>$ p(y)$ 是 GT 标签的 one-hot、multi-hot 或平滑分布， $q_θ(x)$ 是以$ θ $为参数的模型在输入 $x$ 时的预测结果</p><h4 id="2-Dual-Encoder-Contrastive-Learning"><a href="#2-Dual-Encoder-Contrastive-Learning" class="headerlink" title="2. Dual-Encoder Contrastive Learning"></a>2. Dual-Encoder Contrastive Learning</h4><p>$$<br>\mathcal{L}<em>{Con} = - \frac{1}{N} (\sum</em>{i}^{N} \log{\frac{exp(x_i^{T}y_i/\sigma)}{\sum_{j=1}^{N} exp(x_i^{T}y_j/\sigma)}} + \sum_{i}^{N} \log{\frac{exp(y_i^{T}x_i/\sigma)}{\sum_{j=1}^{N} exp(y_i^{T}x_j/\sigma)}})<br>$$</p><p>$x_i$代表第i个图像-文本对的图像embeddings(归一化的)，$y_i$代表第i个图像-文本对的文本embeddings(归一化的)。式子括号里前半部分是计算image-to-text，后半部分是text-to-imge。$N$是batch size</p><h4 id="3-Encoder-Decoder-Captioning"><a href="#3-Encoder-Decoder-Captioning" class="headerlink" title="3. Encoder-Decoder Captioning"></a>3. Encoder-Decoder Captioning</h4><p>图像编码器提供潜在的编码特征，文本解码器学习在前向自回归分解（ forward autoregressive factorization）下最大化成对文本$y$的条件似然：</p><p>$$<br>\mathcal{L}<em>{Cap} = - \sum</em>{t=1}^{T} \log{P_{\theta}(y_t | y_{&lt;t}, x)}<br>$$</p><p>利用teacher-forcing对编-解码器进行训练，以并行化计算和学习效率最大化。与之前的方法不同，字幕处理器方法产生了一个联合的图像-文本表示，可以用于视觉语言理解，并且也能够通过自然语言生成图像字幕应用程序。</p><blockquote><p>自回归：在训练过程中使用模型自身的预测结果作为输入</p><p>teacher-forcing: 在训练过程中使用真实的输出序列作为输入。在Teacher Forcing模式下，模型在每个timesteps t预测输出后，不是将这个预测值用作时间t+1的输入，而是使用真实的数据，即目标序列在时间t+1的真实值。这样，即使前一个预测不准确，网络也可以在准确的数据指导下继续学习‌12。这种方法可以防止错误的累积和传播，使得训练过程更加稳定‌</p></blockquote><h3 id="CoCa"><a href="#CoCa" class="headerlink" title="CoCa"></a>CoCa</h3><p><img src="D:\mynotes\source\images\deepLearning\CoCa结构.png" alt="CoCa结构"></p><p>顾名思义，在训练的时候有两个训练目标：基于对比学习的损失函数 Contrastive Loss 和基于生成式图像字幕任务的目标函数 Captioning Loss (一种生成式任务)。</p><p>类似于标准的图像-文本编码解码器模型，CoCa通过神经网络编码器将图像编码为潜在表示，例如ViT，并使用causal masking transformer解码器解码文本</p><blockquote><p>causal mask：因果掩码或未来掩码，用于自回归模型中，以防止模型在生成序列时窥视未来的符号。这确保了给定位置的预测仅依赖于该位置之前的符号</p></blockquote><p>$$<br>\mathcal{L}<em>{CoCa} = \lambda</em>{Con} \mathcal{L}<em>{Con} + \lambda</em>{Cap} \mathcal{L}_{Cap}<br>$$<br> Unimodal Text Decoder的结构和 MultiModal Text Decoder 不同，前者没有 Cross-Attention 后者有，因此前者就只是提取纯文本特征，后者学习图像文本的多模态联合表征。</p><blockquote><p>交叉注意力机制：计算流程类似于自注意力机制，但自注意力机制中的查询、键和值都来自同一个输入序列，而交叉注意力机制的查询和键/值来自<strong>不同的输入序列</strong>。</p></blockquote><p>Image Encoder 的输出 img_feature 先通过一个 Attention Pooler，分别得到 <code>con_feature</code> 和 <code>cap_feature</code>。然后，<code>con_feature</code> 和Unimodal Text Decoder 的输出 <code>[CLS] token</code> 计算对比学习的损失，<code>cap_feature</code> 和MultiModal Text Decoder的输出计算生成任务的损失，完成预训练。</p><p>Pooler 是一个简单的多头注意力层，目的是把 Image Encoder 的输出 img_feature 分别转化成 [1, dim] 和 [N, dim] 维的向量。[1, dim]是con_feature(全局)，[N, dim]是cap_feature（区域级特征的多模态理解任务）</p><blockquote><p>我们的实验表明，一个单一的池化的图像embedding有助于视觉识别任务作为一个全局表示，而更多的视觉tokens（因此更细粒度）有利于需要区域级特征的多模态理解任务。</p></blockquote><img src="D:\mynotes\source\images\deepLearning\CoCa伪代码.png" alt="CoCa结构" style="zoom:150%;"><ul><li>Contrastive Loss 应用于 Image Encoder 和 Text Decoder 的输出端，使得二者都具备独立提取图像特征和文本特征的能力，而且学习到的是<strong>偏全局的图文表征</strong>。</li><li>Captioning Loss 应用于 MultiModal Decoder 的输出端，使得其具有联合提取图像文本联合表征的能力，而且学习到的是<strong>偏细粒度的图文表征</strong>。</li></ul><p>和ALBEF很相似，但训练目标函数不同；ALBEF 的文本模型部分使用的都是两个 Encoder，不具备直接去做生成式任务的能力，这里用的两个Decoder。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>从头开始训练，只有一个阶段</p><p>训练数据集： JFT-3B、ALIGN(有噪声的)</p><p>主要在三个下游任务进行</p><ol><li>zero-shot transfer, </li><li>frozen-feature evaluation</li><li>finetuning</li><li>还提出了消融实验</li></ol><h3 id="视觉识别任务实验结果："><a href="#视觉识别任务实验结果：" class="headerlink" title="视觉识别任务实验结果："></a>视觉识别任务实验结果：</h3><p> frozen-feature evaluation和finetuning</p><p>CoCa 在 ImageNet 上获得了新的最先进的 91.0% 的Top-1 精度，以及更好的视频动作识别结果。而且，CoCa 模型在视觉编码器中使用的参数比其他方法少得多</p><h3 id="跨模态对齐任务实验结果"><a href="#跨模态对齐任务实验结果" class="headerlink" title="跨模态对齐任务实验结果"></a>跨模态对齐任务实验结果</h3><p>zero-shot</p><h3 id="图像字幕和多模态理解任务"><a href="#图像字幕和多模态理解任务" class="headerlink" title="图像字幕和多模态理解任务"></a>图像字幕和多模态理解任务</h3><p>在3个主流 benchmark 上做了实验，分别是 Visual Question Answering (VQA v2), Visual Entailment (SNLI-VE), 和 Visual Reasoning (NLVR2)。在 MSCOCO Captioning 任务上微调了模型</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/26/kai-yuan-ruan-jian-kai-fa-dao-lun/zuo-ye-python-ruan-jian-bao-shang-chuan-testpypi/"/>
      <url>/2024/11/26/kai-yuan-ruan-jian-kai-fa-dao-lun/zuo-ye-python-ruan-jian-bao-shang-chuan-testpypi/</url>
      
        <content type="html"><![CDATA[<h1 id="实验过程记录"><a href="#实验过程记录" class="headerlink" title="实验过程记录"></a>实验过程记录</h1><h2 id="1-创建testpypi账户"><a href="#1-创建testpypi账户" class="headerlink" title="1. 创建testpypi账户"></a>1. 创建testpypi账户</h2><p>注册一个账户后，根据提示配置2FA验证登录（这个不配置没法上传包），由于我之前GitHub账号也用了2FA，所以这里没费什么功夫。</p><h2 id="2-第一个包"><a href="#2-第一个包" class="headerlink" title="2. 第一个包"></a>2. 第一个包</h2><p>基本结构如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">.                                                                                  </span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── fzw_pkg</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; └── example.py</span><br><span class="line">├── setup.cfg</span><br><span class="line">└── setup.py</span><br></pre></td></tr></tbody></table></figure><p>example.py文件内容如下：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_function</span>():</span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Random number:"</span>, np.random.rand())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"This is an example function."</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    example_function()</span><br></pre></td></tr></tbody></table></figure><p>该包用了numpy包</p><p>按照教程完成项目结构，然后上传到testpypi</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">python -m pip install --upgrade build</span><br><span class="line">python -m build</span><br><span class="line">pip install twine</span><br><span class="line">twine upload --repository testpypi dist/*</span><br></pre></td></tr></tbody></table></figure><p>结果如下：</p><p><img src="D:\mynotes\source\images\开源\pkg1.png" alt="pkg1"></p><p>最终项目结构：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">.                                                                                  </span><br><span class="line">├── LICENSE</span><br><span class="line">├── README.md</span><br><span class="line">├── dist</span><br><span class="line">│&nbsp;&nbsp; ├── fzw_pkg1_cube_sugar-0.0.1-py3-none-any.whl</span><br><span class="line">│&nbsp;&nbsp; └── fzw_pkg1_cube_sugar-0.0.1.tar.gz</span><br><span class="line">├── fzw_pkg</span><br><span class="line">│&nbsp;&nbsp; ├── __init__.py</span><br><span class="line">│&nbsp;&nbsp; └── example.py</span><br><span class="line">├── fzw_pkg1_cube_sugar.egg-info</span><br><span class="line">│&nbsp;&nbsp; ├── PKG-INFO</span><br><span class="line">│&nbsp;&nbsp; ├── SOURCES.txt</span><br><span class="line">│&nbsp;&nbsp; ├── dependency_links.txt</span><br><span class="line">│&nbsp;&nbsp; └── top_level.txt</span><br><span class="line">├── setup.cfg</span><br><span class="line">└── setup.py</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>第一个包的链接：<a href="https://test.pypi.org/project/fzw-pkg-cube-sugar/0.0.1/">https://test.pypi.org/project/fzw-pkg-cube-sugar/0.0.1/</a></p><h2 id="3-第二个包"><a href="#3-第二个包" class="headerlink" title="3. 第二个包"></a>3. 第二个包</h2><p>首先安装刚才的第一个包</p><p><img src="D:\mynotes\source\images\开源\安装pkg1.png" alt="安装pkg1"></p><p>然后新建一个包，import一下，运行，没有问题：</p><p><img src="D:\mynotes\source\images\开源\运行pkg2.png" alt="运行pkg2"></p><p>同样的流程上传到testpypi，结果如下：</p><p><img src="D:\mynotes\source\images\开源\pkg2.png" alt="pkg2"></p><p>第二个包的链接：<a href="https://test.pypi.org/project/fzw-pkg2-cube-sugar/0.0.1/">https://test.pypi.org/project/fzw-pkg2-cube-sugar/0.0.1/</a></p><p>可以成功安装这个包：</p><p><img src="D:\mynotes\source\images\开源\安装pkg2.png" alt="安装pkg2"></p><h2 id="4-参考教程"><a href="#4-参考教程" class="headerlink" title="4. 参考教程"></a>4. 参考教程</h2><p>上传的细节和以前的教程有些不同，因此找了一篇24年的教程</p><p><a href="https://zhuanlan.zhihu.com/p/682004873">2024年，将Python项目发布到PyPI保姆级教程</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/25/lun-wen-yue-du/blip-lun-wen-bi-ji/"/>
      <url>/2024/11/25/lun-wen-yue-du/blip-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p><img src="D:\mynotes\source\images\deepLearning\BLIP框架图.png" alt="BLIP框架图"></p><ol><li>图像编码器：ViT的架构。将输入图像分割成一个个的 Patch 并将它们编码为一系列 Image Embedding，并使用额外的 [CLS] token 来表示全局的图像特征。</li><li>文本编码器：BERT 的架构。[CLS] token 附加到文本输入的开头以总结句子。作用是提取文本特征做对比学习。</li><li>基于图像的文本编码器：使用 Cross-Attention，作用是根据 ViT 给的图片特征和文本输入做二分类，所以使用的是编码器，且注意力部分是双向的 Self-Attention。添加一个额外的 [Encode] token，作为图像文本的联合特征。</li><li>基于图像的文本解码器：使用 Cross-Attention，作用是根据 ViT 给的图片特征和文本输入做文本生成的任务，所以使用的是解码器，且注意力部分是 Casual-Attention，目标是预测下一个 token。添加一个额外的 [Decode] token 和结束 token，作为生成结果的起点和终点。</li></ol><p>（相同颜色的部分参数共享）</p><h3 id="三个训练目标"><a href="#三个训练目标" class="headerlink" title="三个训练目标"></a>三个训练目标</h3><h4 id="对比学习损失-Image-Text-Contrastive-Loss-ITC"><a href="#对比学习损失-Image-Text-Contrastive-Loss-ITC" class="headerlink" title="对比学习损失 (Image-Text Contrastive Loss, ITC)"></a>对比学习损失 (Image-Text Contrastive Loss, ITC)</h4><p>目标是对齐视觉和文本的特征空间。方法是使得正样本图文对的相似性更大，负样本图文对的相似性更低，在 ALBEF 里面也有使用到。使用了 ALBEF 中的动量编码器，它的目的是产生一些伪标签，辅助模型的训练。</p><h4 id="图像-文本匹配损失-Image-Text-Matching-Loss-ITM"><a href="#图像-文本匹配损失-Image-Text-Matching-Loss-ITM" class="headerlink" title="图像-文本匹配损失(Image-Text Matching Loss, ITM)"></a>图像-文本匹配损失(Image-Text Matching Loss, ITM)</h4><p>目标是学习图像文本的联合表征，以捕获视觉和语言之间的细粒度对齐。ITM 是一个二分类任务，使用一个分类头来预测图像文本对是正样本还是负样本。使用了 ALBEF 中的 hard negative mining 技术。</p><h4 id="语言模型损失-Language-Modeling-Loss-LM"><a href="#语言模型损失-Language-Modeling-Loss-LM" class="headerlink" title="语言模型损失 (Language Modeling Loss, LM)"></a>语言模型损失 (Language Modeling Loss, LM)</h4><p>BLIP 包含解码器，用于生成任务，因此需要一个针对于生成任务的语言模型损失。目标是根据给定的图像以自回归方式来生成关于文本的描述。与 VLP 中广泛使用的 MLM 损失 (完形填空) 相比，LM 使模型能够将视觉信息转换为连贯的字幕。</p><h2 id="CapFilt"><a href="#CapFilt" class="headerlink" title="CapFilt"></a>CapFilt</h2><p>为了高效率利用噪声网络数据。</p><p><img src="D:\mynotes\source\images\deepLearning\BLIP-CapFilt.png" alt="CapFilt"></p><h3 id="字幕器-Captioner"><a href="#字幕器-Captioner" class="headerlink" title="字幕器 Captioner"></a>字幕器 Captioner</h3><p>给一张网络图片，生成字幕。是一个基于视觉的文本解码器，使用 LM Loss微调。给定网络图片 $I_w $，Captioner 生成字幕 $T_s$ 。</p><h3 id="过滤器-Filter"><a href="#过滤器-Filter" class="headerlink" title="过滤器 Filter"></a>过滤器 Filter</h3><p>过滤掉噪声图文对。是一个基于视觉的文本编码器，看文本是否与图像匹配，使用 ITC 和 ITM 微调。Filter 删除原始 Web 文本 $T_w$ 和合成文本 $T_s$ 中的噪声文本，如果 ITM 头将其预测为与图像不匹配，则认为文本有噪声。</p><p>最后，将过滤后的图像-文本对与人工注释对相结合，形成一个新的数据集，用它来预训练一个新的模型。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/23/os/tmux-chang-yong-zhi-ling/"/>
      <url>/2024/11/23/os/tmux-chang-yong-zhi-ling/</url>
      
        <content type="html"><![CDATA[<ul><li><p><code>tmux ls</code>：显示所有会话</p></li><li><p><code>tmux new -s &lt;name&gt;</code>：新建一个会话并命名</p></li><li><p><code>tmux rename-session -t 0 &lt;nem-name&gt;</code>：重命名</p></li><li><p><code>tmux kill-session -t 0/name</code>：杀死会话</p></li><li><p><code>tmux a -t 0/name</code>：恢复会话</p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/19/db/shu-ju-ku-biao/"/>
      <url>/2024/11/19/db/shu-ju-ku-biao/</url>
      
        <content type="html"><![CDATA[<p><strong>规范：</strong></p><ol><li><p>主码使用下划线标识</p></li><li><p>外码使用<em>斜体</em>标识</p></li><li><p>“name”列是英文名称，主要用于数据库建表、api的json文件；“字段名称”列是中文名称，数据库表的comment字段</p></li><li><p>报修记录repairRecords</p></li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>报修id</td><td>VARCHAR(50)</td><td>PK, 唯一</td></tr><tr><td>createDate</td><td>创建时间</td><td>DATATIME</td><td></td></tr><tr><td>finisheDate</td><td>完成时间</td><td>DATATIME</td><td>可为null</td></tr><tr><td>applicantId</td><td><em>申请人</em></td><td>VARCHAR(15)</td><td>FK,  唯一，内容为学生学号</td></tr><tr><td>title</td><td>标题</td><td>TEXT</td><td></td></tr><tr><td>content</td><td>详情</td><td>TEXT</td><td></td></tr><tr><td>status</td><td>状态</td><td>ENUM</td><td>枚举选项：完成、未完成</td></tr></tbody></table><ol start="7"><li>公告_发布人annocements</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>id</td><td>INT AUTO_INCREMENT</td><td>PK,唯一</td></tr><tr><td>senderId</td><td><em>发布人工号</em></td><td>VARCHAR(15)</td><td>FK,唯一</td></tr><tr><td>title</td><td>标题</td><td>TEXT</td><td></td></tr><tr><td>content</td><td>内容</td><td>TEXT</td><td></td></tr><tr><td>date</td><td>日期</td><td>DATETIME</td><td></td></tr><tr><td>notifyObject</td><td>通知对象</td><td>ENUM</td><td>枚举选项：全体，1，2，…</td></tr></tbody></table><ol start="8"><li>公告_接收人annocementReceivers</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>id</td><td>INT AUTO_INCREMENT</td><td>PK,唯一</td></tr><tr><td>annocementId</td><td><em>公告ID</em></td><td>VARCHAR(50)</td><td>FK,唯一</td></tr><tr><td>receiveStudentId</td><td><em>接收人学号</em></td><td>VARCHAR(15)</td><td>FK,唯一</td></tr><tr><td>status</td><td>状态</td><td>ENUM</td><td>枚举选型：已读，未读</td></tr></tbody></table><ol start="9"><li>调宿adjustRooms</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>调宿申请ID</td><td>VARCHAR(50)</td><td>PK,唯一</td></tr><tr><td>studentId</td><td><em>学号</em></td><td>VARCHAR(15)</td><td>FK</td></tr><tr><td>curDormId</td><td><em>当前公寓号</em></td><td>INT</td><td>FK(todo)</td></tr><tr><td>curRoomId</td><td><em>当前房间号</em></td><td>INT</td><td>FK</td></tr><tr><td>curBedId</td><td>当前床位号</td><td>INT</td><td></td></tr><tr><td>toDormId</td><td><em>目标公寓号</em></td><td>INT</td><td>FK</td></tr><tr><td>toRoomId</td><td><em>目标房间号</em></td><td>INT</td><td>FK</td></tr><tr><td>toBedId</td><td>目标床位号</td><td>INT</td><td></td></tr><tr><td>status</td><td>申请状态</td><td>ENUM</td><td>枚举选项：通过/驳回/未处理</td></tr><tr><td>createDate</td><td>申请时间</td><td>DATETIME</td><td></td></tr><tr><td>finishDate</td><td>处理时间</td><td>DATETIME</td><td>可为null，为null时不显示</td></tr></tbody></table><ol start="10"><li>系号-学院majors</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>majorId</td><td>系号</td><td>INT</td><td></td></tr><tr><td>major</td><td>学院</td><td>VARCHAR(50)</td><td></td></tr></tbody></table><ol start="11"><li>辅导员counsellors</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>studentID</td><td>学号</td><td>VARCHAR(15)</td><td>PK,唯一</td></tr><tr><td>phoneNum</td><td>电话号码</td><td>VARCHAR(15)</td><td></td></tr><tr><td>majorId</td><td><em>系号</em></td><td>INT</td><td>FK</td></tr><tr><td>grade</td><td>年级</td><td>VARCHAR(15)</td><td></td></tr></tbody></table><ol start="12"><li>消息表notifies</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>id</td><td>INT AUTO_INCREMENT</td><td>PK，唯一</td></tr><tr><td>sendId</td><td><em>发送者id</em></td><td>VARCHAR(15)</td><td>FK, 唯一</td></tr><tr><td>type</td><td>类型</td><td>ENUM</td><td>枚举选型：公告，报修，调宿</td></tr><tr><td>targetId</td><td><em>目标id</em></td><td>INT</td><td>公告/报修/调宿id</td></tr><tr><td>date</td><td>创建时间</td><td>DATATIME</td><td></td></tr></tbody></table><ol start="13"><li>消息-用户表notifyUsers</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>id</td><td>INT AUTO_INCREMENT</td><td>PK</td></tr><tr><td>notifyId</td><td><em>消息id</em></td><td>INT</td><td>FK，唯一</td></tr><tr><td>userId</td><td><em>用户id</em></td><td>INT</td><td>FK，唯一</td></tr><tr><td>status</td><td>状态</td><td>ENUM</td><td>枚举选型：已读，未读</td></tr></tbody></table><ol start="14"><li>操作日志表logs</li></ol><table><thead><tr><th>name</th><th>字段名称</th><th>类型</th><th>备注</th></tr></thead><tbody><tr><td>id</td><td>id</td><td>INT AUTO_INCREMENT</td><td>PK, 唯一</td></tr><tr><td>date</td><td>操作时间</td><td>DATATIME</td><td></td></tr><tr><td>userId</td><td><em>用户id</em></td><td>INT AUTO_INCREMENT</td><td>FK</td></tr><tr><td>type</td><td>操作类型</td><td>ENUM</td><td>枚举选项：登录、注册、修改密码</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/18/dl/detpro-huan-jing-pei-zhi/"/>
      <url>/2024/11/18/dl/detpro-huan-jing-pei-zhi/</url>
      
        <content type="html"><![CDATA[<p>创建有python和pytorch的环境</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n detpro python=3.8</span><br><span class="line">conda activate detpro</span><br><span class="line">conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=10.2 -c pytorch</span><br></pre></td></tr></tbody></table></figure><p>按照readme安装</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pip install -r requirements/build.txt</span><br><span class="line">pip install -e .</span><br><span class="line">pip install git+https://github.com/openai/CLIP.git</span><br><span class="line">pip uninstall pycocotools -y</span><br><span class="line">pip uninstall mmpycocotools -y</span><br><span class="line">pip install mmpycocotools</span><br><span class="line">pip install git+https://github.com/lvis-dataset/lvis-api.git</span><br><span class="line">pip install mmcv-full==1.2.5 -f https://download.openmmlab.com/mmcv/dist/cu110/torch1.7.0/index.html</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/13/dl/baron/"/>
      <url>/2024/11/13/dl/baron/</url>
      
        <content type="html"><![CDATA[<h1 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h1><p>BARON首先对上下文相关的区域进行采样，形成一个“袋子”。由于区域建议网络(RPN)被证明可以覆盖潜在的新对象，我们探索了一种邻域抽样策略，对 region proposals 周围的 box 进行抽样，以帮助建模一袋视觉概念的共同出现。</p><p>其次，BARON通过将区域特征投影到词嵌入空间中，并使用冻结的VLM的文本编码器(TE)对这些伪词进行编码，从而获得区域袋嵌入。BARON将区域特征投射到伪词上，自然使TE能够有效地表示共现语义概念，理解整个场景。为了保留区域框的空间信息，BARON将框形和框中心位置投影到嵌入中，并添加到伪词中，然后将它们馈送给TE。</p><h2 id="环境配置过程"><a href="#环境配置过程" class="headerlink" title="环境配置过程"></a>环境配置过程</h2><p>readme里面给出的指导是用slurm_train训练，但是有如下报错</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">srun: error: Unable to allocate resources: Unable to contact slurm controller (connect failure)</span><br></pre></td></tr></tbody></table></figure><p>尝试改用dist_train训练（但是<strong>readme里没有提到这个文件的训练方式</strong></p><ul><li>系统上的CUDA 10.0</li><li>CUDA驱动版本 12.4</li></ul><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ nvcc --version //查看系统上的CUDA版本为10.0</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2018 NVIDIA Corporation</span><br><span class="line">Built on Sat_Aug_25_21:08:01_CDT_2018</span><br><span class="line">Cuda compilation tools, release 10.0, V10.0.130</span><br></pre></td></tr></tbody></table></figure><p>依次安装readme里提到的包</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ pip install openmim mmengine</span><br></pre></td></tr></tbody></table></figure><p>mmcv安装前需先安装pytorch 下述为<strong>CUDA版本为10.0</strong>对应的pytorch安装命令</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=10.2 -c pytorch</span><br></pre></td></tr></tbody></table></figure><p>mmcv版本不能太高，否则会跟后续mmdet的要求冲突（例如出现如下报错：</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ mmdet 3.3.0 requires mmcv&lt;2.2.0,&gt;=2.0.0rc4; extra == <span class="string">"mim"</span>, but you have mmcv 2.2.0 <span class="built_in">which</span> is incompatible.</span><br></pre></td></tr></tbody></table></figure><p>mmdet&gt;=3.3要求的mmcv版本&gt;=2.0.0 &lt;2.2.0</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ mim install mmcv==2.0.0</span><br></pre></td></tr></tbody></table></figure><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ pip install git+https://github.com/lvis-dataset/lvis-api.git //没成功，但是应该是lvis数据集的，可能用不上？</span><br><span class="line">$ mim install mmdet&gt;=3.0.0rc6 //安装的3.3.0</span><br></pre></td></tr></tbody></table></figure><p>readme没给出的包：ftfy</p><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ conda install ftfy</span><br><span class="line">$ conda install regex</span><br></pre></td></tr></tbody></table></figure><p>运行训练出现报错</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">File "/home/anaconda3/envs/ovdet/lib/python3.10/importlib/__init__.py", line 126, in import_module</span><br><span class="line">    return _bootstrap._gcd_import(name[level:], package, level)</span><br><span class="line">ImportError: libc10_cuda.so: cannot open shared object file: No such file or directory</span><br></pre></td></tr></tbody></table></figure><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li>安装其他库的时候pytorch被更新成cpu的了</li></ol><p><img src="D:\mynotes\source\images\deepLearning\image-20241115101855273.png" alt="image-20241115101855273"></p><p>解决方案:</p><p>先安装包，然后重新下载一遍pytorch</p><ol start="2"><li>之前以为clip模型是一样的，把/home/OVD/OADP-main/pretrained/clip/ViT-B-32.pt连接到checkpoints/clip_vitb32.pth，但是其实不能用，运行下述代码的时候忘记取消软连接了，/home/OVD/OADP-main/pretrained/clip/ViT-B-32.pt被改了</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">import clip</span><br><span class="line">import torch</span><br><span class="line">model, _ = clip.load("ViT-B/32")</span><br><span class="line">torch.save(model.state_dict(), 'checkpoints/clip_vitb32.pth')</span><br></pre></td></tr></tbody></table></figure><p>todo：在oadp里用下面命令重新安装</p><figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">python -c "import clip; clip.load_default()"</span><br></pre></td></tr></tbody></table></figure><ol start="3"><li><p>CUDA out of memory</p><p>减小lr：0.04 -》 0.001</p><p>减小batch：4 -》2</p></li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>可学习的prompt：在<code>models/roi_heads/bbox_head.py</code>里实现了一个BaronBBoxHead类，继承自mmedet的BBoxHead，如下是init里初始化prompt的部分，实现可学习的方法是给cls_embeddings加上一个bias</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> cls_bias <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.cls_bias = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">assert</span> self.loss_cls.use_sigmoid, \</span><br><span class="line">                <span class="string">"cls_bias only used for sigmoid logits"</span></span><br><span class="line">            self.cls_bias = nn.Parameter(torch.ones(<span class="number">1</span>) * cls_bias) <span class="comment"># 可学习的偏移量</span></span><br><span class="line"><span class="keyword">if</span> cls_embeddings_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># config里指明cls_embeddings_path='data/metadata/coco_clip_hand_craft_attn12.npy',</span></span><br><span class="line">            cls_embeddings = torch.from_numpy(</span><br><span class="line">                np.load(cls_embeddings_path)).<span class="built_in">float</span>()</span><br><span class="line">            <span class="keyword">assert</span> self.num_classes == cls_embeddings.shape[<span class="number">0</span>]</span><br><span class="line">            self.register_buffer(<span class="string">'cls_embeddings'</span>, cls_embeddings) <span class="comment"># 注册一个不可训练的buffer</span></span><br><span class="line">            self.learn_bg = <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> bg_embedding == <span class="string">'zero'</span>:</span><br><span class="line">                self.register_buffer(<span class="string">'bg_embedding'</span>,</span><br><span class="line">                                     torch.zeros_like(cls_embeddings[:<span class="number">1</span>]))</span><br><span class="line">            <span class="keyword">elif</span> bg_embedding == <span class="string">'learn'</span>:</span><br><span class="line">                self.bg_embedding = nn.Linear(<span class="number">1</span>, cls_embeddings.shape[<span class="number">1</span>])</span><br><span class="line">                self.init_cfg += [</span><br><span class="line">                    <span class="built_in">dict</span>(</span><br><span class="line">                        <span class="built_in">type</span>=<span class="string">'Xavier'</span>, distribution=<span class="string">'uniform'</span>,</span><br><span class="line">                        override=<span class="built_in">dict</span>(name=<span class="string">'bg_embedding'</span>)),</span><br><span class="line">                ]</span><br><span class="line">                self.learn_bg = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">f"<span class="subst">{bg_embedding}</span> not supported."</span>)</span><br></pre></td></tr></tbody></table></figure><p>训练时的调整在pred_cls_logits方法里</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pred_cls_logits</span>(<span class="params">self, pseudo_words, clip_model</span>):</span><br><span class="line">        text_encoder = clip_model.text_encoder</span><br><span class="line">        <span class="keyword">if</span> pseudo_words.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> pseudo_words.new_zeros(<span class="number">0</span>, self.num_classes + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">with</span> autocast():</span><br><span class="line">            valid_mask = self._drop_word(pseudo_words)</span><br><span class="line">            pseudo_text, end_token_ids = text_encoder.prepare_pseudo_text_tensor(</span><br><span class="line">                pseudo_words, valid_mask)  <span class="comment"># add start and stop token</span></span><br><span class="line">            <span class="keyword">if</span> self.use_attn12_output:</span><br><span class="line">                cls_features, _, _ = \</span><br><span class="line">                    text_encoder.encode_pseudo_text_endk(pseudo_text, end_token_ids,</span><br><span class="line">                                                         text_pe=<span class="literal">True</span>,</span><br><span class="line">                                                         stepk=<span class="number">12</span>, normalize=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cls_features = \</span><br><span class="line">                    text_encoder.encode_pseudo_text(pseudo_text, end_token_ids,</span><br><span class="line">                                                    text_pe=<span class="literal">True</span>, normalize=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> self.learn_bg:</span><br><span class="line">            input_ones = pseudo_words.new_ones(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            bg_embedding = self.bg_embedding(input_ones)</span><br><span class="line">            bg_embedding = F.normalize(bg_embedding, p=<span class="number">2</span>, dim=-<span class="number">1</span>)   <span class="comment"># normalize</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            bg_embedding = self.bg_embedding</span><br><span class="line">        cls_embeddings = torch.cat([self.cls_embeddings, bg_embedding])</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            cls_logits = self.cls_temp * cls_features @ cls_embeddings.T</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cls_logits = self.test_cls_temp * cls_features @ cls_embeddings.T</span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.loss_cls.use_sigmoid:</span><br><span class="line">            cls_logits += self.cls_bias <span class="comment"># 加上可学习的偏移量</span></span><br><span class="line">        <span class="keyword">assert</span> cls_logits.shape[<span class="number">1</span>] == self.num_classes + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> cls_logits</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/11/lun-wen-yue-du/albef-lun-wen-bi-ji/"/>
      <url>/2024/11/11/lun-wen-yue-du/albef-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="ALBEF论文笔记"><a href="#ALBEF论文笔记" class="headerlink" title="ALBEF论文笔记"></a>ALBEF论文笔记</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h4 id="现有方法的缺陷"><a href="#现有方法的缺陷" class="headerlink" title="现有方法的缺陷"></a>现有方法的缺陷</h4><ul><li>图像特征和单词标记嵌入位于它们自己的空间中，这使得多模态编码器学习建模其交互具有挑战性；</li><li>目标检测器既注释昂贵又计算昂贵，因为它在训练前需要边界框注释，以及高分辨率(例如。600×1000)图像；</li><li>广泛使用的图像-文本数据集是从web收集的，具有固有的噪声，现有的训练前目标如MLM可能会过拟合噪声文本，降低模型的泛化性能。</li></ul><h4 id="ALBEF的优势"><a href="#ALBEF的优势" class="headerlink" title="ALBEF的优势"></a>ALBEF的优势</h4><ul><li><strong>图文对齐后再融合</strong>。对于图片的 Embedding 和文本的 Embedding 引入一个对比学习的损失函数 image-text contrastive loss，在融合之前提前把图片和文本的表征对齐。使得后续的多模态 Transformer <strong>更容易执行跨模态学习</strong>。</li><li><strong>不使用目标检测器</strong>，不需要边界框注释或高分辨率的图像。</li><li>提出了<strong>动量蒸馏</strong>方法，这是一种从动量模型产生的伪目标中学习的自训练方法，改进了从有<strong>噪声</strong>的web数据中学习。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ol><li>用一个无检测器的<strong>图像编码器</strong>和一个<strong>文本编码器</strong>独立地编码图像和文本。</li><li>然后利用<strong>多模态编码器</strong>，通过跨模态注意的方法，将图像特征与文本特征进行融合。</li><li>在来自单模态编码器的表示中引入了一个中间的图像-文本对比（ITC）损失，目的：</li></ol><ul><li><p>它对齐图像特征和文本特征，使多模态编码器<strong>更容易执行跨模态学习</strong>；</p></li><li><p>改进单模态编码器，更好地理解图像和文本的语义意义；</p></li><li><p>学习一个通用的低维空间嵌入图像和文本，使图像-文本匹配目标（ITM）通过对比硬负挖掘（Hard Negative Mining）找到信息更丰富的样本。</p><blockquote><p>Hard Negative Mining：基本思想是在训练过程中，<strong>重点关注那些模型难以正确分类的负样本</strong>。在训练过程中，模型会对每个样本进行预测，并生成一个预测概率。对于负样本，如果模型给出的预测概率很高（即模型错误地认为它是一个正样本），那么这个负样本就是一个hard negative。通过将这些hard negatives加入到训练集中，我们可以帮助模型更好地学习如何区分正样本和负样本</p></blockquote></li></ul><ol start="4"><li>为了改进在噪声监督下的学习，我们提出了<strong>动量蒸馏</strong>（MoD），使模型能够利用一个更大的未经管理的web数据集。在训练过程中，我们通过取模型参数的移动平均数来保持模型的动量版本，并使用动量模型生成伪目标作为额外的监督。使用MoD时，该模型不会因为产生其他不同于web注释的合理输出而受到惩罚。我们证明了MoD不仅改进了训练前的任务，而且还改进了具有干净注释的下游任务。</li></ol><p><img src="D:\mynotes\source\images\deepLearning\ALBEF框架图.png" alt="ALBEF框架图"></p><p>三个优化目标：单模态编码器上的图像文本对比学习（ITC）、多模态编码器上的掩码语言建模（MLM）和图像文本匹配（ITM）。其中，通过在线对比Hard Negative Mining改进了ITM</p><h3 id="图像文本对比学习（ITC）"><a href="#图像文本对比学习（ITC）" class="headerlink" title="图像文本对比学习（ITC）"></a>图像文本对比学习（ITC）</h3><p>即Align before Fuse的核心<br>$$<br>s = g_v(v_{cls})^T g_w(w_{cls})<br>$$</p><p>s是相似度函数，$v_{cls}$是[CLS] token的embedding（图像编码器编码得到的），$w_{cls}$是文本编码其得到的， $g_v$和$g_w$是一种线性转换，它们将[CLS]embedding映射到归一化的低维（256-d）表示</p><p>维护<strong>两个队列</strong>来存储来自动量单模态编码器的<strong>最新的M个图像-文本表示</strong>（当作当前图像、文本的负样本）。动量编码器的归一化特征分别记为$g^{‘}<em>v(v^{‘}</em>{cls})$和$g^{‘}<em>w(w^{‘}</em>{cls})$。</p><blockquote><p>动量在数学上就是加权移动平均。例如$ y_t=m \times y_{t-1}+(1-m) \times x_t $，$ y_{t-1}$为上一时刻的输出，$x_t$为当前输入，$m$为动量参数；当 $m$很大时，$y_t$就取决于上一时刻输出，其更新就很缓慢；当$m$很小时，$y_t$就取决于当前时刻输入</p></blockquote><p>$$<br>\text{定义：} s(I, T) = g_v(v_{cls})^T g_w^{‘}(w^{‘}<em>{cls}) \text{，} s(T, I) = g_w(w</em>{cls})^T g_v^{‘}(v^{‘}<em>{cls}) \<br>\text{对于每一对文本和图像，计算两个相似度：} p^{i2t}<em>m(I) = \frac{exp(s(I,T_m) / \tau)}{\sum</em>{m=1}^Mexp(s(I,T_m) / \tau)} \text{，}<br>p^{t2i}<em>m(T) = \frac{exp(s(T,I_m) / \tau)}{\sum</em>{m=1}^Mexp(s(T,I_m) / \tau)} (1)\<br>\text{则损失} L</em>{itc} = \frac{1}{2}E_{(I,T) \sim D}[H(y^{i2t}(I), p^{i2t}(I)) + H(y^{t2i}(T), p^{t2i}(T))]<br>$$<br>$p^{i2t}_m、p^{t2i}_m$是文本到图像、图像到文本的softmax归一化相似度。$y^{i2t}(I)$和$y^{t2i}(T)$表示真实独热编码相似度，其中负对的概率为0，正对的概率为1。 $\tau$是个可学习的参数，H是交叉熵损失。</p><h3 id="多模态编码器上的掩码语言建模（MLM）"><a href="#多模态编码器上的掩码语言建模（MLM）" class="headerlink" title="多模态编码器上的掩码语言建模（MLM）"></a>多模态编码器上的掩码语言建模（MLM）</h3><p>利用图像和上下文文本来预测掩蔽词。我们以15%的概率随机掩码出输入标记，并用特殊的标记$[MASK]$替换它们。设$\hat{T}$表示一个被mask过的文本，而$p^{msk}(I，\hat{T})$表示模型对一个该文本被mask处的token的预测概率, $y^{msk}$是一个独热编码的词汇分布，其中ground-truth token的概率为1。MLM使交叉熵损失最小化：<br>$$<br>L_{mlm} = E_{(I, \hat{T}) \sim D}H(y^{msk}, p^{msk}(I，\hat{T}))<br>$$</p><h3 id="图像文本匹配（ITM）"><a href="#图像文本匹配（ITM）" class="headerlink" title="图像文本匹配（ITM）"></a>图像文本匹配（ITM）</h3><p>可以预测一对图像和文本是正的（匹配）还是负的（不匹配）。使用多模态编码器的输出嵌入$[CLS]$ token作为图像-文本对的联合表示，并附加一个全连接（FC）层，然后使用softmax来计算一个二分类概率$p^{itm}$。ITM损失为：<br>$$<br>L_{itm} = E_{(I, T) \sim D}H(y^{itm}, p^{itm}(I,T))<br>$$<br>$y^{itm}$是一个二维独热编码向量，表示真实标签。</p><p>Hard Negative Mining：我们提出了一种在零计算开销的ITM任务中进行硬负样本采样的策略。如果一个负的图像-文本对共享相似的语义，但在细粒度的细节上有所不同，那么它们是硬的（hard）。我们使用方程1中的对比相似度来寻找一个batch内的硬负样本：对于一个batch中的每一幅图像，按照对比相似度分布采样与图像最相似的文本作为负文本（即模型认为它非常可能是正样本，但其实它是负样本)。同样地，为每个文本采样一个最hard的负图像。</p><blockquote><p>设 B 为 Batch Size，从代码中可以看出最终的预测维度是 [3B, 2]，标签维度是 3B，前 B 个样本都是正样本，其余 2B 都是负样本。</p></blockquote><h3 id="动量蒸馏"><a href="#动量蒸馏" class="headerlink" title="动量蒸馏"></a>动量蒸馏</h3><p>解决的问题：从网上收集的数据存在噪声。对于ITC学习，图像的负文本也可能与图像的内容相匹配。对于MLM，可能存在其他不同于描述图像但描述得同样好（或更好）的注释的词。然而，ITC和MLM的one-hot标签惩罚所有负面预测，不管它们的正确性。</p><p><img src="D:\mynotes\source\images\deepLearning\ALBEF动量蒸馏部分.png" alt="动量蒸馏部分"></p><p>解决办法：从动量模型产生的伪目标中学习。动量模型是一个连续发展的<strong>教师</strong>模型，它由单模态和多模态编码器的指数-移动平均版本组成。在训练过程中，我们<strong>训练base模型，使其预测与动量模型的预测相匹配</strong>。具体来说，对于ITC，我们首先使用动量单模态编码器的特征来计算图像-文本相似度，即$s^{‘}(I, T) = g_v^{‘}(v_{cls})^T g_w^{‘}(w^{‘}<em>{cls})$和$s^{‘}(T, I) = g_w^{‘}(w</em>{cls})^T g_v^{‘}(v^{‘}<em>{cls})$。然后把方程1中的$s$替换成$s^{‘}$来计算软伪目标$q^{i2t}$和$q^{t2i}$。$ITC</em>{MoD}$损失的定义为：<br>$$<br>L_{itc}^{mod} = (1- \alpha)L_{itc} + \frac{\alpha}{2}E_{(I,T) \sim D}[KL(q^{i2t}(I) || p^{i2t}(I)) + KL(q^{t2i}(T) || p^{t2i}(T))]<br>$$<br>类似的，$MLM_{MoD}$损失的定义为：<br>$$<br>L_{mlm}^{mod} = (1- \alpha)L_{mlm} + \alpha E_{(I,\hat{T}) \sim D}KL(q^{msk}(I, \hat{T}) || p^{msk}(I, \hat{T}))<br>$$</p><blockquote><p>KL:KL散度计算，又称相对熵或信息散度。我们设定两个概率分布分别为P和Q，在设定为连续随机变量的前提下，他们对应的概率密度函数分别为p(x)和q(x)。如果我们用p(x)去近似q(x)，则KL散度可以表示为：</p><p>$$<br>KL(Q||P) = \int q(x)log\frac{q(x)}{p(x)}dx<br>$$</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/11/07/db/db-di-wu-zhang-shi-yan-zuo-ye/"/>
      <url>/2024/11/07/db/db-di-wu-zhang-shi-yan-zuo-ye/</url>
      
        <content type="html"><![CDATA[<h1 id="第五章实验部分作业"><a href="#第五章实验部分作业" class="headerlink" title="第五章实验部分作业"></a>第五章实验部分作业</h1><h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># （1）创建用户王明，并授予他对学生表的SELECT权限</span><br><span class="line">    CREATE USER '王明'@'localhost' IDENTIFIED BY '123456';</span><br><span class="line">    GRANT SELECT ON 学生 TO '王明'@'localhost';</span><br><span class="line"># （2）授予用户王明对学生表的INSERT和DELETE权限</span><br><span class="line">    GRANT INSERT, DELETE ON 学生 TO '王明'@'localhost';</span><br><span class="line"># （3）授予用户王明对选课表的SELECT权限和对成绩字段的UPDATE权限</span><br><span class="line">    GRANT SELECT, UPDATE(成绩) ON 选课 TO '王明'@'localhost';</span><br><span class="line"># （4）授予用户王明创建表的权限</span><br><span class="line">    GRANT CREATE ON *.* TO '王明'@'localhost';</span><br><span class="line"># （5）授予用户王明对学生表的所有权限，并具有转授给他人的权力。</span><br><span class="line">    GRANT ALL ON 学生 TO '王明'@'localhost' WITH GRANT OPTION;</span><br><span class="line"># （6）撤销用户王明对学生表的INSERT和DELETE权限</span><br><span class="line">    REVOKE INSERT, DELETE ON 学生 FROM '王明'@'localhost';</span><br><span class="line"># （7）撤销用户王明对选课表的SELECT权限和对成绩字段的UPDATE权限</span><br><span class="line">    REVOKE SELECT, UPDATE(成绩) ON 选课 FROM '王明'@'localhost';</span><br><span class="line"># （8）撤销用户王明创建表的权限</span><br><span class="line">    REVOKE CREATE ON *.* FROM '王明'@'localhost';</span><br><span class="line"># （9）使得用户王明只能查看每个班级的最高分、最低分、平均分，但不能查看每个学生所选课程的具体成绩。</span><br><span class="line">    CREATE VIEW 班级统计数据 AS</span><br><span class="line">    SELECT 学生.系号, MAX(选课.成绩) AS 最高分, MIN(选课.成绩) AS 最低分, AVG(选课.成绩) AS 平均分 FROM 学生</span><br><span class="line">    JOIN 选课 ON 学生.学号 = 选课.学号</span><br><span class="line">    GROUP BY 学生.系号;</span><br><span class="line"></span><br><span class="line">    GRANT SELECT ON 班级统计数据 TO '王明'@'localhost';</span><br></pre></td></tr></tbody></table></figure><p>执行结果如下：<br><img src="D:\mynotes\source\images\image-20241107164210908.png" alt="执行结果"></p><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">CREATE TABLE 员工表(</span><br><span class="line">    工号 INT PRIMARY KEY,</span><br><span class="line">    姓名 VARCHAR(50),</span><br><span class="line">    每月工资 FLOAT</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">INSERT INTO 员工表(工号, 姓名, 每月工资) VALUES (1, '张三', 10000);</span><br><span class="line"></span><br><span class="line">DELIMITER //</span><br><span class="line"></span><br><span class="line">CREATE TRIGGER 工资修改约束</span><br><span class="line">    BEFORE UPDATE ON 员工表</span><br><span class="line">    FOR EACH ROW</span><br><span class="line">BEGIN</span><br><span class="line">    IF NEW.每月工资 &lt; OLD.每月工资 THEN</span><br><span class="line">        SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT = '工资不能降低！';</span><br><span class="line">    end if;</span><br><span class="line">end;</span><br><span class="line">//</span><br><span class="line">DELIMITER ;</span><br><span class="line"></span><br><span class="line">UPDATE 员工表 SET 每月工资 = 5000 WHERE 工号 = 1;</span><br></pre></td></tr></tbody></table></figure><p>测试结果如下</p><p><img src="D:\mynotes\source\images\image-20241107171704502.png" alt="image-20241107171704502"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/10/31/lun-wen-yue-du/blip2-lun-wen-bi-ji/"/>
      <url>/2024/10/31/lun-wen-yue-du/blip2-lun-wen-bi-ji/</url>
      
        <content type="html"><![CDATA[<h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li>通用的</li><li>计算高效：更少的可训练参数，因为直接使用预训练好的视觉模型、大语言模型（冻结了参数）</li></ol><h2 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h2><p><img src="D:\mynotes\source\images\deepLearning\BLIP2框架.png" alt="BLIP2框架图"></p><p>训练分为两个学习阶段：</p><ol><li>视觉和语言的表示学习阶段</li><li>视觉到语言的生成式学习阶段</li></ol><p>预训练好的视觉模型提供高质量的视觉表示，预训练好的大语言模型（LLM）有良好的语言生成能力和零样本迁移能力。这两个在训练时都会冻结参数。</p><p>然而，由于LLM在单模态预训练中没有看到图像，训练时冻结LLM的参数会使视觉-语言对齐特别具有挑战性。在这方面，现有的方法选择通过图像到文本的生成损失来弥补，但是这也不足以弥合模态差距。</p><p>因此为了通过冻结的单模态模型实现有效的视觉-语言对齐，该论文提出了一种采用新的两阶段预训练策略的查询变压器（Q-Former）。如图所示，Q-Former是一个轻量级的变压器，它使用一组可学习的查询向量从冻结的图像编码器中提取视觉特征。它充当了<strong>冻结的图像编码器和冻结的LLM之间的信息桥梁，为LLM提供最有用的视觉特征，以便LLM输出期望的文本</strong>。</p><h2 id="Q-Former"><a href="#Q-Former" class="headerlink" title="Q-Former"></a>Q-Former</h2><h3 id="第一阶段：视觉和语言的表示学习阶段"><a href="#第一阶段：视觉和语言的表示学习阶段" class="headerlink" title="第一阶段：视觉和语言的表示学习阶段"></a>第一阶段：视觉和语言的表示学习阶段</h3><p>目的：使用冻结的图像编码器，使Q-former的Queries能够<strong>学习提取到文本信息最丰富的视觉表示</strong>。</p><p>Q-Former有两个Transformer子模块组成，两个子模块共享相同的自注意力层</p><p><img src="D:\mynotes\source\images\deepLearning\BLIP2-stage1.png" alt="stage1"></p><p>共同优化了三个共享相同输入格式和模型参数的预训练目标。每个目标在查询和文本之间使用不同的注意力掩蔽策略来控制它们的交互。</p><h4 id="补充：BLIP框架图"><a href="#补充：BLIP框架图" class="headerlink" title="补充：BLIP框架图"></a>补充：BLIP框架图</h4><p><img src="D:\mynotes\source\images\deepLearning\BLIP框架图.png" alt="BLIP框架图"></p><p>从左到右：视觉编码器、文本编码器、基于图像的文本编码器、基于图像的文本解码器（相同颜色的部分参数共享）</p><ol><li>图像编码器：ViT的架构。将输入图像分割成一个个的 Patch 并将它们编码为一系列 Image Embedding，并使用额外的 [CLS] token 来表示全局的图像特征。</li><li>文本编码器：BERT 的架构。[CLS] token 附加到文本输入的开头以总结句子。作用是提取文本特征做对比学习。</li><li>基于图像的文本编码器：使用 Cross-Attention，作用是根据 ViT 给的图片特征和文本输入做二分类，所以使用的是编码器，且注意力部分是双向的 Self-Attention。添加一个额外的 [Encode] token，作为图像文本的联合特征。</li><li>基于图像的文本解码器：使用 Cross-Attention，作用是根据 ViT 给的图片特征和文本输入做文本生成的任务，所以使用的是解码器，且注意力部分是 Casual-Attention，目标是预测下一个 token。添加一个额外的 [Decode] token 和结束 token，作为生成结果的起点和终点。</li></ol><h4 id="图像文本对比学习"><a href="#图像文本对比学习" class="headerlink" title="图像文本对比学习"></a>图像文本对比学习</h4><p>计算每个查询输出和文本之间的两两相似度，然后选择最高的一个作为图像-文本相似度。</p><p>为了避免信息泄漏，这里采用了一个<strong>单模态的自注意掩码</strong>，其中查询Q和文本T不允许相互查看。</p><h4 id="基于图像的文本生成"><a href="#基于图像的文本生成" class="headerlink" title="基于图像的文本生成"></a>基于图像的文本生成</h4><p>这里使用了一个<strong>多模态的因果自注意掩模</strong>来控制查询-文本交互，Q可以看到彼此但是看不到T，T可以看到所有Q以及它之前的T。</p><h4 id="图像文本匹配"><a href="#图像文本匹配" class="headerlink" title="图像文本匹配"></a>图像文本匹配</h4><p>它是一个<strong>二分类</strong>任务，即模型只需判断图像-文本对是否匹配。</p><p>这里使用了一个<strong>双向的自注意掩码</strong>，即所有的查询和文本都可以相互查看彼此。</p><h3 id="第二阶段：视觉到语言的生成式学习阶段"><a href="#第二阶段：视觉到语言的生成式学习阶段" class="headerlink" title="第二阶段：视觉到语言的生成式学习阶段"></a>第二阶段：视觉到语言的生成式学习阶段</h3><p><img src="D:\mynotes\source\images\deepLearning\BLIP2-stage2.png" alt="stage2"></p><p>目的：在生成预训练阶段，将Q-Former（附带冻结图像编码器）连接到冻结LLM，以<strong>获得LLM的语言生成能力</strong>。</p><p>首先使用了一个全连接（FC）层来将输出查询嵌入Z线性地投影到与LLM的文本嵌入相同的维度中。投影的查询嵌入预先添加到输入的文本嵌入中。它们作为软视觉提示，使LLM适应从Q-Former提取的视觉特征。</p><p>由于Q-Former已经被预先训练来提取语言信息丰富的视觉表示，它有效地作为一个信息bottleneck，在去除无关的视觉信息的同时<strong>为LLM提供最有用的信息</strong>。这<strong>减轻了LLM学习视觉-语言对齐的负担</strong>，从而减轻了灾难性的遗忘问题。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/10/16/db/db-di-san-zhang-zuo-ye/"/>
      <url>/2024/10/16/db/db-di-san-zhang-zuo-ye/</url>
      
        <content type="html"><![CDATA[<h2 id="一、在school数据库中完成以下练习"><a href="#一、在school数据库中完成以下练习" class="headerlink" title="一、在school数据库中完成以下练习"></a>一、在school数据库中完成以下练习</h2><h3 id="sql语句"><a href="#sql语句" class="headerlink" title="sql语句"></a>sql语句</h3><figure class="highlight sql"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">-- 1 创建表，并定义相应的完整性约束</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> 系 (</span><br><span class="line">系号 <span class="type">INT</span>,</span><br><span class="line">系名 <span class="type">VARCHAR</span>(<span class="number">30</span>) <span class="keyword">UNIQUE</span>,</span><br><span class="line">系主任 <span class="type">VARCHAR</span>(<span class="number">15</span>),</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY(系号)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> 学生(</span><br><span class="line">学号 <span class="type">VARCHAR</span>(<span class="number">15</span>),</span><br><span class="line">姓名 <span class="type">VARCHAR</span>(<span class="number">10</span>),</span><br><span class="line">性别 <span class="type">CHAR</span>(<span class="number">1</span>) <span class="keyword">CHECK</span> (性别 <span class="keyword">IN</span> (<span class="string">'男'</span>, <span class="string">'女'</span>)),</span><br><span class="line">年龄 <span class="type">INT</span> <span class="keyword">CHECK</span> (年龄 <span class="keyword">BETWEEN</span> <span class="number">10</span> <span class="keyword">AND</span> <span class="number">50</span>),</span><br><span class="line">入学年份 <span class="type">VARCHAR</span>(<span class="number">10</span>),</span><br><span class="line">籍贯 <span class="type">VARCHAR</span>(<span class="number">50</span>),</span><br><span class="line">系号 <span class="type">INT</span>,</span><br><span class="line">手机号 <span class="type">VARCHAR</span>(<span class="number">15</span>) <span class="keyword">UNIQUE</span>,</span><br><span class="line">班长学号 <span class="type">VARCHAR</span>(<span class="number">10</span>),</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY(学号),</span><br><span class="line"><span class="keyword">FOREIGN</span> KEY(系号) <span class="keyword">REFERENCES</span> 系(系号),</span><br><span class="line"><span class="keyword">FOREIGN</span> KEY(班长学号) <span class="keyword">REFERENCES</span> 学生(学号)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> 课程 (</span><br><span class="line">课程号 <span class="type">VARCHAR</span>(<span class="number">15</span>),</span><br><span class="line">课程名 <span class="type">VARCHAR</span>(<span class="number">15</span>) <span class="keyword">UNIQUE</span>,</span><br><span class="line">先修课 <span class="type">VARCHAR</span>(<span class="number">15</span>),</span><br><span class="line">学分 <span class="type">DECIMAL</span>(<span class="number">2</span>,<span class="number">1</span>),</span><br><span class="line"><span class="keyword">CHECK</span> (学分<span class="operator">&gt;</span><span class="number">0</span> <span class="keyword">AND</span> 学分<span class="operator">&lt;</span><span class="number">5</span>),</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY(课程号)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> 选课 (</span><br><span class="line">学号 <span class="type">VARCHAR</span>(<span class="number">15</span>),</span><br><span class="line">课程号 <span class="type">VARCHAR</span>(<span class="number">15</span>),</span><br><span class="line">成绩 <span class="type">DECIMAL</span>(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY(学号, 课程号),</span><br><span class="line"><span class="keyword">FOREIGN</span> KEY(学号) <span class="keyword">REFERENCES</span> 学生(学号),</span><br><span class="line"><span class="keyword">FOREIGN</span> KEY(课程号) <span class="keyword">REFERENCES</span> 课程(课程号),</span><br><span class="line"><span class="keyword">CHECK</span> (成绩 <span class="keyword">BETWEEN</span> <span class="number">0</span> <span class="keyword">AND</span> <span class="number">100</span>)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2 将提供的上述各表的数据导入SQL Server</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3 在学生表中插入学号为26，姓名为'李四'，性别为'女'，年龄为20，入学年份为2008，籍贯为'广东'，手机号码为10010001000，班长学号为10的一条记录</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> 学生 <span class="keyword">VALUES</span>(<span class="number">26</span>, <span class="string">'李四'</span>, <span class="string">'女'</span>, <span class="number">20</span>, <span class="number">2008</span>, <span class="string">'广东'</span>, <span class="keyword">null</span>, <span class="number">10010001000</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4  删除上述记录</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">from</span> 学生 <span class="keyword">where</span> 学号 <span class="operator">=</span> <span class="number">26</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 5 将学生表中的姓名字段的长度改为6个汉字</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> 学生 MODIFY 姓名 <span class="type">VARCHAR</span>(<span class="number">18</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 6. 为学生表增加一个字段电子邮件，20个字符。</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> 学生 <span class="keyword">ADD</span> 电子邮件 <span class="type">VARCHAR</span>(<span class="number">20</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 7.对课程表的学分字段上的完整性约束进行修改，使其在0到6之间取值。</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> 课程 MODIFY 学分 <span class="type">DECIMAL</span>(<span class="number">2</span>,<span class="number">1</span>) <span class="keyword">CHECK</span> (学分<span class="operator">&gt;=</span><span class="number">0</span> <span class="keyword">AND</span> 学分<span class="operator">&lt;=</span><span class="number">6</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 8. 为学生表在学号列上创建cluster索引。</span></span><br><span class="line"><span class="comment">-- CACHE INDEX idx_学生_学号 ON 学生(学号);</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 9. 创建一个视图，计算每门课的最高分。</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> 每门课最高分 <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> 课程号, <span class="built_in">MAX</span>(成绩) <span class="keyword">AS</span> 最高分 <span class="keyword">FROM</span> 选课</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> 课程号;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 10.查找每个学生的学号、姓名、总成绩和平均分。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学生.学号, 姓名, <span class="built_in">SUM</span>(选课.成绩) <span class="keyword">AS</span> 总成绩, <span class="built_in">AVG</span>(选课.成绩) <span class="keyword">AS</span> 平均分 <span class="keyword">FROM</span> 学生</span><br><span class="line"><span class="keyword">JOIN</span> 选课 <span class="keyword">on</span> 学生.学号 <span class="operator">=</span> 选课.学号 <span class="keyword">GROUP</span> <span class="keyword">BY</span> 学生.学号;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 11.将6系所有学生的年龄，改为7系学生的平均年龄。</span></span><br><span class="line"><span class="keyword">SET</span> <span class="variable">@avg_age</span> <span class="operator">=</span> (<span class="keyword">SELECT</span> <span class="built_in">AVG</span>(年龄) <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 系号 <span class="operator">=</span> <span class="number">7</span>);</span><br><span class="line"><span class="keyword">UPDATE</span> 学生 <span class="keyword">SET</span> 年龄 <span class="operator">=</span> <span class="variable">@avg_age</span> <span class="keyword">WHERE</span> 系号 <span class="operator">=</span> <span class="number">6</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 12.将’曹洪’同学操作系统课程的成绩改为62分。</span></span><br><span class="line"><span class="keyword">UPDATE</span> 选课</span><br><span class="line"><span class="keyword">SET</span> 成绩 <span class="operator">=</span> <span class="number">62</span></span><br><span class="line"><span class="keyword">WHERE</span> 学号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 姓名 <span class="operator">=</span> <span class="string">'曹洪'</span>)</span><br><span class="line">  <span class="keyword">AND</span> 课程号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'操作系统'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 13. 查找所有学生的姓名、入学年份和籍贯。</span></span><br><span class="line"><span class="keyword">SELECT</span> 姓名, 入学年份, 籍贯 <span class="keyword">FROM</span> 学生;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 14.列出籍贯为'山东'的同学的所有属性。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 籍贯 <span class="operator">=</span> <span class="string">'山东'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 15.查找年龄最小的学生的学号和姓名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号, 姓名 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 年龄 <span class="operator">=</span> (<span class="keyword">SELECT</span> <span class="built_in">MIN</span>(年龄) <span class="keyword">FROM</span> 学生);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 16.查找选修了'数据库'的学生的学号。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 17. 查找选修了'编译技术'的女学生的学号和姓名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学生.学号, 姓名 <span class="keyword">FROM</span> 学生</span><br><span class="line">    <span class="keyword">JOIN</span> 选课 <span class="keyword">ON</span> 学生.学号 <span class="operator">=</span> 选课.学号</span><br><span class="line">        <span class="keyword">WHERE</span> 选课.课程号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'编译技术'</span>) <span class="keyword">AND</span> 性别 <span class="operator">=</span> <span class="string">'女'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 18. 查找'典韦'同学的班长所选修的课程的课程号。</span></span><br><span class="line"><span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 选课</span><br><span class="line"><span class="keyword">WHERE</span> 学号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 班长学号 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 姓名 <span class="operator">=</span> <span class="string">'典韦'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 19. 查找名字中倒数第二字为'侯'的学生的学号、姓名和所在系的系名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号, 姓名, 系.系名 <span class="keyword">FROM</span> 学生</span><br><span class="line"><span class="keyword">JOIN</span> 系 <span class="keyword">ON</span> 学生.系号 <span class="operator">=</span> 系.系号</span><br><span class="line"><span class="keyword">WHERE</span> <span class="built_in">SUBSTRING</span>(姓名, <span class="number">-2</span>, <span class="number">1</span>) <span class="operator">=</span> <span class="string">'侯'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 20. 查找名字以P打头，倒数第三字为L的课程的名字</span></span><br><span class="line"><span class="keyword">SELECT</span> 课程名 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="keyword">LIKE</span> <span class="string">'P%'</span> <span class="keyword">AND</span> <span class="built_in">SUBSTRING</span>(课程名, <span class="number">-3</span>, <span class="number">1</span>) <span class="operator">=</span> <span class="string">'L'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 21. 查找'甘宁'同学所有选修课程的总分。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">SUM</span>(成绩) <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 姓名 <span class="operator">=</span> <span class="string">'甘宁'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 22. 查找既选修了'数据库'，也选修了'操作系统'的同学。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 学号 <span class="keyword">IN</span> (</span><br><span class="line"><span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span></span><br><span class="line">           )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">INTERSECT</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 学号 <span class="keyword">IN</span> (</span><br><span class="line"><span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'操作系统'</span></span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 23. 查找没有选修'数据库'课程的学生的学号和姓名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号, 姓名 <span class="keyword">FROM</span> 学生</span><br><span class="line"><span class="keyword">WHERE</span> 学号 <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> 学号</span><br><span class="line">                   <span class="keyword">FROM</span> 选课</span><br><span class="line">                   <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 课程号</span><br><span class="line">                                   <span class="keyword">FROM</span> 课程</span><br><span class="line">                                   <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 24. 查找'数据库'课程及格了，但'编译技术'没有及格的学生的学号和姓名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号, 姓名 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 学号 <span class="keyword">IN</span> (</span><br><span class="line"><span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (</span><br><span class="line"><span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span></span><br><span class="line">) <span class="keyword">AND</span> 成绩 <span class="operator">&gt;=</span> <span class="number">60</span></span><br><span class="line">    <span class="keyword">INTERSECT</span></span><br><span class="line">    <span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (</span><br><span class="line"><span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'编译技术'</span></span><br><span class="line">) <span class="keyword">AND</span> 成绩 <span class="operator">&lt;</span> <span class="number">60</span></span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 25. 查找数据库成绩低于数据库课平均成绩的同学的学号和姓名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号, 姓名 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 学号 <span class="keyword">IN</span> (</span><br><span class="line"><span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课</span><br><span class="line"><span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span></span><br><span class="line">) <span class="keyword">AND</span> 成绩 <span class="operator">&lt;</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="built_in">AVG</span>(成绩) <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span>)</span><br><span class="line">)</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 26. 查找与'貂蝉'同学选修课程完全相同的学生的学号和姓名（不能多选也不能少选）。</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="variable">@diaochan_id</span> :<span class="operator">=</span> 学号 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 姓名 <span class="operator">=</span> <span class="string">'貂蝉'</span>;</span><br><span class="line"><span class="keyword">SELECT</span> 学生.学号, 姓名 <span class="keyword">FROM</span> 学生</span><br><span class="line"><span class="keyword">JOIN</span> 选课 <span class="keyword">ON</span> 学生.学号 <span class="operator">=</span> 选课.学号</span><br><span class="line"><span class="keyword">WHERE</span> 学生.学号 <span class="operator">&lt;&gt;</span> <span class="variable">@diaochan_id</span> <span class="comment">-- 排除自己</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> 学生.学号</span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">COUNT</span>(选课.课程号) <span class="operator">=</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(课程号) <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> <span class="variable">@diaochan_id</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">AND</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> ( <span class="comment">-- 没有选修貂蝉没有选的课程</span></span><br><span class="line">        <span class="keyword">SELECT</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">FROM</span> 选课 <span class="keyword">AS</span> T1</span><br><span class="line">        <span class="keyword">WHERE</span> T1.学号 <span class="operator">=</span> 学生.学号</span><br><span class="line">          <span class="keyword">AND</span> T1.课程号 <span class="keyword">NOT</span> <span class="keyword">IN</span> (</span><br><span class="line">            <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> <span class="variable">@diaochan_id</span></span><br><span class="line">        )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">AND</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> ( <span class="comment">-- 貂蝉没有选修这些学生没有选的课程</span></span><br><span class="line">        <span class="keyword">SELECT</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">FROM</span> 选课 <span class="keyword">AS</span> T2</span><br><span class="line">        <span class="keyword">WHERE</span> T2.学号 <span class="operator">=</span> <span class="variable">@diaochan_id</span></span><br><span class="line">          <span class="keyword">AND</span> T2.课程号 <span class="keyword">NOT</span> <span class="keyword">IN</span> (</span><br><span class="line">            <span class="keyword">SELECT</span> 课程号</span><br><span class="line">            <span class="keyword">FROM</span> 选课</span><br><span class="line">            <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> 学生.学号</span><br><span class="line">        )</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 27. 查找不仅选修了'貂蝉'同学选修的课程，而且还选修了其他课程的同学。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学生.学号, 姓名 <span class="keyword">FROM</span> 学生</span><br><span class="line"><span class="keyword">JOIN</span> 选课 <span class="keyword">ON</span> 学生.学号 <span class="operator">=</span> 选课.学号</span><br><span class="line"><span class="keyword">WHERE</span> 学生.学号 <span class="operator">&lt;&gt;</span> <span class="variable">@diaochan_id</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> 学生.学号</span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">COUNT</span>(选课.课程号) <span class="operator">&gt;</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(课程号) <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> <span class="variable">@diaochan_id</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">AND</span> <span class="keyword">EXISTS</span> ( <span class="comment">-- 选修了貂蝉没有选的课程</span></span><br><span class="line">    <span class="keyword">SELECT</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">FROM</span> 选课 <span class="keyword">AS</span> T1</span><br><span class="line">    <span class="keyword">WHERE</span> T1.学号 <span class="operator">=</span> 学生.学号</span><br><span class="line">      <span class="keyword">AND</span> T1.课程号 <span class="keyword">NOT</span> <span class="keyword">IN</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> <span class="variable">@diaochan_id</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"><span class="keyword">AND</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>( <span class="comment">-- 貂蝉没有选修这些学生没有选的课程</span></span><br><span class="line">    <span class="keyword">SELECT</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">FROM</span> 选课 <span class="keyword">AS</span> T1</span><br><span class="line">    <span class="keyword">WHERE</span> T1.学号 <span class="operator">=</span> <span class="variable">@diaochan_id</span> <span class="keyword">AND</span> T1.课程号 <span class="keyword">NOT</span> <span class="keyword">IN</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="operator">=</span> 学生.学号</span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 28. 查找'高等数学'平均成绩最高的系的系名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 系名 <span class="keyword">FROM</span> (<span class="keyword">SELECT</span> 系.系名, <span class="built_in">AVG</span>(成绩) <span class="keyword">AS</span> 平均成绩</span><br><span class="line">    <span class="keyword">FROM</span> 选课</span><br><span class="line">    <span class="keyword">JOIN</span> 学生 <span class="keyword">ON</span> 选课.学号 <span class="operator">=</span> 学生.学号</span><br><span class="line">    <span class="keyword">JOIN</span> 课程 <span class="keyword">ON</span> 选课.课程号 <span class="operator">=</span> 课程.课程号</span><br><span class="line">    <span class="keyword">JOIN</span> 系 <span class="keyword">ON</span> 学生.系号 <span class="operator">=</span> 系.系号</span><br><span class="line">    <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数学'</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> 系.系号) <span class="keyword">as</span> 平均成绩表</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> 平均成绩 <span class="keyword">DESC</span> LIMIT <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 29. 查找至少有一个籍贯为'四川'同学所选修的课程的课程名。</span></span><br><span class="line"><span class="keyword">SELECT</span> 课程名 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程号 <span class="keyword">IN</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 学号 <span class="keyword">IN</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 学生 <span class="keyword">WHERE</span> 籍贯 <span class="operator">=</span> <span class="string">'四川'</span></span><br><span class="line">    )</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 30. 查询选修了'数据库'课程的学生的学号和获得的学分。</span></span><br><span class="line"><span class="keyword">SELECT</span> 学号, <span class="built_in">SUM</span>(课程.学分) <span class="keyword">AS</span> 学分 <span class="keyword">FROM</span> 选课</span><br><span class="line">    <span class="keyword">JOIN</span> 课程 <span class="keyword">ON</span> 选课.课程号 <span class="operator">=</span> 课程.课程号</span><br><span class="line">    <span class="keyword">WHERE</span> 学号 <span class="keyword">IN</span> (<span class="keyword">SELECT</span> 学号 <span class="keyword">FROM</span> 选课 <span class="keyword">WHERE</span> 课程号 <span class="operator">=</span> (<span class="keyword">SELECT</span> 课程号 <span class="keyword">FROM</span> 课程 <span class="keyword">WHERE</span> 课程名 <span class="operator">=</span> <span class="string">'数据库'</span>))</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> 学号;</span><br></pre></td></tr></tbody></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>由于页面有限，部分查询结果只展示前几条）</p><h4 id="2"><a href="#2" class="headerlink" title="2"></a>2</h4><p>学生表导入存在问题，因为班长学号不存在，应调整学生顺序，使班长先导入</p><p>选课表导入存在问题，具体如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">0: Duplicate entry '0106-c04' for key '选课.PRIMARY'</span><br><span class="line">0: Duplicate entry '0108-c04' for key '选课.PRIMARY'</span><br><span class="line">0: Duplicate entry '0301-c03' for key '选课.PRIMARY'</span><br><span class="line">0: Duplicate entry '0317-c02' for key '选课.PRIMARY'</span><br><span class="line">0: Cannot add or update a child row: a foreign key constraint fails (`mysql`.`选课`, CONSTRAINT `选课_ibfk_1` FOREIGN KEY (`学号`) REFERENCES `学生` (`学号`))</span><br><span class="line">0: Cannot add or update a child row: a foreign key constraint fails (`mysql`.`选课`, CONSTRAINT `选课_ibfk_1` FOREIGN KEY (`学号`) REFERENCES `学生` (`学号`))</span><br></pre></td></tr></tbody></table></figure><p>（1）有主码重复的实体，需剔除；（2）表明有学号不在学生表里的</p><h4 id="3-7运行结果："><a href="#3-7运行结果：" class="headerlink" title="3-7运行结果："></a>3-7运行结果：</h4><p>其中3执行失败，因为班长的学号不存在</p><p><img src="D:\mynotes\source\images\altertable.png" alt="image-20241016200935130"></p><h4 id="8"><a href="#8" class="headerlink" title="8"></a>8</h4><p>MySQL 中无聚集索引概念，通常会在创建表时定义主键自动创建</p><h4 id="9"><a href="#9" class="headerlink" title="9"></a>9<img src="D:\mynotes\source\images\9.png" alt="9"></h4><h4 id="10"><a href="#10" class="headerlink" title="10"></a>10</h4><p><img src="D:\mynotes\source\images\image-20241016201258250.png" alt="10"></p><h4 id="11"><a href="#11" class="headerlink" title="11"></a>11</h4><p>学生表中不存在7系的学生，因此执行完后，6系学生的年龄会变为null</p><h4 id="12"><a href="#12" class="headerlink" title="12"></a>12</h4><p><img src="D:\mynotes\source\images\image-20241016201725960.png" alt="12"></p><h4 id="13"><a href="#13" class="headerlink" title="13"></a>13</h4><p><img src="D:\mynotes\source\images\image-20241016201850412.png" alt="13"></p><h4 id="14"><a href="#14" class="headerlink" title="14"></a>14</h4><p><img src="D:\mynotes\source\images\image-20241016201914397.png" alt="14"></p><h4 id="15"><a href="#15" class="headerlink" title="15"></a>15</h4><p><img src="D:\mynotes\source\images\image-20241016201935723.png" alt="15"></p><h4 id="16"><a href="#16" class="headerlink" title="16"></a>16</h4><p><img src="D:\mynotes\source\images\image-20241016201955201.png" alt="16"></p><h4 id="17"><a href="#17" class="headerlink" title="17"></a>17</h4><p><img src="D:\mynotes\source\images\image-20241016202021108.png" alt="17"></p><h4 id="18"><a href="#18" class="headerlink" title="18"></a>18</h4><p><img src="D:\mynotes\source\images\image-20241016202043466.png" alt="18"></p><h4 id="19"><a href="#19" class="headerlink" title="19"></a>19</h4><p><img src="D:\mynotes\source\images\image-20241016202102253.png" alt="19"></p><h4 id="20"><a href="#20" class="headerlink" title="20"></a>20</h4><p><img src="D:\mynotes\source\images\image-20241016202130156.png" alt="20"></p><h4 id="21"><a href="#21" class="headerlink" title="21"></a>21</h4><p><img src="D:\mynotes\source\images\image-20241016202157024.png" alt="21"></p><h4 id="22"><a href="#22" class="headerlink" title="22"></a>22</h4><p><img src="D:\mynotes\source\images\image-20241016202250751.png" alt="22"></p><h4 id="23"><a href="#23" class="headerlink" title="23"></a>23</h4><p><img src="D:\mynotes\source\images\image-20241016202322878.png" alt="23"></p><h4 id="24"><a href="#24" class="headerlink" title="24"></a>24</h4><p><img src="D:\mynotes\source\images\image-20241016202344397.png" alt="24"></p><h4 id="25"><a href="#25" class="headerlink" title="25"></a>25</h4><p><img src="D:\mynotes\source\images\image-20241016202404825.png" alt="25"></p><h4 id="26"><a href="#26" class="headerlink" title="26"></a>26</h4><p><img src="D:\mynotes\source\images\image-20241016215837520.png" alt="26"></p><h4 id="27"><a href="#27" class="headerlink" title="27"></a>27</h4><p><img src="D:\mynotes\source\images\image-20241016220848483.png" alt="27"></p><h4 id="28"><a href="#28" class="headerlink" title="28"></a>28</h4><p><img src="D:\mynotes\source\images\image-20241016220955411.png" alt="28"></p><h4 id="29"><a href="#29" class="headerlink" title="29"></a>29</h4><p>无</p><h4 id="30"><a href="#30" class="headerlink" title="30"></a>30</h4><p><img src="D:\mynotes\source\images\image-20241016221040514.png" alt="30"></p><h2 id="二、回答问题："><a href="#二、回答问题：" class="headerlink" title="二、回答问题："></a>二、回答问题：</h2><ol><li><p>SQL语言的特点。</p><ul><li><p>综合统一</p></li><li><p>高度非过程化</p></li><li><p>面向集合的操作方式</p></li><li><p>以同一种语法结构提供两种使用方式</p></li><li><p>语言简捷，易学易用</p></li></ul></li><li><p>创建一个数据库，需要创建几个文件，它们分别是做什么用的？它们对应于三级模式中的哪一级？创建的表存储在什么地方？它们对应于三级模式中的哪一级？</p><p><strong>数据库文件类型</strong>：</p><ol><li><strong>主数据文件（Primary Data File）</strong>：用于存储数据库的所有数据，包括用户数据、系统表、元数据等。每个数据库有且只有一个主数据文件。对应于<strong>内模式</strong>（物理模式），也就是数据库的物理存储结构。</li><li><strong>辅助数据文件（Secondary Data File）</strong>：当主数据文件无法容纳更多数据时，可以创建辅助数据文件来扩展数据库的存储容量。辅助数据文件是可选的，只有在需要扩展时才会创建。同样对应于<strong>内模式</strong>，用于物理存储扩展。</li><li><strong>日志文件（Transaction Log File）</strong>：用于记录数据库的所有事务活动（如插入、更新、删除等），确保在数据库出现故障时能够进行恢复。它主要负责事务管理与故障恢复。日志文件也属于<strong>内模式</strong>，管理数据库的事务和恢复过程。</li></ol><p><strong>表的存储位置</strong>：创建的表及其数据存储在主数据文件和辅助数据文件中（即 <code>.mdf</code> 或 <code>.ibd</code> 文件中）。</p><p><strong>表对应的三级模式</strong>：</p><ul><li><strong>内模式</strong>：表在物理存储上的实现方式，例如文件如何存储、数据如何在硬盘上分布、数据如何通过索引来访问。这是数据库系统内部如何管理存储的具体实现。</li><li><strong>概念模式</strong>：从用户角度看，表是逻辑层的数据结构（例如表的字段、主键、外键等）。在这个层次上，用户不需要关心数据的实际存储结构，只关心如何访问和操作表。</li></ul></li><li><p>可以为表定义哪些完整性约束？它们各自的作用是什么？</p><ul><li><strong>主键约束（PRIMARY KEY）</strong>：确保每一行都有唯一标识的字段，主键列的值不能重复，且不能为空。</li><li><strong>唯一约束（UNIQUE）</strong>：保证列或列的组合在表中具有唯一值，可以为空值，但多个空值允许存在。</li><li><strong>外键约束（FOREIGN KEY）</strong>：用于维护数据之间的参照完整性，确保一个表中的值必须在另一个表的主键或唯一列中存在。</li><li><strong>非空约束（NOT NULL）</strong>：防止列中出现空值，确保必须为该列插入有效数据。</li><li><strong>检查约束（CHECK）</strong>：用于限制列中的数据值范围或条件，确保数据符合指定的条件。</li></ul></li><li><p>自然连接和等值连接有什么差别？</p><ul><li><strong>自然连接</strong>自动基于两个表中相同的属性列进行连接，省去了连接条件</li><li><strong>等值连接</strong>是基于两个表中某一列（或多列）值相等的连接，通常需要使用ON或WHERE条件来指定连接的条件</li></ul></li><li><p>子查询分为哪几种？它们之间有什么区别？</p><p>子查询按照与外部查询的联系不同，分为<strong>普通子查询</strong>和<strong>相关子查询</strong></p><ul><li><p>普通子查询：与外部查询无关，可单独执行得一组值。</p></li><li><p>相关子查询：把外查询的列值作为检索条件的条件值</p></li></ul></li><li><p>索引有什么作用和缺点？</p><p><strong>作用</strong>：</p><ul><li>加速查询：通过创建索引可以显著加快数据的查询速度。</li><li>提高排序性能：索引有助于快速实现排序操作。</li><li>加速表连接：索引有助于在连接表时提高性能。</li></ul><p><strong>缺点</strong>：</p><ul><li>占用空间：索引会占用额外的存储空间，特别是大数据集上的多索引。</li><li>增加修改成本：对带有索引的表进行插入、删除或更新时，索引需要同步更新，增加了写操作的开销。</li></ul></li><li><p>基本表和视图有什么区别？视图有什么优点？什么样的视图是可以更新的？</p><p>区别：</p><ul><li>基本表：物理存在的表，存储实际数据。</li><li>视图：逻辑上的虚拟表，不存储数据，仅是对查询结果的命名。</li></ul><p>视图的优点：</p><ul><li>能够简化用户操作</li><li>使用户能够以多种角度看待同一数据</li><li>提供了一定程度的逻辑独立性</li><li>能够对数据提供安全保护</li></ul><p>可更新的视图：通常视图涉及单个表，并且视图中包含了基础表的主键和非计算列，才能进行更新。</p></li><li><p>请针对第三章SQL语言讲义中的除法例子，给出其他两种除法的实现方法。<br>假设需要查询选修了全部课程的学生姓名</p><figure class="highlight sql"><table><tbody><tr><td class="code"><pre><span class="line"><span class="operator">-</span> 方法一：使用<span class="keyword">group</span> by...having</span><br><span class="line"><span class="keyword">SELECT</span> Sname <span class="keyword">FROM</span> Student <span class="keyword">WHERE</span> Sno <span class="keyword">IN</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> Sno <span class="keyword">FROM</span>  SC <span class="keyword">GROUP</span> <span class="keyword">BY</span> Sno <span class="keyword">HAVING</span> <span class="built_in">COUNT</span>(SC.Cno)<span class="operator">=</span>(<span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(Cno) <span class="keyword">FROM</span> Course</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> 方法二：使用双<span class="keyword">not</span> <span class="keyword">exists</span></span><br><span class="line"><span class="keyword">Select</span> Sname <span class="keyword">From</span> Student</span><br><span class="line">    <span class="keyword">Where</span> <span class="keyword">Not</span> <span class="keyword">Exists</span> </span><br><span class="line">          (<span class="keyword">Select</span> <span class="operator">*</span> <span class="keyword">from</span> Course</span><br><span class="line">              <span class="keyword">Where</span> <span class="keyword">Not</span> <span class="keyword">Exists</span></span><br><span class="line">                  (<span class="keyword">Select</span> <span class="operator">*</span> <span class="keyword">from</span> SC</span><br><span class="line">                     <span class="keyword">Where</span> Sno<span class="operator">=</span>Student.Sno </span><br><span class="line">                          <span class="keyword">And</span> Cno<span class="operator">=</span>Course.Cno));</span><br><span class="line"></span><br><span class="line"><span class="operator">-</span> 方法三：参考第二章除运算的定义，将其翻译为<span class="keyword">sql</span>语句</span><br><span class="line"><span class="keyword">SELECT</span> Sname <span class="keyword">FROM</span> Student <span class="keyword">WHERE</span> Sno <span class="keyword">NOT</span> <span class="keyword">IN</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> Sno <span class="keyword">FROM</span> (</span><br><span class="line">        <span class="operator">-</span> 存在课程没选的学生</span><br><span class="line">        <span class="keyword">SELECT</span> Sno, Cno <span class="keyword">FROM</span> Student <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> (<span class="keyword">SELECT</span> Cno <span class="keyword">FROM</span> Course) <span class="keyword">AS</span> T1 <span class="operator">-</span> 所有学生都选择了所有课程的选课表</span><br><span class="line">        <span class="keyword">EXCEPT</span> <span class="keyword">SELECT</span> Sno, Cno <span class="keyword">FROM</span> SC <span class="keyword">AS</span> T2 <span class="operator">-</span> 做差运算，得出有课没选的情况</span><br><span class="line">) <span class="keyword">AS</span> T3</span><br><span class="line">);</span><br></pre></td></tr></tbody></table></figure></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/10/15/kai-yuan-ruan-jian-kai-fa-dao-lun/ge-ren-shi-jian-zuo-ye-1/"/>
      <url>/2024/10/15/kai-yuan-ruan-jian-kai-fa-dao-lun/ge-ren-shi-jian-zuo-ye-1/</url>
      
        <content type="html"><![CDATA[<h1 id="vue-js开源社区调研报告"><a href="#vue-js开源社区调研报告" class="headerlink" title="vue.js开源社区调研报告"></a>vue.js开源社区调研报告</h1><p>vue是前端开发中常用到的框架，目前市场上比较流行的前后端分离的开发模式，大多前端都是vue.js做的。我自己在开发中也经常在前端使用vue框架，为了对它有更多的了解，这次选择了vue.js开源项目作为调研的对象。</p><h4 id="贡献指南"><a href="#贡献指南" class="headerlink" title="贡献指南"></a>贡献指南</h4><p>在核心代码仓库的<a href="https://github.com/vuejs/core/blob/main/.github/contributing.md">contributing.md</a>文件中阐述了详细的贡献指南，关键要点如下：</p><blockquote><ul><li><p>Vue 核心有两个主要工作分支：主分支和次分支。 如果拉取请求是添加新 API 表面的功能，则应提交到次要分支。 否则，应针对主分支提交。 </p></li><li><p>确保勾选 “允许维护者编辑 “复选框。 这样能直接进行次要编辑/重构，节省大量时间。 </p></li><li><p>如果要<strong>添加新功能</strong>：</p><ul><li><p>要添加相应的测试用例。 </p></li><li><p>提供添加该功能的令人信服的理由。 <strong>最好先提一个suggestion issue，并在获得批准后再开展工作</strong>。</p></li></ul></li><li><p>如果是<strong>修复错误</strong>： </p><ul><li><p>如果是解决特殊问题，请在 PR 标题中添加（fix #xxxx[,#xxxx]）（#xxxx 是问题 id），以便更好地记录发布日志，例如更新实体编码/解码（fix #3899）。 </p></li><li><p>在 PR 中提供错误的详细描述。 最好提供现场演示。</p></li><li><p>如果适用，添加适当的测试覆盖率。 您可以通过运行 nr test-coverage 来检查代码的覆盖率。</p></li></ul></li><li><p><strong>确保测试通过</strong></p></li><li><p>提交信息必须遵循提交信息惯例，以便自动生成更新日志。 提交信息会在提交前自动验证（通过 simple-git-hooks 调用 Git 挂钩）。 </p></li><li><p>如果安装了 dev 依赖项，就无需担心代码风格问题。修改后的文件会在提交时自动使用 Prettier 格式化（通过 simple-git-hooks 调用 Git 挂钩）。</p></li></ul></blockquote><p>因此如果要对vue.js开源项目做贡献，我们应了解不同分支的作用，明确所要提交的代码属于那种类型的贡献，应提交到哪个分支。并在提交前做好充分的测试，提交时注意commit message的编写规范（见后文）。</p><h4 id="社区准则"><a href="#社区准则" class="headerlink" title="社区准则"></a>社区准则</h4><p>在<a href="https://cn.vuejs.org/about/coc.html">行为规范 | Vue.js (vuejs.org)</a>写明:</p><blockquote><p>有助于创造积极环境的行为包括：</p><ul><li>使用欢迎和包容的语言</li><li>尊重不同的观点和经历</li><li>优雅地接受建设性批评</li><li>关注社区利益</li><li>对其他社区成员表现出同理心</li></ul><p>  参与者不可接受的行为包括：</p><ul><li>使用性相关的语言或图像以及令人反感的性关注或挑逗</li><li>挑衅、侮辱性/贬损性评论、人身或政治攻击</li><li>进行公开或私下的骚扰</li><li>未经明确许可，公开他人的私人信息，如实际地址或电子地址</li><li>其他在专业环境中可能被合理认为不合适的行为</li></ul></blockquote><p> 可以看出核心要点是要尊重他人的观点以及隐私，交流、发布问题时需注意措辞，以打造和谐的社区环境。</p><h4 id="许可证"><a href="#许可证" class="headerlink" title="许可证"></a>许可证</h4><p>该开源项目使用的是MIT License(详见<a href="https://github.com/vuejs/core/blob/main/LICENSE">仓库此处</a>)。</p><p>MIT license是一种非常宽松和灵活的许可证，允许他人自由使用、复制、修改、合并、发布、分发、再授权和/或销售软件，只需在源代码中保留原始许可证和版权声明即可。这种许可证对商业应用非常友好，有助于vue的传播和使用。</p><h4 id="Commit-Message编写规范"><a href="#Commit-Message编写规范" class="headerlink" title="Commit Message编写规范"></a>Commit Message编写规范</h4><p>这部分在<a href="https://github.com/vuejs/core/blob/main/.github/commit-convention.md">commit-convention.md</a>中讲述，commit message需遵循如下正则匹配：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">/^(revert: )?(feat|fix|docs|dx|style|refactor|perf|test|workflow|build|ci|chore|types|wip)(\(.+\))?: .{1,50}/</span><br></pre></td></tr></tbody></table></figure><p>例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">fix(v-model): handle events on blur</span><br><span class="line"></span><br><span class="line">close #28</span><br></pre></td></tr></tbody></table></figure><p>该文档给出了commit message结构化的描述，对是否回退、提交类型、修改范围、主题等的要求做了详细阐述，有助于版本管理和追踪代码变更。</p><h4 id="issue报告与管理"><a href="#issue报告与管理" class="headerlink" title="issue报告与管理"></a>issue报告与管理</h4><p>vue.js有一个专门的网站<a href="https://new-issue.vuejs.org/">https://new-issue.vuejs.org/</a>用来做issue报告，如下图，该网站提供了清晰可视化的issue创建页面，可以选择issue的仓库、类型，填写主题、具体描述等。同时，GitHub仓库里也专门给出了issue模板（在.github/ISSUE_TEMPLATE下）</p><p><img src="D:\mynotes\source\images\vue-issue.png" alt="image-20241016192530985"></p><p>issue的处理流程在GitHub仓库给出，具体流程如下</p><p><img src="D:\Downloads\issue-workflow.png" alt="issue"></p><h4 id="个人理解与建议"><a href="#个人理解与建议" class="headerlink" title="个人理解与建议"></a>个人理解与建议</h4><p>vue.js是一个比较成熟、完善的开源项目，这次调研让我惊喜地发现vue官网上不仅有关于开源项目的相关信息，还有比较详尽的Vue教程，以及和vue相关的工具、插件、组件库等资源，它清晰得展示了vue工具的作用，便于快速上手实践，对于新手十分友好（而我刚开始接触vue的时候是直接选择在csdn、w3school等网站去学习如何使用vue的，这样搜集到的信息比较琐碎，并且往往忽略了一些深入的知识）。同时，分类清晰地网页页面、丰富的资源让开发者可以在这里一站式获取所需信息，提升开发效率。</p><p>官网中提到了丰富的参与到该开源项目中的方式，包括翻译文档、协助分流issue等方式，我认为有一些对于新手参与开源项目来说是比较友好的。vue的GitHub仓库也处于一个比较活跃的状态，issue、pull requests的最后一次操作时间都比较新，说明仍有很多工作者参与到它的开发中。</p><p>这次的调研让我对参与开源项目的流程有了比较深入的了解，也获得了一些经验，例如对于一个比较大型的开源项目，一般都会有有一个比较丰富、详尽、可视化的网站写明开源项目相关的信息，而代码仓库中也会有图片、markdown文件分类写出如何参与到开源项目中（这些文件名一般都比较固定，文件位置位于.github下，还是很容易检索到的）。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/27/db/db/"/>
      <url>/2024/09/27/db/db/</url>
      
        <content type="html"><![CDATA[<h2 id="数据库表设计修改"><a href="#数据库表设计修改" class="headerlink" title="数据库表设计修改"></a>数据库表设计修改</h2><ol><li><p>除学号、工号外<strong>id都改为<code>INT AUTO_INCREMENT</code>自增长整型</strong> </p></li><li><p>添加操作日志表</p></li><li><p>消息表调整为两个表</p></li><li><p>公告改名Notice -&gt; Annocement，因为消息会命名成Notify，区别一下</p></li></ol><p>补充：</p><p>消息类型：<strong>网上看到的，maybe我们只需要设计提醒类型的</strong></p><ul><li><p>公告：系统发送一则含有具体内容的消息，站内所有（指定部分）用户都能读取到这条消息 </p></li><li><p><strong>提醒</strong>：系统发送的一则消息，其文案<strong>格式固定</strong>，并且对特殊对象一般拥有<strong>超链接</strong> ，例如：</p><p>公告：xx人发布了一条主题为xxx的公告</p><p>报修：xx发起了报修请求；xx处理了您的报修请求</p><p>调宿：xx发起了调宿请求；xx处理了您的调宿请求</p></li><li><p>私信：用户发送给用户的一则消息，有具体的信息内容</p></li></ul><h2 id="前端补充"><a href="#前端补充" class="headerlink" title="前端补充"></a>前端补充</h2><ol><li><p>修改密码的功能</p></li><li><p>添加一个显示操作日志页面（学校管理员）</p></li></ol><h2 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h2><p>注意并发控制</p><h1 id="消息通知设计"><a href="#消息通知设计" class="headerlink" title="消息通知设计"></a>消息通知设计</h1><h2 id="消息的两种获取方式"><a href="#消息的两种获取方式" class="headerlink" title="消息的两种获取方式"></a>消息的两种获取方式</h2><ul><li>推 Push</li><li>拉 Pull</li></ul><p><strong>以知乎为例</strong><br> 推的比较常见，需要针对某一个问题维护着一张关注者的列表，每当触发这个问题推送的条件时（例如有人回答问题），就把这个通知发送给每个关注者。</p><p>拉的相对麻烦一点，就是推的反向，例如每个用户都有一张关注问题的列表，每当用户上线的时候，对每个问题进行轮询，当问题的事件列表出现了比我原本时间戳大的信息就进行拉取。</p><p><strong>而我们则根据消息的不同分类采用不同的获取方式</strong>：<br> 通告和提醒，适合使用拉取的方式，消息产生之后，会存在消息表中，用户在某一特定的时间根据自己关注问题的表进行消息的拉取，然后添加到自己的消息队列中，</p><p>信息，适合使用推的方式，在发送者建立一条信息之后，同时指定接收者，把消息添加到接收者的消息队列中。</p><h2 id="合理的开发流程"><a href="#合理的开发流程" class="headerlink" title="合理的开发流程"></a>合理的开发流程</h2><ol><li>确定业务，设计好所有数据库表的结构</li><li>前后端共同开发，注意写好api文档<ul><li>确定前端页面种类、数量和大致样式，前端开发</li><li>后端开发</li></ul></li></ol><p>ppt</p><ol><li><p>给出框架图（可以放系统功能结构图）</p></li><li><p>介绍三种用户</p></li><li><p>安全验证机制：双token</p></li><li><p>管理员身份的页面（数据流图、页面操作截图等）</p></li><li><p>宿管（数据流图、页面操作截图等）</p></li><li><p>学生身份的页面（数据流图、页面操作截图等）</p></li><li><p>登录、注册</p></li><li><p>导入、导出功能：只录制宿舍管理页面的这个功能就行了、其他一句话带过就行</p></li><li><p>多种条件排序及搜索：可以录学生页面的这个功能，因为这个页面支持地比较好，其他页面没bug就录，有bug就算了</p></li><li><p>三个业务：</p><ol><li>通知：录制一个 宿管发、学生变已读的流程；</li><li>报修、调宿申请：录制一个 学生发、宿管处理的流程，注意调宿现在只支持同宿舍楼的调宿申请</li></ol></li><li><p>消息通知：展示一下页面，能正常调转、变成已读就行</p></li><li><p>数据分析页面：能展示就行了</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/10/kai-yuan-ruan-jian-kai-fa-dao-lun/zuo-ye-2/"/>
      <url>/2024/09/10/kai-yuan-ruan-jian-kai-fa-dao-lun/zuo-ye-2/</url>
      
        <content type="html"><![CDATA[<h1 id="作业2"><a href="#作业2" class="headerlink" title="作业2"></a>作业2</h1><h4 id="1-PingCAP的商业模式是什么？"><a href="#1-PingCAP的商业模式是什么？" class="headerlink" title="1. PingCAP的商业模式是什么？"></a>1. PingCAP的商业模式是什么？</h4><p>社区迭代+企业级订阅服务+云服务</p><ul><li><p>社区迭代：在 PingCAP 的理解中， 开源社区由产品、 用户和贡献者三部分构成。这三个组成部分并非静态存在， 而是形成了一个不断循环转动的飞轮：用户使用产品时的真实场景驱动了新需求的产生， 社区齐心协力开发软件、 维护文档以更新产品， 更强大的产品又带来更多的用户和使用场景。大量的社区贡献者本身就是产品的直接或间接用户， 他们拥有开发能力。而他们背靠的是基数更大的、 对产品使用场景有着深刻认识但未必有能力直接进行代码和文档维护的用户群体， 并和他们在社区中有着紧密的、 端到端的接触。这些用户既促进了产品的开发迭代，同时社区吸引了目标用户了解产品后，用户觉得不错就会自动分享出去。</p></li><li><p>企业级订阅服务：PingCAP 会基于 LTS 版本提供 TiDB 企业版订阅服务，企业客户通过购买 PingCAP 企业订阅，商业用户可持续获得企业版产品演进带来的创新与能力提升，同时享有产品故障的及时诊断与修复和来自 PingCAP 产品支持专家的专业指导与支持。</p></li><li><p>云服务：通过向用户提供托管资源和基础设施并收取管理和租赁费用，这样企业级用户只需关注自己的业务以及如何更快将业务推向市场。从而使用户使用数据的心智负担降到最低，建立起用户对产品的信任，同时实现收益。</p><h4 id="2-TDengine的商业模式是什么？"><a href="#2-TDengine的商业模式是什么？" class="headerlink" title="2. TDengine的商业模式是什么？"></a>2. TDengine的商业模式是什么？</h4></li></ul><p>与PingCAP类似：</p><ul><li>开源社区模式：TDengine 作为开源项目，免费提供给开发者和企业使用。开源版本帮助公司建立社区用户基础，提高产品知名度，并通过社区贡献来推动产品迭代和优化。</li><li>企业版及订阅服务：对于大型企业用户，TDengine 提供了增强的企业版，包含高级功能、安全特性、数据处理能力以及定制化的部署方案。这部分通过订阅模式或一次性许可收费，类似于其他企业软件的收费模式。</li><li>云服务：TDengine Cloud 提供托管版时序数据库，企业可以通过云端服务来轻松部署和管理 TDengine 实例。此服务以订阅或按需计费模式向用户收费，主要面向不想自建和维护数据库的用户。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/08/kai-yuan-ruan-jian-kai-fa-dao-lun/zuo-ye-1/"/>
      <url>/2024/09/08/kai-yuan-ruan-jian-kai-fa-dao-lun/zuo-ye-1/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Richard-Stallman是谁？试评价其主要工作和观点。"><a href="#1-Richard-Stallman是谁？试评价其主要工作和观点。" class="headerlink" title="1. Richard Stallman是谁？试评价其主要工作和观点。"></a>1. Richard Stallman是谁？试评价其主要工作和观点。</h4><ul><li>Richard Stallman：自由软件运动的创始人。他在1983年发起了GNU项目，旨在开发一个完全自由的类Unix操作系统。他主张所有软件都应该是自由的，用户应拥有查看、修改和再分发软件的权利。为了保护这些权利，Stallman编写了GNU通用公共许可证（GPL），它确保软件即使被修改后依然保持开源和自由。</li><li>评价：Richard Stallman的工作对开源社区产生了深远影响。他不仅引导了自由软件运动，还为开源软件奠定了法律和道德基础。他的观点推动了自由软件的发展，自由软件理念已经深刻影响了计算机行业的发展。他的努力不仅推动了自由软件运动的兴起和发展，也为计算机行业的未来提供了新的可能性和机遇。然而，Stallman的观点也引起了一些争议。一些人认为他对“自由”概念的极端重视在某些情况下过于理想化。他的观点推动了自由软件的发展，但也可能阻碍了某些商业和用户友好型的开源模式的发展，对商业模式和知识产权保护持批判态度，可能会对软件产业的发展造成负面影响。</li></ul><h4 id="2-Linux内核是如何开发的，与Windows的开发有什么不同？"><a href="#2-Linux内核是如何开发的，与Windows的开发有什么不同？" class="headerlink" title="2. Linux内核是如何开发的，与Windows的开发有什么不同？"></a>2. Linux内核是如何开发的，与Windows的开发有什么不同？</h4><ul><li>开发模式：Linux采用开源的社区协作模式，开发者来自世界各地，贡献代码通过公共平台（如GitHub）进行协作。Windows则是由一个封闭的、内部团队开发，源代码是保密的。</li><li>开发过程：Linux社区使用开源工具和开源协议，任何人都可以提交代码，代码通过社区审核后被合并。Windows的开发流程则是内部进行的，由公司统一规划和管理。</li><li>自由度：Linux遵循GPL许可证，用户拥有自由修改和分发的权利；而Windows的代码是闭源的，用户只能按照微软的许可使用和修改。</li></ul><h4 id="3-RedHat是如何盈利的？"><a href="#3-RedHat是如何盈利的？" class="headerlink" title="3. RedHat是如何盈利的？"></a>3. RedHat是如何盈利的？</h4><p>RedHat 的收入主要来自向企业提供支持服务</p><ul><li>订阅服务：客户购买RedHat的企业版Linux订阅，以获取长期的技术支持、安全更新和软件维护。</li><li>定制服务：为企业客户提供量身定制的开源解决方案，并且提供额外的技术支持和咨询服务。</li><li>培训与认证：RedHat提供专门的开源技术培训和认证，企业为了提高员工技能，会付费参加这些课程。 这种盈利模式依赖于提供增值服务，而不是靠出售软件本身。</li></ul><h4 id="4-与闭源相比，开源有哪些优缺点？"><a href="#4-与闭源相比，开源有哪些优缺点？" class="headerlink" title="4. 与闭源相比，开源有哪些优缺点？"></a>4. 与闭源相比，开源有哪些优缺点？</h4><p>优点：</p><ul><li>透明性：开源软件的源代码是公开的，用户可以查看和验证代码的安全性，确保不存在恶意代码或后门。</li><li>灵活性：用户可以自由修改和定制软件，满足个性化需求。</li><li>协作和创新：开源软件通常通过社区协作开发，全球的开发者都可以贡献代码，从而加速创新和改进。</li><li>成本低：大多数开源软件是免费的，特别适合小企业和个人使用。</li></ul><p>缺点：</p><ul><li>支持有限：开源软件通常缺乏专业的客户支持，特别是对于不愿意付费的用户来说，遇到问题可能需要自行解决。</li><li>兼容性问题：由于开源软件的多样性，不同的软件或版本之间可能存在兼容性问题。</li><li>技术要求较高：使用开源软件通常需要一定的技术知识和专业能力。用户可能需要深入了解软件功能和配置，以便正确安装、集成和管理开源软件。</li><li>商业化挑战：对于开源软件的开发者来说，盈利模式较为复杂，往往依赖服务和支持，而不是直接销售软件。</li></ul><h4 id="5-你对中国发展开源有什么建议？"><a href="#5-你对中国发展开源有什么建议？" class="headerlink" title="5. 你对中国发展开源有什么建议？"></a>5. 你对中国发展开源有什么建议？</h4><ul><li>加强政策支持：政府部门可提供资金和政策支持，引导企业、学术界协同开展开源软、硬件的交互研究。推动开源基金会、开源行业协会等发展，鼓励中国开源基金会建立知识产权联盟。引导、发挥媒体平台在开源宣传推广中的重要作用。在高校、科研机构中培养开源教育和研究文化。</li><li>建立完善的开源生态体系：通过大力发展多样化社区运营组织形式，繁荣开源生态，同时不断提升基础设施生态影响力及安全保障能力，优化开源生态发展环境‌4。</li><li>促进企业与高校结合、加强教育培训：鼓励企业与高校、科研机构合作，建立开源软件研发和实施的合作框架，开设与开源相关的课程，以便普及开源知识，培养更多的开发者和专业人才，促进技术转移和创新。</li><li>强化国际交流与合作：积极参与国际开源社区，学习和借鉴国际开源软件的经验和成功案例。与其他国家和地区合作，加强开源技术的交流与合作，推动中国开源软件在全球范围内的影响力和竞争力。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/06/compiler/she-ji-wen-dang/"/>
      <url>/2024/09/06/compiler/she-ji-wen-dang/</url>
      
        <content type="html"><![CDATA[<h1 id="编译设计文档"><a href="#编译设计文档" class="headerlink" title="编译设计文档"></a>编译设计文档</h1><h2 id="参考编译器介绍"><a href="#参考编译器介绍" class="headerlink" title="参考编译器介绍"></a>参考编译器介绍</h2><h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><ol><li>模块化设计：编译器被划分为多个模块，每个模块负责编译过程中的一个特定任务。例如，有专门处理词法分析的模块（<code>insymbol</code>）、语法分析的模块（<code>block</code>）、代码生成的模块（<code>emit</code>系列函数）等。</li><li>递归下降解析：语法分析部分采用递归下降解析方法，通过一系列的过程（procedure）和函数（function）来识别和构建语法树。</li><li>三地址代码生成：在语法分析的同时进行语义分析和代码生成，生成三地址代码，这是一种中间表示形式，用于后续的优化和目标代码生成。</li><li>错误处理：能够在遇到语法错误时停止编译过程，并报告错误信息。</li><li>内存管理：用于管理符号表、代码存储等。</li></ol><h3 id="接口设计"><a href="#接口设计" class="headerlink" title="接口设计"></a>接口设计</h3><ol><li>标准输入输出：通过标准输入输出接口与用户交互，接收源代码文件和输出编译结果或错误信息。</li><li>文件接口：提供了接口来读取源代码文件和输出目标代码文件，以及错误信息和警告信息。</li><li>内部数据结构访问：内部使用了一系列数组和记录来存储符号表、代码等信息，这些数据结构通过一系列的过程和函数接口对外提供访问。</li><li>错误信息输出：提供了<code>errormsg</code>过程来输出错误信息，使得错误处理具有用户友好性。</li></ol><h3 id="文件组织"><a href="#文件组织" class="headerlink" title="文件组织"></a>文件组织</h3><ol><li>按照功能模块划分为不同的区块。</li><li>文件开始部分定义了编译器所需的常量和类型，如<code>const</code>和<code>type</code>部分。</li><li>全局变量声明</li><li>主要部分由一系列的过程和函数定义组成，这些是编译器的核心逻辑。</li><li>文件的最后部分是主程序的入口点，从这里开始执行编译器的流程。</li><li>源代码中包含了大量的注释，这些注释有助于理解代码的功能和目的。</li><li>代码按照功能逻辑组织，每个过程和函数都有明确的功能，且通过参数和返回值与其他模块交互。</li></ol><h2 id="编译器总体设计"><a href="#编译器总体设计" class="headerlink" title="编译器总体设计"></a>编译器总体设计</h2><p>我的编译器项目组织如下：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">src/</span><br><span class="line">├── backend</span><br><span class="line">│&nbsp;&nbsp; └── mips</span><br><span class="line">│&nbsp;&nbsp;     ├── data</span><br><span class="line">│&nbsp;&nbsp;     └── text</span><br><span class="line">├── frontend</span><br><span class="line">│&nbsp;&nbsp; ├── error</span><br><span class="line">│&nbsp;&nbsp; ├── lexer</span><br><span class="line">│&nbsp;&nbsp; └── parser</span><br><span class="line">├── midend</span><br><span class="line">│&nbsp;&nbsp; ├── llvm</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── global</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── instr</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; ├── stdioInstr</span><br><span class="line">│&nbsp;&nbsp; │&nbsp;&nbsp; └── type</span><br><span class="line">│&nbsp;&nbsp; └── symbol</span><br><span class="line">└── tree</span><br><span class="line">    ├── decl</span><br><span class="line">    ├── exp</span><br><span class="line">    └── func</span><br></pre></td></tr></tbody></table></figure><ol><li><code>frontend</code>：存放和源程序有关的前端部分，包括词法分析、语法分析以及错误输出的管理工具类。</li><li><code>midend</code>: 存放语义分析和中间代码生成部分，语义分析主要进行的是错误处理和符号表生成（<code>symbol</code>文件夹中），中间代码使用的是llvm</li><li><code>backend</code>：存放目标代码生成部分，生成的目标代码为mips，由data和text两部分组成，data中是关于全局变量的定义类，text中是指令类</li><li><code>tree</code>：存放语法树相关的类，每个语法成分对应一个类，根据功能的不同分别放在了三个目录下，主要是便于查找。</li></ol><h2 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h2><p>在Lexer类里解析词法。同时使用Token类（如下）管理每个token，其中在LexType是一个枚举类，枚举了所有token类别。</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Token</span> <span class="keyword">extends</span> <span class="title class_">Node</span> {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> LexType type;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String value;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> lineNum;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>注意对字符串和字符中转义字符的处理，根据文法指导书可知：</p><blockquote><ul><li>转义字符不包含<code>\r</code>，其余都有可能在字符常量和字符串常量中出现</li><li>printf里的字符串常量中转义字符只会出现<code>\n</code>，但其他地方的字符串常量仍可能出现除<code>\r</code>以外的转义字符;</li></ul></blockquote><p>这里我选择在词法解析时把转义字符按照字面量读入，也就是把一个转义字符保存成两个符号。需要注意字符串常量里读入双引号时，不要错误把它识别成字符串结束地方的双引号了。</p><h2 id="语法分析"><a href="#语法分析" class="headerlink" title="语法分析"></a>语法分析</h2><p>使用递归下降分析法，具体来说，在Parser类里解析语法：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Parser</span> {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;Token&gt; tokens;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> curTokenIndex;</span><br><span class="line">    <span class="keyword">private</span> Token curToken;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>词法分析阶段得到的token列表作为输入，采用递归下降的方式逐一解析token：为每个语法规则编写对应的语法解析方法，对于每个token，识别其属于哪条语法规则的FIRST集合的元素（必要时可能还需判断second），然后递归调用对应方法进行解析，最后得到语法树。</p><p>对于左递归的文法，如<code>AddExp → MulExp | AddExp ('+' | '−') MulExp</code>，解析时先假定是MulExp，如果后面还有<code>('+' | '−')</code> ，则为刚才的MulExp上面添一层AddExp。</p><p>树的每个结点用Node类表示：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Node</span> {</span><br><span class="line">    <span class="keyword">protected</span> ArrayList&lt;Node&gt; children;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">int</span> beginLine;</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">int</span> endLine;</span><br><span class="line">    <span class="keyword">protected</span> Symbol symbol; <span class="comment">// varDef, constDef, funcDef, funcFParams有意义</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>同时为每个语法成分建一个类，并使其继承Node类，方便后续处理。</p><h2 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h2><p>主要完成生成符号表和错误处理的部分。</p><h3 id="符号表"><a href="#符号表" class="headerlink" title="符号表"></a>符号表</h3><p>使用Symbol类来管理符号，各属性如下：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Symbol</span> {</span><br><span class="line">    <span class="keyword">protected</span> <span class="type">int</span> tableId;  <span class="comment">// 当前单词所在的符号表编号。</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> String name;    <span class="comment">// 当前单词所对应的字符串。</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> BType btype;    <span class="comment">// 0 -&gt; int, 1 -&gt; char</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">final</span> <span class="type">boolean</span> isConst;       <span class="comment">// 0 -&gt; const, 1 -&gt; var</span></span><br><span class="line">    <span class="keyword">protected</span> Node node; <span class="comment">// funcSymbol -&gt; funcDef, varSymbol -&gt;varDef/FuncFParam, arraySymbol -&gt; arrayDef/FuncFParam</span></span><br><span class="line">    <span class="keyword">protected</span> Value llvmValue;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>同时让ArraySymbol、FuncSymbol、VarSymbol继承Symbol，实现各自的特殊处理：</p><ol><li>数组符号需要记录长度</li><li>数组、变量符号需要记录初始值</li><li>函数符号需要记录参数表</li></ol><p>SymbolTable管理每张符号表。SymbolManager管理符号表生成，为单例模式，方便在全局各个地方调用。</p><p>在Node类实现一个semanticAnalysis的方法，用于做语义分析（/符号表生成），然后在必要的语法树的类里复写该方法。由于要处理作用域，因此我们需要在SymbolManager里设置属性记录必要的信息：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SymbolManager</span> {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">SymbolManager</span> <span class="variable">instance</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SymbolManager</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;SymbolTable&gt; symbolTablesList;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Stack&lt;SymbolTable&gt; stack;</span><br><span class="line">    <span class="comment">//表示循环嵌套的层数，主要是为了break、continue语句的错误处理</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">loop</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//函数参数需加入函数内部的符号表，需要延缓处理</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">needAddFuncFParams</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;Symbol&gt; paramSymbols;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">isFunc</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// 用于返回语句的错误处理</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="variable">returnStmt</span> <span class="operator">=</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">returnLine</span> <span class="operator">=</span> -<span class="number">1</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>我写了一个Error类来管理错误，按照题目的输出要求包含了必要的信息（当然为了方便自己查看和debug，可以多扩展一些信息或者复写toString）。</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Error</span> {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String type;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> lineNum;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>此外，我还实现了一个单例模式的ErrorList类，用来记录所有错误。</p><p>另外，对于重复命名的错误：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">左值表达式 LVal → Ident ['[' Exp ']'] // c</span><br></pre></td></tr></tbody></table></figure><p>该错误统一移到如下两处处理：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">语句 Stmt → LVal '=' Exp ';' // h</span><br><span class="line">|...</span><br><span class="line">语句 ForStmt → LVal '=' Exp // h</span><br></pre></td></tr></tbody></table></figure><h2 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h2><h3 id="生成中间代码"><a href="#生成中间代码" class="headerlink" title="生成中间代码"></a>生成中间代码</h3><p>在这里重新遍历语法树，生成llvm中间代码和重新生成符号表（symbol不重新构建，但是symbolTable重新构建，构建过程和语义分析的过程类似）。同时，类似SybmbolManager，用了一个单例模式的类IRBuilder管理一些要全局使用的东西以及基本块、指令的插入。</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">IRBuilder</span> {</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">IRBuilder</span> <span class="variable">irBuilder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IRBuilder</span>();</span><br><span class="line">    <span class="keyword">private</span> Module <span class="keyword">module</span>;</span><br><span class="line">    <span class="keyword">private</span> Function curFunc;</span><br><span class="line">    <span class="keyword">private</span> BasicBlock curBlock;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">loop</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 用来保存当前循环里break、continue语句需要跳转到的基本块</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Stack&lt;BasicBlock&gt; break2BlkStack = <span class="keyword">new</span> <span class="title class_">Stack</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Stack&lt;BasicBlock&gt; continue2BlkStack = <span class="keyword">new</span> <span class="title class_">Stack</span>&lt;&gt;();</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>根据llvm中“万物皆Value”的特性，我的结构如下：</p><p><img src="D:\mynotes\source\images\llvm.png" alt="llvm"></p><p>基本按照llvm的官方网站和实验讲座里的实现的，Value、User、Instruction是抽象类，Instruction继承自User（因为它需要记录使用者信息），Instruction下会实现各种指令。其他类都继承自Value。</p><p>同时参考官方网站，实现了类型架构，LlvmType会作为Value的属性，相当于它的返回值类型。</p><p><img src="D:\mynotes\source\images\llvmType.png" alt="llvmType"></p><p>在Node类中定义genIR()方法，同时在每个子类中复写该方法，实现具体逻辑。生成中间代码的过程就是递归调用该方法的过程。</p><p>需要注意的点：</p><ol><li>llvm代码中涉及到三种数据类型i32、i8、i1，不同类型的数据不能直接运算，因此在运算时还需要注意做相互转换，具体细节如下图：</li></ol><p><img src="D:\mynotes\source\images\image-20241130143358883.png" alt="image-20241130143358883"></p><ol start="2"><li><p>数组的初始化，教程中提到：</p><blockquote><p>对于任何有初始值的字符数组，编译器应该在初始化时将未使用的部分置0</p></blockquote><p>所以数组后面要记得补0，尤其是对于局部数组（debug的血泪教训&gt;_&lt;）。</p></li><li><p>对于条件（对应非终结符 <code>Cond</code>）的解析，主要涉及三个基本块：条件为真跳转的目标块，条件为假跳转的目标块，以及条件的运算所属的基本块。这三个块根据表达式的不同（如 <code>if</code>，<code>for</code>）而有一些区别，但是都符合这一模式。在解析前，就准备好这三个基本块，但暂时不将其插入函数中，而到开始解析该基本块时再插入（即把该基本块设置成当前基本块时，<code>IRBuilder.getInst().setCurBlock(trueBlock)</code>。</p></li><li><p>对于循环（对应for）的解析，和上述类似，涉及五个基本块，如下图（来源于教程），在我的实现里我把后面四个分别叫做condBlock、loopBlock、stepBlock、endBlock。注意condBlock、stepBlock不一定有。同时让IRBuilder记录endBlock, stepBlock供解析break、continue语句时使用。</p></li></ol><p><img src="D:\mynotes\source\images\for逻辑流程.png" alt="for"></p><ol start="5"><li><p>LLVM 中需要对 <code>\n</code> 和 <code>\0</code> 进行转义。</p></li><li><p>getelementptr指令的格式比较多，我的实现细节如下，这样保证每个a[i]都只需要一条指令就能取到地址。</p></li></ol><figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// 给字符数组赋值，第一个字符赋值用这个</span></span><br><span class="line">%<span class="number">1</span> = getelementptr [<span class="number">5</span> x i32], [<span class="number">5</span> x i32]* @a, i32 <span class="number">0</span>, i32 <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 不使用</span></span><br><span class="line">%<span class="number">2</span> = getelementptr [<span class="number">5</span> x i32], [<span class="number">5</span> x i32]* @a, i32 <span class="number">0</span> </span><br><span class="line">%<span class="number">3</span> = getelementptr i32, i32* %<span class="number">2</span>, i32 <span class="number">3</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">// 给字符数组赋值，除第一个字符赋值外，其他用这个</span></span><br><span class="line"><span class="comment">// c = s[i]</span></span><br><span class="line">%<span class="number">3</span> = getelementptr i32, i32* @a, i32 <span class="number">3</span></span><br></pre></td></tr></tbody></table></figure><ol start="7"><li>一个block块只能有一条跳转类型的指令（br/ret），因此需要删除第一条跳转后面的指令</li></ol><h3 id="目标代码生成"><a href="#目标代码生成" class="headerlink" title="目标代码生成"></a>目标代码生成</h3><p>在llvm中的Value类编写方法<code>genMips</code>，然后在函数、基本块、指令类中复写该方法，实现递归生成目标代码。</p><p>一条llvm指令对应一块栈空间或者一个寄存器，其中<strong>每条alloca指令一定对应一块栈空间</strong>。</p><p>Value中的index属性存储Instruction在Function里的序列，与id的不同在于：即使该Instruction没有返回值，其index也有意义；而id用于生成虚拟寄存器（就是%1,%2,…)，没返回值的Instruction的id没有意义。</p><h2 id="代码优化"><a href="#代码优化" class="headerlink" title="代码优化"></a>代码优化</h2><h3 id="乘法优化"><a href="#乘法优化" class="headerlink" title="乘法优化"></a>乘法优化</h3><p>有如下几种情况：</p><ul><li>乘以0 rd寄存器直接赋0</li><li>乘以1 把rs寄存器move到rd寄存器</li><li>乘以2的幂：改为sll语句</li><li>其他按普通乘法处理</li></ul><p>关键代码：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (m == <span class="number">1</span>) {</span><br><span class="line">    <span class="keyword">if</span> (rd != reg1) {</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MoveText</span>(rd, reg1);</span><br><span class="line">    }</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> (m == <span class="number">0</span>) {</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">LiText</span>(rd, <span class="number">0</span>);</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> ((m &amp; (m - <span class="number">1</span>)) == <span class="number">0</span>) { <span class="comment">// 2的幂</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> ((m &amp; (<span class="number">1</span> &lt;&lt; k)) == <span class="number">0</span>) {</span><br><span class="line">        k++;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SLL, rd, reg1, k);</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">LiText</span>(reg2, m); <span class="comment">// 重新加载operand2</span></span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">MDText</span>(MDText.Op.MULT, reg1, reg2);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">MFText</span>(MFText.MFType.MFLO, rd);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="除法优化"><a href="#除法优化" class="headerlink" title="除法优化"></a>除法优化</h3><p>不能简单地把2的幂翻译成srl（会有溢出问题），因此我按照讲座里给出的公式来算</p><p><img src="D:\mynotes\source\images\image-20241130102745452.png" alt="image-20241130102745452"></p><p>关键代码：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (d == <span class="number">1</span>) {</span><br><span class="line">    <span class="keyword">if</span> (rd != reg1) {</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MoveText</span>(rd, reg1);</span><br><span class="line">    }</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> ((d &amp; (d - <span class="number">1</span>)) == <span class="number">0</span>){ <span class="comment">// 2的幂</span></span><br><span class="line">    <span class="comment">// 使用公式：reg1/d= SRA(reg1 + SRL(SRA(reg1, k − 1), 32 − k), k)</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> ((d &amp; (<span class="number">1</span> &lt;&lt; k)) == <span class="number">0</span>) {</span><br><span class="line">        k++;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, reg2, reg1, k - <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRL, reg2, reg2, <span class="number">32</span> - k);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, reg2, reg2, reg1);</span><br><span class="line">    <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, rd, reg2, k);</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">    <span class="comment">// 处理一般情况 d &gt; 0 且非 2 的幂</span></span><br><span class="line">    <span class="type">int</span> <span class="variable">N</span> <span class="operator">=</span> <span class="number">31</span>; <span class="comment">// 假设常量位数为 32 位</span></span><br><span class="line">    <span class="type">long</span> <span class="variable">m</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 确定 m 和 l</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) {</span><br><span class="line">        l++;</span><br><span class="line">        m = (<span class="type">long</span>) Math.ceil((<span class="type">double</span>) (<span class="number">1L</span> &lt;&lt; (N + l)) / d);</span><br><span class="line">        <span class="keyword">if</span> (m * d &lt;= (<span class="number">1L</span> &lt;&lt; (N + l)) + (<span class="number">1L</span> &lt;&lt; l)) {</span><br><span class="line">            <span class="keyword">break</span>; <span class="comment">// 满足条件，退出循环</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (m &lt; (<span class="number">1L</span> &lt;&lt; N)) {</span><br><span class="line">        <span class="comment">// 使用公式: SRA(MULSH(m, n), l − 1) + SRL(n, 31)</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">LiText</span>(reg2, (<span class="type">int</span>) m);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MDText</span>(MDText.Op.MULT, reg1, reg2);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MFText</span>(MFText.MFType.MFHI, reg2);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, reg2, reg2, l - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRL, reg1, reg1, <span class="number">31</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, rd, reg2, reg1);</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        <span class="comment">// 使用公式: SRA(n + MULSH(m − 2^32, n), l − 1) + SRL(n, 31)</span></span><br><span class="line">        m -= (<span class="number">1L</span> &lt;&lt; <span class="number">32</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">LiText</span>(reg2, (<span class="type">int</span>) m);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MDText</span>(MDText.Op.MULT, reg1, reg2);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">MFText</span>(MFText.MFType.MFHI,reg2);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, reg2, reg2, reg1);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRA, reg2, reg2, l - <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.SRL, reg1, reg1, <span class="number">31</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">ALUText</span>(ALUText.Op.ADDU, rd, reg2, reg1);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="基本块合并"><a href="#基本块合并" class="headerlink" title="基本块合并"></a>基本块合并</h3><p>若一个基本块无条件跳转到它的下一个基本块，且下一个基本块只有一个前驱，那么可以直接合并这两个基本块，省去跳转指令。</p><p>关键代码：</p><figure class="highlight java"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> (bbIter.hasNext()) {</span><br><span class="line">    <span class="type">BasicBlock</span> <span class="variable">curBB</span> <span class="operator">=</span> bbIter.next();</span><br><span class="line">    <span class="keyword">if</span> (prevBB.getInstrs().getLast() <span class="keyword">instanceof</span> BR br &amp;&amp; br.isUncondition() &amp;&amp; br.getDest() == curBB</span><br><span class="line">            &amp;&amp; curBB.getUsers().size() == <span class="number">1</span>) { <span class="comment">// curBB只有preBB一个前驱</span></span><br><span class="line">        prevBB.removeInstr(br);</span><br><span class="line">        <span class="keyword">for</span> (Instruction instr : curBB.getInstrs()) {</span><br><span class="line">            prevBB.addInstr(instr);</span><br><span class="line">            instr.setItsBB(prevBB);</span><br><span class="line">        }</span><br><span class="line">        bbIter.remove();</span><br><span class="line">    } <span class="keyword">else</span> {</span><br><span class="line">        prevBB = curBB;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="自己编写的测试程序"><a href="#自己编写的测试程序" class="headerlink" title="自己编写的测试程序"></a>自己编写的测试程序</h2><ol><li><p>str的输出和长度计算</p></li><li><p>多于4个参数的函数</p></li><li><p>函数调用时实参为常数</p></li><li><p>数组初始化时有表达式</p></li><li><p>数组某元素作为if中的判断条件</p></li><li><p>递归调用的函数</p></li><li><p>除法优化</p></li><li><p>测试基本块合并优化</p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/07/19/ge-ren-zuo-ye-ji-zhu-bao-gao/"/>
      <url>/2024/07/19/ge-ren-zuo-ye-ji-zhu-bao-gao/</url>
      
        <content type="html"><![CDATA[<h1 id="选择题目：3-糖尿病诊断"><a href="#选择题目：3-糖尿病诊断" class="headerlink" title="选择题目：3 糖尿病诊断"></a>选择题目：3 糖尿病诊断</h1><h2 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h2><ol><li>数据集中存在为布尔值的属性列，先把它们转化成数字（0：False, 1：True)</li><li>把标签列提取出来编码，采用标签编码即可（0: NoDiabetes, 1: Prediabetes, 2: Diabetes）</li></ol><p>相关代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_data</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># 将布尔值转换为数值</span></span><br><span class="line">    data = data.replace({<span class="literal">True</span>: <span class="number">1</span>, <span class="literal">False</span>: <span class="number">0</span>}).infer_objects(copy=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将 Diabetes 列编码为数值（0: Diabetes, 1: NoDiabetes, 2: Prediabetes）</span></span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    data[<span class="string">'Diabetes'</span>] = le.fit_transform(data[<span class="string">'Diabetes'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data, le</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>数据的样本涉及到特征种类较多，为避免无关特征的影响，我们先对特征进行选择，找出与标签（即糖尿病的情况）相关性最高的几个特征。然后将特征进行标准化，让不同特征的数值范围一致，这样可以让模型更好地进行训练和收敛。</p><p>相关代码在<code>model_search.py</code>文件中:</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 特征选择</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feature_selection</span>(<span class="params">data, top_k=<span class="number">9</span></span>):</span><br><span class="line">    X = data.iloc[:, <span class="number">1</span>:-<span class="number">1</span>]  <span class="comment"># 特征变量</span></span><br><span class="line">    Y = data.iloc[:, -<span class="number">1</span>]  <span class="comment"># 目标变量</span></span><br><span class="line"></span><br><span class="line">    select_top_k = SelectKBest(score_func=chi2, k=top_k)  <span class="comment"># 选择排名前k的特征</span></span><br><span class="line">    fit = select_top_k.fit(X, Y)</span><br><span class="line">    features = fit.transform(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取特征名称</span></span><br><span class="line">    selected_features_indices = fit.get_support()</span><br><span class="line">    selected_feature_names = X.columns[selected_features_indices].tolist()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"选择的特征:"</span>, selected_feature_names)</span><br><span class="line">    <span class="keyword">return</span> features, selected_feature_names</span><br></pre></td></tr></tbody></table></figure><h2 id="模型探索"><a href="#模型探索" class="headerlink" title="模型探索"></a>模型探索</h2><p>该问题是一个分类问题，数据集比较易于处理，考虑从简单的模型开始，如逻辑回归或决策树，逐步过渡到更复杂的模型，如随机森林或支持向量机。这里我选择了如下几种模型：</p><ol><li>逻辑回归（Logistic Regression）</li><li>高斯朴素贝叶斯（Gaussian Naive Bayes）</li><li>K近邻分类（K-Nearest Neighbors）</li><li>决策树分类（Decision Tree Classifier）</li><li>支持向量机（Support Vector Machine, SVM）</li><li>XGBoost</li></ol><p>通过交叉检验对每个模型进行评估，以下是各模型在训练集上的交叉验证mP：</p><p><img src="D:\mynotes\source\images\image-20241201132158703.png" alt="image-20241201132158703"></p><p>SVM耗时较长，后续补充XGBoost模型时没有评估SVM：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">LR 0.8461940174191783</span><br><span class="line">NB 0.7803187407247</span><br><span class="line">KNN 0.8286078911982038</span><br><span class="line">DT 0.7842212562301403</span><br><span class="line">XGB 0.8481994913743092</span><br></pre></td></tr></tbody></table></figure><p><img src="D:\mynotes\source\images\image-20241220185448750.png" alt="image-20241220185448750"></p><p>可以看到逻辑回归、支持向量机、XGBoost的结果较好，因此可以选择这些模型进行接下来的探索。</p><p>这部分的关键代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#模型选择</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_selection</span>(<span class="params">X_train, Y_train</span>):</span><br><span class="line">    models = []</span><br><span class="line">    models.append((<span class="string">"LR"</span>, LogisticRegression()))  <span class="comment"># 逻辑回归</span></span><br><span class="line">    models.append((<span class="string">"NB"</span>, GaussianNB()))  <span class="comment"># 高斯朴素贝叶斯</span></span><br><span class="line">    models.append((<span class="string">"KNN"</span>, KNeighborsClassifier()))  <span class="comment"># K近邻分类</span></span><br><span class="line">    models.append((<span class="string">"DT"</span>, DecisionTreeClassifier()))  <span class="comment"># 决策树分类</span></span><br><span class="line">    models.append((<span class="string">"XGB"</span>, XGBClassifier()))  <span class="comment"># XGBoost分类</span></span><br><span class="line">    <span class="comment"># models.append(("SVM", SVC(kernel='linear')))  # 支持向量机分类</span></span><br><span class="line">    <span class="comment">#模型评估</span></span><br><span class="line">    results = []</span><br><span class="line">    names = []</span><br><span class="line">    <span class="keyword">for</span> name, model <span class="keyword">in</span> models:</span><br><span class="line">        kfold = KFold(n_splits=<span class="number">10</span>)</span><br><span class="line">        cv_result = cross_val_score(</span><br><span class="line">            model, X_train, Y_train, cv=kfold, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">        names.append(name)</span><br><span class="line">        results.append(cv_result)</span><br><span class="line">    <span class="comment">#输出模型评估结果</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(names)):</span><br><span class="line">        <span class="built_in">print</span>(names[i], results[i].mean())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结果可视化</span></span><br><span class="line">    ax = sns.boxplot(data=results)</span><br><span class="line">    ax.set_xticklabels(names)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure><h2 id="平衡数据集"><a href="#平衡数据集" class="headerlink" title="平衡数据集"></a>平衡数据集</h2><p>注意到数据集中NoDiabetes的类别明显比Diabetes和PreDiabetes多，可能导致结果预测不平衡，因此class_weighted选择‘balanced’。这个可以显著提高结果的mP。</p><p>另外还采用了SMOTE过采样：</p><p>SMOTE 的作用</p><ol><li><strong>平衡数据集</strong>:<ul><li>通过生成合成的少数类样本，使数据集中的各个类别的样本数量更加平衡。</li><li>这有助于模型更好地学习少数类的特征，提高其预测能力。</li></ul></li><li><strong>减少过拟合</strong>:<ul><li>当数据集不平衡时，模型可能会对多数类进行过度拟合，而对少数类拟合不足。</li><li>使用 SMOTE 增加少数类样本可以减少这种偏向，提高模型的泛化能力。</li></ul></li><li>提高模型性能</li></ol><p>SMOTE 的工作原理</p><ol><li>首先，从少数类中随机选择一个样本。</li><li>计算该样本与所有其他少数类样本之间的距离，找到最近的几个样本。</li><li>在选择的样本与其最近邻之间随机选择一个点，并生成一个新的合成样本。这个新样本是通过线性插值在两个样本之间的某一位置生成的。</li><li>重复上述过程，直到少数类的样本数量达到指定的平衡比例。</li></ol><p>相关代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 过采样</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用 SMOTE 进行过采样</span></span><br><span class="line">smote = SMOTE()</span><br><span class="line">X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)</span><br></pre></td></tr></tbody></table></figure><h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>使用网格搜索对参数进行调优，关键代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment"># 使用网格搜索进行调参，并设置 class_weight</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置逻辑回归的超参数范围</span></span><br><span class="line">param_grid = {</span><br><span class="line">    <span class="string">'C'</span>: [<span class="number">0.005</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>],  <span class="comment"># 正则化参数</span></span><br><span class="line">    <span class="string">'solver'</span>: [<span class="string">'lbfgs'</span>, <span class="string">'liblinear'</span>],  <span class="comment"># 求解器</span></span><br><span class="line">    <span class="string">'max_iter'</span>: [<span class="number">100</span>, <span class="number">150</span>, <span class="number">200</span>]  <span class="comment"># 最大迭代次数</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(LogisticRegression(class_weight=<span class="string">'balanced'</span>), param_grid, cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>)</span><br><span class="line">grid_search.fit(X_train_resampled, Y_train_resampled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最佳参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"最佳参数: <span class="subst">{grid_search.best_params_}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"最佳得分: <span class="subst">{grid_search.best_score_}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用最佳参数训练最终模型</span></span><br><span class="line">best_model = grid_search.best_estimator_</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>最优参数如图：</p><p><img src="D:\mynotes\source\images\img.png" alt="调参"></p><h2 id="附：实验结果记录"><a href="#附：实验结果记录" class="headerlink" title="附：实验结果记录"></a>附：实验结果记录</h2><p>文件夹中有网站的分数截图，下表是对这些分数的具体说明</p><table><thead><tr><th>模型</th><th>分数</th></tr></thead><tbody><tr><td>LR</td><td>0.383851</td></tr><tr><td>LR+param grid</td><td>0.382004</td></tr><tr><td>LR+param_grid+class_weighted</td><td>0.506784</td></tr><tr><td>随机森林</td><td>0.338977</td></tr><tr><td>svm</td><td>0.333333</td></tr><tr><td>XGBoost</td><td>0.372797</td></tr><tr><td>XGBoost+param_grid</td><td>0.36659</td></tr><tr><td>集成学习LR+RF+XGBoost</td><td>0.384489</td></tr><tr><td></td><td></td></tr></tbody></table><p>XGBoost:</p><p>最佳参数: {‘subsample’: 0.9, ‘n_estimators’: 200, ‘max_depth’: 7, ‘learning_rate’: 0.1, ‘colsample_bytree’: 0.8}<br>最佳得分: 0.7601263601263601</p><p>LR:</p><p>最佳参数: {‘C’: 1, ‘max_iter’: 100, ‘solver’: ‘lbfgs’}<br>最佳得分: 0.5195390195390195</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/07/06/db/springboot/"/>
      <url>/2024/07/06/db/springboot/</url>
      
        <content type="html"><![CDATA[<h1 id="springboot配置中遇到的问题"><a href="#springboot配置中遇到的问题" class="headerlink" title="springboot配置中遇到的问题"></a>springboot配置中遇到的问题</h1><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>idea第一次连接MySQL数据库出现Failed to download ‘<a href="https://download.jetbrains.com/idea/jdbc-drivers/MySQL/8.1/LICENSE.txt">https://download.jetbrains.com/idea/jdbc-drivers/MySQL/8.1/LICENSE.txt</a>‘: Connect timed out</p><p>解决办法：</p><p>step1:点击Drivers</p><p><img src="/..%5Cimages%5Cerror_1.png" alt="error_1"></p><p>step2:在侧边栏找到MySQL，然后点击红色框里的DownLoad（我已经下载好了，所以现在这里没有这个选项了,不然的话这个框里会有字，并且框所在的选项的字都是红色的）</p><p><img src="D:\mynotes\source\images\error_2.png" alt="step2"></p><h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>连接mysql报错<code>2003-Can‘t connect to MySql server on ‘localhost‘(10061 "Unknown error" )</code></p><p>解决办法：windows+r后，输入<code>services.msc</code>，在服务里启动MySql服务</p><h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>拉取别人的项目后maven配置报错</p><p>解决办法：File–&gt;Maven–&gt;Reload project</p><h1 id="springboot架构层次"><a href="#springboot架构层次" class="headerlink" title="springboot架构层次"></a>springboot架构层次</h1><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>业务逻辑层，封装了应用程序的核心业务逻辑。</p><p>这部分代码通常不直接处理http请求或响应，而是处理数据验证、计算、事务管理等任务。</p><p>可以被Controller层通过依赖注入(<code>@Autowired</code>)来调用</p><h2 id="Controller"><a href="#Controller" class="headerlink" title="Controller"></a>Controller</h2><p>应用程序的前端控制器</p><p>主要负责:</p><ol><li>接收来自用户的http请求，解析请求参数，</li><li>调用相应的业务逻辑（Service层），处理业务逻辑返回的结果，</li><li>最终将响应数据封装成http响应返回给客户端（如浏览器）</li></ol><h2 id="Bean-Entity"><a href="#Bean-Entity" class="headerlink" title="Bean/Entity"></a>Bean/Entity</h2><p>数据模型，定义了数据库表的结构。每个Entity通常对应数据库的一张表，类里的每个属性对应表里的一列。</p><p>ORM映射：在ORM（Object-Relational Mapping）框架中，如JPA，Entity类用于实现对象和关系数据库表之间的映射。这意味着你可以用面向对象的方式来操作数据库，而不需要编写原生SQL。</p><p>生命周期与管理<br>Persistence Context：在JPA中，实体对象的生命周期由EntityManager管理，分为瞬时态（New）、托管态（Managed）、脱管态（Detached）和移除态（Removed）。</p><p>CRUD操作：通过EntityManager或其衍生接口如 JpaRepository（Spring Data JPA 提供）来进行创建(Create)、读取(Retrieve)、更新(Update)和删除(Delete)操作。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p><a href="https://www.jb51.net/program/325156hnw.htm">springboot框架各个层次基础详解_java_脚本之家 (jb51.net)</a></p><p><a href="https://blog.csdn.net/qq_53070263/article/details/137252721">新建springboot</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/06/28/dl/zhu-yi-li-ji-zhi/"/>
      <url>/2024/06/28/dl/zhu-yi-li-ji-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h1><ol><li>决定需要关注输入的哪部分：在处理大量信息时，注意力机制能够帮助模型或个体确定哪些部分是重要的，并集中资源进行处理。例如，在机器翻译任务中，注意力机制可以帮助模型确定源语言句子中哪些词汇对目标语言的翻译更为重要。</li><li>分配有限的信息处理资源给重要的部分：通过给重要的信息部分分配更多的注意力，模型可以更有效地利用资源，从而提高处理效率和准确性，解决信息过载问题。这类似于人类视觉注意力机制，通过快速扫描全局图像，获得需要重点关注的目标区域，然后对这一区域投入更多注意力资源，以获取更多细节信息，同时抑制其他无用信息。</li></ol><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><ol><li>查询向量Query：指的是查询的范围，自主提示，即主观意识的特征向量</li><li>键向量Key：指的是被比对的项，非自主提示，即物体的突出特征信息向量</li><li>值向量Value：物体本身的特征向量，通常和Key成对出现</li></ol><h1 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h1><p>对于每一个Query，计算所有Key与它的相关性，然后根据这个相关性去找对应的Value（Key、Value成对出现），Query和Key相关性高的其对应Value要重点关注（分配更高的注意力权重）。</p><p>计算Key与Query的相关性<br>$$<br>scores_i = Query \cdot Key_i<br>$$<br>softmax进行归一化，得到权重系数<br>$$<br>\alpha_i = softmax(scores_i) = \frac{e^{scores_i}}{\sum_{j=1}^{Lx} e^{scores_j}}<br>$$<br>对Value进行加权求和，得到Attetion Value<br>$$<br>Attetion = \sum_{i=1}^{Lx} \alpha_i \cdot Value_i<br>$$</p><h2 id="自注意力机制"><a href="#自注意力机制" class="headerlink" title="自注意力机制"></a>自注意力机制</h2><p>提出背景：输入向量大小不一，并且不同向量之间有一定的关系，但是实际训练的时候无法充分发挥这些输入之间的关系，导致模型训练结果效果不好。</p><p>特色：让机器注意到整个输入中不同部分之间的相关性。是一组元素内部相互做注意力机制，因此自注意力机制也叫做内部注意力机制。</p><p>实现过程：Q、K、V是同一个东西，或者三者来源于同一个输入X。</p><ol><li>对于每一个输入向量X，分别乘上系数$W^q、W^k、W^v$（需要学习的参数），得到Q、K、V。</li><li>利用Q和K计算每两个输入向量之间的相关性（scores），一般采用点积计算</li><li>用softmax归一化，得到注意力权重</li><li>用权重乘每个Value，最终得到输出向量Z</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line"># 自注意力机制</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def soft_max(z):</span><br><span class="line">    t = np.exp(z)</span><br><span class="line">    a = np.exp(z) / np.expand_dims(np.sum(t, axis=1), 1)</span><br><span class="line">    return a</span><br><span class="line"></span><br><span class="line"># 每一行为一个序列单元的查询向量</span><br><span class="line">Query = np.array([</span><br><span class="line">    [1,0,2],</span><br><span class="line">    [2,2,2],</span><br><span class="line">    [2,1,3]</span><br><span class="line">]) </span><br><span class="line"></span><br><span class="line"># 每一行为一个查询单元的键向量</span><br><span class="line">Key = np.array([</span><br><span class="line">    [0,1,1],</span><br><span class="line">    [4,4,0],</span><br><span class="line">    [2,3,1]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"># 每一行为一个查询单元的值向量</span><br><span class="line">Value = np.array([</span><br><span class="line">    [1,2,3],</span><br><span class="line">    [2,8,0],</span><br><span class="line">    [2,6,3]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">scores = Query @ Key.T # 计算每一个序列单元的分数，即代表重要程度。结果的每一行为所有序列单元相对于某一个Query的重要程度</span><br><span class="line">print(scores)</span><br><span class="line">scores = soft_max(scores)//对scores做归一化处理，使得一行的和为1</span><br><span class="line">print(scores)</span><br><span class="line">out = scores @ Value //每一行代表每个序列单元新的编码</span><br><span class="line">print(out)</span><br></pre></td></tr></tbody></table></figure><p>缺点：</p><ol><li>自注意力机制过滤了不重要的信息，会导致模型有效信息的抓取能力比CNN弱，因为模型无法利用图像本身具有的尺度、平移不变性以及图像的特征局部性这些先验知识，只能通过大量数据进行学习。所以一般自注意力机制在大数据的基础上才能有效地建立准确的全局关系，而在小数据的情况下可能效果不如CNN。</li><li>没有考虑向量的位置信息</li></ol><h2 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><p>提出背景：使用自注意力机制的模型，对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置，有效信息抓取能力变差。</p><p>特色：用独立学习得到的h组（一般h=8）不同的线性投影（linear projections）来变换Q、K、V。 然后，这h组变换后的Q、K、V将并行地送到注意力汇聚中。 最后，将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。</p><p>实现过程：</p><ol><li>对于一个输入向量X， 定义多组W：$W_0^q、W_0^k、W_0^v…W_7^q、W_7^k、W_7^v$</li><li>每组都按上述自注意力机制实现过程走一遍，得到多个输出$Z_0…Z_7$</li><li>将多个输出拼接后乘以一个矩阵转化为跟输入X相同的维度</li></ol><h2 id="通道注意力机制"><a href="#通道注意力机制" class="headerlink" title="通道注意力机制"></a>通道注意力机制</h2><p>提出背景：之前都是关注图片不同位置的重要性，而图片的另一个维度就是通道，所有也可以计算不同通道的重要性。</p><h2 id="CA-coordinate-attettion-注意力机制"><a href="#CA-coordinate-attettion-注意力机制" class="headerlink" title="CA(coordinate attettion)注意力机制"></a>CA(coordinate attettion)注意力机制</h2><p>提出背景：现有的注意力机制在求取通道注意力的时候，一般采用的全局最大池化/平均池化，这样会造成物体空间信息的损失。</p><p>特色：在引入通道注意力机制的同时，引入空间注意力机制。</p><p>实现过程：将位置信息嵌入到通道注意力中</p><p><img src="/../../images/deepLearning/CA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.PNG" alt="CA注意力机制"></p><p>CA注意力机制分为两个并行阶段。</p><ol><li>首先将输入特征图(CxHxW)分别在宽度、高度两个方向进行全局平均池化，获得在宽度、高度两个方向的特征图(Cx1xH、CxWx1)。</li><li>将两个并行阶段合并，将宽和高转置到同一个维度，然后堆叠，将宽高特征合并得到特征层C x 1 x (H+W)</li><li>利用卷积+标准化+激活函数获得特征</li><li>再次分开为两个并行阶段：Cx1xH、CxWx1</li><li>利用1x1卷积调整通道数后取sigmoid获得宽、高维度上的注意力情况，扩展成CxHxW矩阵，乘上原有的特征图</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>shell挑战性任务</title>
      <link href="/2024/06/28/os/shell-tiao-zhan-xing-ren-wu/"/>
      <url>/2024/06/28/os/shell-tiao-zhan-xing-ren-wu/</url>
      
        <content type="html"><![CDATA[<p>前言：项目代码在<a href="https://github.com/slkfoiw/BUAA-OS-lab">slkfoiw/BUAA-OS-lab (github.com)</a>的master分支，有些地方设计得可能不是很完美，也可能存在错误，欢迎批评指正</p><h2 id="实现不带-b指令"><a href="#实现不带-b指令" class="headerlink" title="实现不带.b指令"></a>实现不带.b指令</h2><p>在spawn函数里，当尝试打开文件（文件名存储在prog中）失败时，在prog字符串后面添加“.b”后再尝试打开一次</p><h2 id="实现指令条件执行"><a href="#实现指令条件执行" class="headerlink" title="实现指令条件执行"></a>实现指令条件执行</h2><p>step1:在gettoken里添加检测条件运算符”&amp;&amp;“，”||“的部分</p><p>step2:在parsecmd中处理指令：跟管道类似，需要调用fork分别处理运算符两边的两条指令，不同的是”&amp;&amp;“，”||“两边的指令不一定都要执行，而且右边的指令需要等左边的指令处理完，得到返回值才能确定是否处理，所以我在parsecmd函数添加了两个参数:</p><ul><li>is_executable:用来记录当前处理的指令能否执行</li><li>return_value:用来记录处理到目前这条指令时，整个式子的返回值，有三种可能，我用宏定义如下：</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">//前面的指令还无法确定返回值，如cmd1 &amp;&amp; cmd2 || cmd3 的cmd1执行成功，当前在处理cmd2</span><br><span class="line">#define UNCERTAIN -1 </span><br><span class="line">//前面的指令已经能确定返回值</span><br><span class="line">#define CERTAIN0 0</span><br><span class="line">#define CERTAIN1 1</span><br></pre></td></tr></tbody></table></figure><p>当is_executable==1时，return_value一定是UNCERTAIN；当is_executable==0时，return_value一定是CERTAIN0或CERTAIN1</p><p>fork出的child处理左边的指令</p><ul><li>若能执行（is_executable==1），执行完后用ipc发送返回值给parent进程</li><li>若不能执行（is_executable==0），说明返回值已确定，当前指令不需要执行，直接通过ipc发送return_value给parent进程</li></ul><p>parent进程处理右边的指令</p><ul><li>接收child发来的返回值，并等待child进程结束</li><li>根据&amp;&amp;、||的不同设置 is_executable, return_value的值，然后继续调用parsecmd解析指令</li></ul><h2 id="实现更多指令"><a href="#实现更多指令" class="headerlink" title="实现更多指令"></a>实现更多指令</h2><p>首先建三个文件touch.c、mkdir.c、rm.c，然后修改include.mk里，使它们能编译成可执行文件</p><p>touch：创建很简单，open一下，权限设置为O_CREAT就行。主要复杂在错误处理。判断文件所在目录是否存在，解析文件路径，找到最后一个”/“，”/“之前的部分就是目录，用open判断目录是否存在</p><p>mkdir：</p><p>修改serve_open函数，加入对O_MKDIR权限的处理</p><p>修改file_create函数，添加一个type参数，用来给文件的f_type字段赋值</p><p>rm：核心就是调用remove。不过需要判断是否为目录，这里要用到stat函数</p><h2 id="实现反引号"><a href="#实现反引号" class="headerlink" title="实现反引号"></a>实现反引号</h2><p>gettoken里添加检测”`”的部分</p><p>parsecmd里添加处理“`”的部分，需要设置一个全局标记flag来判断是左反引号还是右反引号</p><h2 id="实现注释功能"><a href="#实现注释功能" class="headerlink" title="实现注释功能"></a>实现注释功能</h2><p>gettoken里添加检测”#”的部分</p><p>parsecmd里添加处理“#”的部分，后面的不要，直接开始处理指令就行了</p><h2 id="实现历史指令"><a href="#实现历史指令" class="headerlink" title="实现历史指令"></a>实现历史指令</h2><p>最耗时的指令之一，首先需要注意history是内置指令，也就是说不要新建一个history.c文件，然后调用spawn去fork出一个新进程处理。实现过程中需要注意：避免不同进程修改 跟history相关的数据 导致数据不一致。</p><p>跟history指令有关的函数如下（都在sh.c中）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">void history_init();//在shell进程开始时初始化跟history相关的数据，如打开.mosh_history文件</span><br><span class="line">void save_history();//将每次输入的指令保存到.mosh_history文件</span><br><span class="line">void add_one_history(const char *command);//向历史记录里加入一条指令</span><br><span class="line">int get_last_history(char *oldbuf, char *newbuf);//获得上一条指令</span><br><span class="line">int get_next_history(char *newbuf);//获得下一条执行</span><br><span class="line">void history();//执行history指令</span><br></pre></td></tr></tbody></table></figure><p>注意凡是要修改数据的函数都需要在main所在进程里调用</p><h2 id="实现一行多指令"><a href="#实现一行多指令" class="headerlink" title="实现一行多指令"></a>实现一行多指令</h2><p>跟管道指令类似，fork一下就行啦，不用创建管道了。</p><h2 id="实现追加重定向"><a href="#实现追加重定向" class="headerlink" title="实现追加重定向"></a>实现追加重定向</h2><p>parsecmd里在读到一个“&gt;”后加一个判断，后面一个token是否也为”&gt;”</p><p>修改open函数实现追加功能，主要是要修改offset。</p><h2 id="实现引号支持"><a href="#实现引号支持" class="headerlink" title="实现引号支持"></a>实现引号支持</h2><p>修改gettoken函数，读到左边引号时一直读到下去，直到读到右边引号才能结束。我还加了一些异常处理，防止输入错误</p><h2 id="实现前后台任务管理"><a href="#实现前后台任务管理" class="headerlink" title="实现前后台任务管理"></a>实现前后台任务管理</h2><p>另一个实现起来超级耗时的指令。同样，因为都是内置指令，不需要新建文件、新建进程。但是我还是在runcmd进程里处理的（因为需要解析指令参数），然后通过exit传回值，在main进程里修改与jobs相关的数据。</p><p>主要函数如下（都在sh.c中）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">void add_one_job(int env_id, char *cmd);//添加一条后台指令</span><br><span class="line">void update_jobs_status();//更新后台指令的状态</span><br><span class="line">void update_jobs();//由于挑战性任务跟实际Linux运行不太一样，不需要删除已经结束的指令记录，所以这个函数没用</span><br><span class="line">void print_all_jobs();//打印所有后台指令</span><br><span class="line">int fg_job(int job_id);//执行fg指令</span><br><span class="line">int kill_job(int job_id);//执行kill指令</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OO第三单元总结</title>
      <link href="/2024/06/11/oo/oo-unit4-summary/"/>
      <url>/2024/06/11/oo/oo-unit4-summary/</url>
      
        <content type="html"><![CDATA[<h1 id="正向建模与开发"><a href="#正向建模与开发" class="headerlink" title="正向建模与开发"></a>正向建模与开发</h1><p>根据指导书给出的需求：</p><ol><li>首先肯定需要有一个图书馆，图书馆承担着处理请求的任务，并需要记录所有图书、用户的信息。</li><li>图书馆里要有书架、借还处、预定处、用户以及之后作业引入的漂流角这些对象<ul><li>书籍就在这些对象之间流动，所以这些对象里需要一个装着书籍信息的容器。</li><li>需要接收图书、移除图书的动作。参考实验课的提示，我让书架、借还处、预定处、漂流角都通过与图书馆通信来完成，而不直接互相通信，这样每个地方都抽象出来接收图书、移除图书的两个动作，可扩展性强。</li></ul></li><li>由于预约处要保留预约信息，所以我额外设计了一个类，该类的每个对象存储一条预约信息</li></ol><h1 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h1><h2 id="第一次作业"><a href="#第一次作业" class="headerlink" title="第一次作业"></a>第一次作业</h2><ul><li>图书馆类下管理着一个书架、借还处、预定处以及所有书本、用户，对于每种请求都有对应的一个方法去处理</li><li>书架、借还处、预定处、用户类中有一个管理图书的容器，并有相应的增删图书的方法。第一次作业我用的HashMap存储图书及其数量，也就是说每种图书的所有副本共用一个对象，value来记录副本数量，这样扩展性较差，后续作业有所修改</li><li>不同位置不直接通信，而是通过图书馆来移动图书</li><li>图书对象的一致性问题：由于每次读入的请求中的图书都是一个新的对象，但在书架、借还处、预定处、用户间流动的图书应该是图书馆里管理的图书，所以我用了一个checkBookId的方法，返回图书馆里的图书对象的引用。</li></ul><p><img src="/../../images/OO/unit4-1.jpg" alt="unit4-1"></p><h2 id="第二次作业"><a href="#第二次作业" class="headerlink" title="第二次作业"></a>第二次作业</h2><p>根据新增加的需求：</p><ol><li>增加了漂流角类，属性和方法与书架、借还处、预定处这些类似</li><li>图书馆里增加新请求类型的处理方法和整理图书的方法</li><li>新增加了图书借阅期限，所以官方包里的LibraryBookId无法满足需求，于是新建了一个继承自LibraryBookId的Book类，记录每本书的借还时间</li><li>图书管理容器的修改：由于第二次作业，每个图书副本可能会具有不同的属性，例如借阅次数、借阅日期、归还日期，所以原本的HashMap容器不能满足副本之间的差异管理，于是改用ArrayList，每个副本都对应一个图书对象。</li></ol><p><img src="/../../images/OO/unit4-2.jpg" alt="unit4-2"></p><h2 id="第三次作业"><a href="#第三次作业" class="headerlink" title="第三次作业"></a>第三次作业</h2><p>第三次作业的需求较为简单，几乎不需要怎么动架构，只需要在用户类（即Student）里增加积分属性和相应的调整动作即可，然后根据指导书添加处理方法，在恰当的时候调整用户的积分即可，不过还是有一些细节需要注意：</p><ol><li>积分有上限，这意味着不同的增、减顺序会导致积分结果不一样，最开始我忽略了这一点，不过改正也很简单，只要保证先减后增就行了</li><li>积分为负才不能借阅、预约，而不是非正，也是刚开始没注意的点</li></ol><p><img src="/../../images/OO/unit4-3.jpg" alt="unit4-3"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="架构设计思维的演进"><a href="#架构设计思维的演进" class="headerlink" title="架构设计思维的演进"></a>架构设计思维的演进</h2><p>第一单元表达式的架构设计是让我对面向过程和面向对象的两者区别有最深体会的时候，表达式处理所涉及到的递归下降和c语言里的递归函数很像，实验课给了一个很好的架构参考，让我对“递归”在面向对象里的应用有了具象化的认识。</p><p>第二单元电梯的设计学习到了多线程的相关知识，与同步进行的OS很多知识相呼应，让我对线程间通信、生产者-消费者这样的问题有了更深的印象，也帮助了我第四单元的架构设计。</p><p>第三单元的JML和第四单元的UML主要是对设计方面的训练，JML让我了解了规格化设计的过程，它很好的避免了二义性，并且给测试提供了思路。UML虽然作为一个单独的单元，但其实一直贯穿整个课程，每次作业之前都会画个粗略的类图帮助我理清思路，不过之前不太清楚每种元素的作用，连线比较随意，这个单元就更规范化了，并且还学习了状态图、顺序图，相信这些知识都会在未来帮助我更好地规范设计。</p><p>总的来说，随着课程的深入，我对面向对象有了更清晰的认知，在了解了SOLID原则后，每次我的架构设计都尽可能往这上面靠，虽然可能并不完美，但是比学期初的我一定是有很大不同的。</p><h2 id="测试思维的演进"><a href="#测试思维的演进" class="headerlink" title="测试思维的演进"></a>测试思维的演进</h2><p>我的测试思维演进应该说是一个从手动构造数据，到自动化评测，再到发现两者缺一不可的过程。首先，这学期我学会了自动化测试，在上学期的OOpre课程我只会手动构造数据，但这学期的作业仅靠手动构造数据是肯定不够的。</p><p>第一单元我实现了一个比较完整的评测机，数据生成器部分比较难写，正确性的检验倒是挺好实现的。而到第二单元的时候就是数据生成器部分容易实现，而正确性检验比较难了，由于本人能力有限再加上时间不够，所以基本都是靠大佬的评测机活过来的（超级感谢）。第三单元又学会了Junit单元测试，上学期的OOpre课程已经有涉及，但当时我不能理解其妙义，这学期发现它和JML结合一起倒是非常好用。第四单元相对重视架构设计，所以在测试方面没有其他的变化。</p><p>在这门课程中，我测试思维上最大的收获就是一定要格外重视高并发的情况，也是这个点让我觉得手动构造数据和自动化随机生成数据二者缺一不可。在第二单元，我发现仅靠评测机的随机数据是不行的，必须考虑一些高并发的极端情况，尤其是到了第三单元，这种现象也很明显，这让我意识到手动构造一些极端情况的数据也是十分有必要的。这两个单元都让我对高并发有了深刻的认识，想起之前老师在课上提到的12306，此刻对这个例子有了具象化的认知，以前的我只会觉得12306挺难用的，但现在从设计者的角度再看，不禁感叹12306要承担起节假日、春运时海量的访问还不崩，真的太不容易了。所以高并发在实际项目、工程中也是关键点，在以后的设计中，我也会把这点牢记在心，把极端情况考虑在内。</p><h2 id="课程收获"><a href="#课程收获" class="headerlink" title="课程收获"></a>课程收获</h2><ol><li>锻炼了我写代码、调试的能力。多线程单元debug的时候虽然痛苦，但收获颇丰。</li><li>掌握了Java这门面向对象的语言。不仅学习了多线程、递归下降等知识，还学习到了很多设计方法、设计原则。</li><li>每个单元迭代式的开发，让我学会了先设计再实现的开发方式。好的架构设计确实能提高效率，在实际应用中也能提高项目扩展性，方便增加需求。</li><li>学会了自动化测试、单元测试等测试方法，对高并发的处理有了深刻认识。</li></ol>]]></content>
      
      
      <categories>
          
          <category> OO </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>文件系统</title>
      <link href="/2024/05/27/os/os-file-system/"/>
      <url>/2024/05/27/os/os-file-system/</url>
      
        <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>为什么不定义为指针，是因为这是要写入文件的数据，如果你在程序运行的时候构建了这个链表，计算出了当前next的地址（实际上是主存的地址）然后写入辅存，可能下次把这些数据调入主存的时候就不在这个地址了。</p><p>文件管理的要求</p><p>用户视角：使用<strong>逻辑文件</strong>，即内容是什么</p><p>操作系统视角：组织和管理<strong>物理文件</strong>，即怎么存</p><p>按逻辑结构分：有结构文件（由记录组成，分为定长记录、可变长记录）、无结构文件（由二进制流或字符流组成，无明显的逻辑结构）</p><p>按文件中物理结构分：顺序文件、链接文件、索引文件</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>也是文件，是由文件说明索引组成的用于文件检索的特殊文件，文件目录的内容是文件访问和控制的信息（不包括文件内容）。</p><h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><ol><li><p>基本信息：文件名</p></li><li><p>文件类型：</p><ul><li><p>有/无结构（记录文件，流式文件）;</p></li><li><p>内容（二进制，文本）</p></li><li><p>用途（源代码，目标代码，可执行文件，数据）</p></li><li><p>属性attribute（如系统，隐含等）</p></li><li><p>文件组织（如顺序，索引等）</p></li></ul></li><li><p>地址信息：存放位置、文件长度</p></li><li><p>访问控制信息：文件所有者、访问权限</p></li><li><p>使用信息：创建时间、最后一次读/写访问的时间和用户</p></li></ol><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>单级目录</p><p>两级目录</p><p>多级目录</p><h1 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h1><h2 id="文件控制块"><a href="#文件控制块" class="headerlink" title="文件控制块"></a>文件控制块</h2><p><strong>基本信息</strong></p><ul><li>文件名：字符串，通常在不同系统中允许不同的最大长度，可修改</li><li>物理位置；</li><li>文件逻辑结构：有/无结构（记录文件，流式文件）</li><li>文件物理结构：（如顺序，索引等）</li></ul><p><strong>访问控制信息</strong></p><ul><li>文件所有者（属主）：通常是创建文件的用户，或者改变已有文件的属主；</li><li>访问权限（控制各用户可使用的访问方式）：如读、写、执行、删除等；</li></ul><p><strong>使用信息</strong></p><p>创建时间，上一次修改时间，当前使用信息等。</p><h2 id="文件物理结构"><a href="#文件物理结构" class="headerlink" title="文件物理结构"></a>文件物理结构</h2><h3 id="顺序-连续结构"><a href="#顺序-连续结构" class="headerlink" title="顺序/连续结构"></a>顺序/连续结构</h3><p>容易出现磁盘碎片，适用于变化不大的文件</p><h3 id="串联-链接文件"><a href="#串联-链接文件" class="headerlink" title="串联/链接文件"></a>串联/链接文件</h3><p>随机存取效率太低，如果访问文件的最后的内容，实际上是要访问整个文件。</p><p>可靠性问题，如指针出错;</p><p>链接指针占用一定的空间</p><h3 id="索引结构"><a href="#索引结构" class="headerlink" title="索引结构"></a>索引结构</h3><p>一个文件的信息存放在若干个不连续物理块中。系统为每个文件建立一个专用数据结构：索引表，并将这些物理块的块号存放在该索引中。索引表就是磁盘块地址数组，其中第i个条目指向文件的第i块</p><p>索引表可放在文件目录中、文件的开头等</p><p>索引文件在存储区中占两个区：索引区和数据区。索引区存放索引表，数据区存放数据文件本身。</p><p>访问索引文件需要两步操作：</p><ol><li>读取文件索引区，由逻辑块号查得物理块号</li><li>访问物理块号而获得所需信息</li></ol><p><strong>优点：</strong></p><ul><li>保持了链接结构的优点，又避免了其缺点</li><li>即能顺序存取，又能随机存取</li><li>满足了文件动态增长、插入删除的要求</li><li>能充分利用外存空间</li></ul><p><strong>缺点：</strong></p><p>索引表本身带来了系统开销，如：内外存空间，存取时间</p><h4 id="索引表的组织"><a href="#索引表的组织" class="headerlink" title="索引表的组织"></a>索引表的组织</h4><ul><li>链接模式：一个盘块一个索引表，多个索引表链接起来</li><li>多级索引（间接索引）：将一个大文件的所有索引表（二级索引)的地址放在另一个索引表（一级索引)中</li><li>综合模式：直接索引方式与间接索引方式结合</li></ul><h2 id="目录的实现"><a href="#目录的实现" class="headerlink" title="目录的实现"></a>目录的实现</h2><h3 id="目录项的内容"><a href="#目录项的内容" class="headerlink" title="目录项的内容"></a>目录项的内容</h3><ul><li>直接法：目录项＝文件名＋文件控制块（属性信息、在外存上的存放位置）。如MS-DOS/Windows；</li><li>间接法：目录项＝文件名＋文件控制块的地址（索引号）。如Unix（inode）</li></ul><p>不管是何种方法，给定一个文件名，即可返回相应的文件信息</p><h3 id="长文件名问题"><a href="#长文件名问题" class="headerlink" title="长文件名问题"></a>长文件名问题</h3><p>方法1：在目录项中，将文件名的长度固定为255个字符。缺点：浪费空间，很少文件用很长的名字；</p><p>方法2：每个目录项的长度可变，分为三部分：目录项长度、文件的属性信息（此两项长度固定）、文件名（长度可变）。缺点：文件被删除后，该目录项所占用的空间不太好回收利用；</p><p>方法3：目录项本身的长度固定，把长度可变的文件名统一放在目录文件的末尾。</p><h3 id="目录的搜索方法"><a href="#目录的搜索方法" class="headerlink" title="目录的搜索方法"></a>目录的搜索方法</h3><p>顺序查寻法</p><p>Hash方法/散列法</p><h3 id="便于共享的目录组织"><a href="#便于共享的目录组织" class="headerlink" title="便于共享的目录组织"></a>便于共享的目录组织</h3><p>硬链接：多个文件名指向同一inode，一个文件拥有多个有效路径名</p><p>软连接/符号链接：一个文件实际上是一个文本文件，包含了另一个文件的位置信息（路径名）</p><h2 id="保护文件的方法"><a href="#保护文件的方法" class="headerlink" title="保护文件的方法"></a>保护文件的方法</h2><p>建立副本</p><p>定时转储（unix采用）：每隔一定时间把文件转储到其他存储介质上，当文件发生故障，就用转储的文件来复原，把有故障的文件恢复到转储时刻文件的状态</p><p>规定文件的权限</p><h2 id="文件系统的性能问题"><a href="#文件系统的性能问题" class="headerlink" title="文件系统的性能问题"></a>文件系统的性能问题</h2><p>块高速缓存</p>]]></content>
      
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>磁盘管理</title>
      <link href="/2024/05/20/os/os-disk-management/"/>
      <url>/2024/05/20/os/os-disk-management/</url>
      
        <content type="html"><![CDATA[<h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>扇区sector：对于磁盘，每个磁道的扇区数并不是常量</p><p>磁道track：从外往里数，也就是说最外面为0磁道。外道数据访问速度更快（相同角速度下，半径越大线速度越大）</p><p>柱面cylinder：不同盘片相同半径的磁道所组成的圆柱</p><p>磁头head：每个磁盘有两个面，每个面都有一个磁头</p><h2 id="Flash-Disk"><a href="#Flash-Disk" class="headerlink" title="Flash Disk"></a>Flash Disk</h2><p>优点：低功耗、大容量、数据访问速度快</p><p>缺点：容量、价格、寿命、可靠性、读写不对称（读快、写慢）</p><p>两种技术：NADND、NOR</p><h1 id="磁盘的组织与调度"><a href="#磁盘的组织与调度" class="headerlink" title="磁盘的组织与调度"></a>磁盘的组织与调度</h1><p>主引导记录MBR</p><p>硬盘分区表DPT：64字节。分为四小部分，每个部分表示一个分区的信息，占16字节</p><h2 id="CHS模式"><a href="#CHS模式" class="headerlink" title="CHS模式"></a>CHS模式</h2><p>柱面数NC：最大1024（10位）</p><p>磁头数NH：表示硬盘总共有几个磁头，也就是几个盘片，最多256个（8位）</p><p>扇区数NS：表示每一条磁道上有几个扇区，由于所有的0扇区用于存放固件以及一些硬盘的专用的文件，最大为<strong>63</strong> (用 6 位存储)。用户可见扇区从1开始</p><p>8.46GB问题:<br>$$<br>1024 \times 256 \times 63 = 16,515,072 \<br>\text{这个扇区之前的所有物理扇区所包含的字节数} =  16,515,072 \times 512B = 8.46GB<br>$$</p><h2 id="LBA模式"><a href="#LBA模式" class="headerlink" title="LBA模式"></a>LBA模式</h2><p>Logic Block Address，将磁盘驱动器可以看做一个一维的逻辑块的数组，逻辑块是最小的传输单位</p><p>CHS与LBA地址转换<br>$$<br>LBA = (NH×NS×C) + (NS×H) + (S-1)\<br>\<br>C = LBA / (NS×NH)\<br>H = (LBA / NS) mod NH\<br>S = (LBA mod NS) + 1<br>$$</p><h1 id="磁盘空间的管理"><a href="#磁盘空间的管理" class="headerlink" title="磁盘空间的管理"></a>磁盘空间的管理</h1><p>位图：空闲为1，已分配为0</p><p>空闲表法：将所有空闲块记录在一个表中。主要记录两项内容：起始块号，块数</p><p>空闲链表法：把所有空闲块链成一个表，链会很长</p><p>成组链接法：把空白物理块分成组，再通过指针把组与组之间链接起来</p><h1 id="磁盘访问时间"><a href="#磁盘访问时间" class="headerlink" title="磁盘访问时间"></a>磁盘访问时间</h1><p>寻道时间：指的是把磁臂（磁头）从当前位置移动到指定磁道上所经历的时间。<strong>Ts</strong>=启动磁盘的时间s + 磁头移动n条磁道所花费的时间之和（m x n, m为常数）</p><p>旋转延迟时间：平均旋转延迟时间<strong>Tr</strong>=转一圈所需时间/2=1/（2*转速r）</p><p>传输时间：把数据从磁盘读出，或向磁盘写入数据所经历的时间，<strong>Tt</strong> = 每次所读/写的字节数b/（转速r*磁道上的字节数）</p><p>总访问时间Ta = Ts + Tr + Tt</p><h1 id="磁盘调度算法"><a href="#磁盘调度算法" class="headerlink" title="磁盘调度算法"></a>磁盘调度算法</h1><ol><li>先来先服务算法（FCFS）</li><li>最短寻道时间优先算法（SSTF，Shortest Seek Time First）：优先选择距当前磁头最近的访问请求进行服务，主要考虑寻道优先。可能出现饥饿现象</li><li>扫描算法（SCAN）</li><li>循环扫描算法（C-SCAN）：移动臂到达最后一个柱面后，立即带动读写磁头快速返回到0号柱面，<strong>返回时不为任何的等待访问者服务</strong>，返回后可再次进行扫描。</li><li>LOOK</li><li>C-LOOK</li></ol><h1 id="提高磁盘I-O性能"><a href="#提高磁盘I-O性能" class="headerlink" title="提高磁盘I/O性能"></a>提高磁盘I/O性能</h1><ul><li><p>缓存</p></li><li><p>优化数据布局</p></li><li><p>提前读：顺序访问时，常采用提前读入下一块到缓冲区中</p></li><li><p>延迟写：将本应立即写回磁盘的数据挂到空闲缓冲区的队列的末尾。直到该数据块移到链头时才将其写回磁盘，再作为空闲区分配出去</p></li><li><p>虚拟盘：利用内存空间去仿真磁盘（RAM盘）</p><p>Vitual disk 与disk cache的区别是：</p><ul><li><p>Vitual disk的存放的内容由用户完全控制</p></li><li><p>Disk cache中的内容完全是由操作系统控制</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>IO管理</title>
      <link href="/2024/05/17/os/os-io-management/"/>
      <url>/2024/05/17/os/os-io-management/</url>
      
        <content type="html"><![CDATA[<h1 id="I-O管理概述"><a href="#I-O管理概述" class="headerlink" title="I/O管理概述"></a>I/O管理概述</h1><h2 id="I-O设备分类"><a href="#I-O设备分类" class="headerlink" title="I/O设备分类"></a>I/O设备分类</h2><p>按 数据组织/信息交换的单位 分类：</p><ul><li>块设备：传输快，可寻址</li><li>字符设备：传输慢，不可寻址，常采用中断驱动方式</li></ul><p>按资源分配：</p><ul><li>独占式设备：只允许各个进程串行使用。如打印机</li><li>共享设备：允许多个进程同时使用（微观上交替使用）。如硬盘</li><li>虚拟设备：在一类设备上模拟另一类设备，常用的方法是，用共享设备模拟独占设备，用高速设备模拟低速设备。如：用Spooling技术将打印机变成共享设备。</li></ul><p>按用途分类：</p><ul><li>存储设备：磁盘、磁带；</li><li>传输设备：网卡，Modem；</li><li>人机交互设备：显示器、键盘、鼠标。</li></ul><h2 id="I-O管理示意"><a href="#I-O管理示意" class="headerlink" title="I/O管理示意"></a>I/O管理示意</h2><ul><li>逻辑I/O :完成设备无关的操作，如设备分配，设备回收，数据准备等；</li><li>设备驱动程序：负责对设备控制器进行控制（通过读写其中的寄存器）。</li><li>中断服务程序：设备工作结束后负责向 CPU 发中断信号,中断服务程序完成相应处理。</li></ul><h1 id="I-O硬件组成"><a href="#I-O硬件组成" class="headerlink" title="I/O硬件组成"></a>I/O硬件组成</h1><h2 id="设备控制器"><a href="#设备控制器" class="headerlink" title="设备控制器"></a>设备控制器</h2><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><ul><li>接受和识别CPU命令：用控制寄存器</li><li>数据交换：在CPU与控制器、控制器与设备之间，用数据寄存器</li><li>设备状态的了解和报告：用状态寄存器</li><li>设备地址识别：识别CPU要读写哪个寄存器，要实现I/O地址（见下“I/O端口地址”）</li><li>缓冲区</li><li>对设备传来的数据进行差错检测</li></ul><h3 id="组成"><a href="#组成" class="headerlink" title="组成"></a>组成</h3><p>控制器与CPU接口：数据寄存器、控制寄存器、状态寄存器，采用内存映射或专门的I/O指令</p><p>控制器与设备接口：数据信号、控制信号、状态信号</p><p>I/O逻辑：用于实现CPU对I/O设备的控制</p><h3 id="I-O端口地址"><a href="#I-O端口地址" class="headerlink" title="I/O端口地址"></a>I/O端口地址</h3><p>定义：接口电路中每个寄存器具有唯一的地址</p><p>所有I/O端口地址形成I/O端口的地址空间，I/O指令形式与I/O地址相互关联，主要有以下形式：</p><ol><li><p>内存映像地址–&gt;内存映像I/O模式：控制器的内存/寄存器作为物理内存空间的一部分</p><p>优点：</p><ul><li>不需要特殊的保护机制来阻止用户进程进行相应的I/O 操作。操作系统要避免把包含了控制寄存器的那部分地址空间放入用户的虚拟地址空间中</li><li>引用内存的每一条指令都适用于引用控制寄存器</li></ul><p>缺点：不允许对一个控制寄存器的内容进行高速缓存（如果我们把设备控制寄存器进行了高速缓存，那么第一次引用的时候就把它 放入了高速缓存。以后再对它的引用都是从高速缓存当中取值，而不会再去对设备进行相应的检测）</p></li><li><p>I/O独立编址–&gt;I/O专用指令：Intel体系架构in/out指令</p><p>优点：</p><ul><li>外设不占用内存的地址空间</li><li>编程时易于区分是对内存操作还是对I/O操作</li></ul><p>缺点：I/O端口操作的指令类型少，操作不灵活</p></li></ol><h2 id="I-O控制方式！"><a href="#I-O控制方式！" class="headerlink" title="I/O控制方式！"></a>I/O控制方式！</h2><h3 id="程序控制I-O-PIO-Programmed-I-O"><a href="#程序控制I-O-PIO-Programmed-I-O" class="headerlink" title="程序控制I/O(PIO,Programmed I/O)"></a>程序控制I/O(PIO,Programmed I/O)</h3><p>CPU<strong>轮询</strong>检查状态寄存器</p><h3 id="中断驱动方式-Interrupt-driven-I-O"><a href="#中断驱动方式-Interrupt-driven-I-O" class="headerlink" title="中断驱动方式(Interrupt-driven I/O)"></a>中断驱动方式(Interrupt-driven I/O)</h3><ol><li>CPU发出读/写命令后，将等待I/O的进程阻塞，切换到其他进程。</li><li>当I/O完成后，设备控制器向CPU发出中断信号。</li><li>CPU在每个指令周期的<strong>末尾</strong>检查中断，若检测到中断信号，则保存当前进程的运行环境信息，转去执行中断处理程序处理该中断。</li><li>处理中断的过程中，CPU从控制器读<strong>一个字</strong>的数据传送到CPU寄存器，再写入主存</li><li>CPU恢复等待I/O的进程的运行环境，继续执行</li></ol><h3 id="直接存储访问方式-DMA-Direct-Memory-Access"><a href="#直接存储访问方式-DMA-Direct-Memory-Access" class="headerlink" title="直接存储访问方式(DMA, Direct Memory Access)"></a>直接存储访问方式(DMA, Direct Memory Access)</h3><p>主要用于<strong>块设备</strong>的I/O控制，不再一个字一个字传送，但是块必须是<strong>连续的</strong>。CPU仅在传送开始和结束时进行干预，I/O设备与内存可以直接交互，不需要经过CPU</p><p>过程：</p><ol><li><p>由程序设置DMA控制器中的若干寄存器值（如内存始址，传送字节数），然后发起I/O操作；</p></li><li><p>DMA控制器完成内存与外设的成批数据交换；</p></li><li><p>在操作完成时由DMA控制器向CPU发出中断</p></li></ol><p>DMA控制器和I/O控制器类似，有如下寄存器：</p><ul><li>命令/状态寄存器（CR）：用于接收从CPU发送来的I/O命令，或有关控制信息，或设备的状态。</li><li>内存地址寄存器（MAR）：在输入时，它存放把数据从设备传送到内存的起始目标地址，在输出时，它存放由内存到设备的内存源地址。</li><li>数据寄存器（DR）：用于暂存从设备到内存，或从内存到设备的数据。</li><li>数据计数器（DC）：存放本次CPU要读或写的字（节）数。</li></ul><p>优点：</p><p>CPU只需干预I/O操作的开始和结束，而后续成批的数据读写则无需CPU控制，适于高速设备。</p><p>缺点：</p><ul><li>数据传送的方向、存放数据的内存地址及传送数据的长度等都由CPU控制，占用了CPU时间。</li><li>每个设备占用一个DMA控制器，当设备增加时，需要增加新的DMA控制器</li></ul><table><thead><tr><th>区别</th><th>中断驱动</th><th>DMA</th></tr></thead><tbody><tr><td>何时中断？</td><td>每个单位数据传送完成后中断CPU</td><td>传送的一批数据完成后中断</td></tr><tr><td>谁控制数据传送？</td><td>CPU控制完成数据传送，涉及程序切换，需要保护和恢复现场</td><td>由DMA控制器控制完成的，在传输过程中不需要CPU干预，DMA控制器直接在主存和I/O设备之间传送数据，只有开始和结束才需要CPU干预</td></tr><tr><td></td><td>具有对异常事件的处理能力</td><td>适用于数据块的传输</td></tr></tbody></table><h3 id="通道技术（Channel）"><a href="#通道技术（Channel）" class="headerlink" title="通道技术（Channel）"></a>通道技术（Channel）</h3><p>I/O通道是专门负责输入输出的处理器，独立于CPU。</p><p>与DMA的原理几乎是一样的，通道是一个特殊功能的处理器，它有自己的指令和程序专门负责数据输入输出的传输控制。CPU将“传输控制”的功能下放给通道后只负责“数据处理”功能。这样，通道与CPU分时使用内存，实现了CPU内部运算与I/O设备的并行工作</p><table><thead><tr><th>区别</th><th>DMA</th><th>通道</th></tr></thead><tbody><tr><td></td><td>数据的传送方向、存放数据的内存起始地址和数据块长度都由CPU控制</td><td>是一个特殊的处理器，有自己的指令和程序，通过执行通道程序实现对数据传输的控制，所以通道具有更强的独立处理I/O的功能</td></tr><tr><td></td><td>通常只能控制一台或者少数几台同类设备</td><td>一个通道可同时控制多种设备</td></tr></tbody></table><h2 id="I-O软件组成"><a href="#I-O软件组成" class="headerlink" title="I/O软件组成"></a>I/O软件组成</h2><p><img src="/../../images/OS/IO%E8%BD%AF%E4%BB%B6%E7%BB%84%E6%88%90" alt="I/O软件组成"></p><h3 id="逻辑设备表LUT-Logical-Unit-Table"><a href="#逻辑设备表LUT-Logical-Unit-Table" class="headerlink" title="逻辑设备表LUT(Logical Unit Table)"></a>逻辑设备表LUT(Logical Unit Table)</h3><p>为了实现设备的独立性，系统必须设置一张逻辑设备表，用于将应用程序中所使用的逻辑设备名映射为物理设备名。</p><p>该表的每个表目中包含了三项，逻辑设备名（设备类型）、物理设备名、设备驱动程序的入口地址</p><h1 id="I-O缓冲管理"><a href="#I-O缓冲管理" class="headerlink" title="I/O缓冲管理"></a>I/O缓冲管理</h1><h1 id="I-O设备管理"><a href="#I-O设备管理" class="headerlink" title="I/O设备管理"></a>I/O设备管理</h1><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>通道可以有多个，一个通道管理多个控制器，一个控制器管理多个设备</p><h3 id="设备控制表DCT"><a href="#设备控制表DCT" class="headerlink" title="设备控制表DCT"></a>设备控制表DCT</h3><p>每个设备一张DCT，用于记录设备情况</p><ul><li>设备队列队首指针：凡因为请求本设备而未得到满足的进程，其PCB都应按照一定的策略排成一个队，称该队列为设备请求队列或简称设备队列，其队首指针指向队首PCB；</li><li>设备状态：当设备处于使用状态时，应该把设备忙/闲标志置为1；</li><li>控制器表指针：该指针指向该设备所连接的控制器的控制表；</li><li>重复执行次数：外部设备在传送数据时，较容易发生数据传送错误。在许多系统中，如果发生传送错误，并不立即认为传送失败，而是令它重传，并由系统规定设备在工作中发生错误时应重复执行的次数</li></ul><h3 id="控制器控制表CCT"><a href="#控制器控制表CCT" class="headerlink" title="控制器控制表CCT"></a>控制器控制表CCT</h3><h3 id="通道控制表CHCT"><a href="#通道控制表CHCT" class="headerlink" title="通道控制表CHCT"></a>通道控制表CHCT</h3><h3 id="系统设备表SDT"><a href="#系统设备表SDT" class="headerlink" title="系统设备表SDT"></a>系统设备表SDT</h3><p>记录了系统中全部设备的情况，每个设备对应一个表目</p><h2 id="假脱机技术（SPOOLing技术）"><a href="#假脱机技术（SPOOLing技术）" class="headerlink" title="假脱机技术（SPOOLing技术）"></a>假脱机技术（SPOOLing技术）</h2><p>用户空间的I/O软件</p><p>也称为虚拟设备技术，可把独享设备转变成具有共享特征的虚拟设备，从而提高设备利用率</p><h3 id="SPOOLing程序和外设进行数据交换：实际I-O"><a href="#SPOOLing程序和外设进行数据交换：实际I-O" class="headerlink" title="SPOOLing程序和外设进行数据交换：实际I/O"></a>SPOOLing程序和外设进行数据交换：实际I/O</h3><ul><li>SPOOLing程序预先从外设读取数据并加以缓冲，在以后需要的时候输入到应用程序</li><li>SPOOLing程序接受应用程序的输出数据并加以缓冲，在以后适当的时候输出到外设</li></ul><h3 id="应用程序和SPOOLing程序交换数据：虚拟I-O"><a href="#应用程序和SPOOLing程序交换数据：虚拟I-O" class="headerlink" title="应用程序和SPOOLing程序交换数据：虚拟I/O"></a>应用程序和SPOOLing程序交换数据：虚拟I/O</h3><p>应用程序进行I/O操作时，实际上是从SPOOLing程序的缓冲池中读出数据或把数据送入缓冲池，而不是跟实际的外设进行I/O操作</p><h1 id="I-O性能问题"><a href="#I-O性能问题" class="headerlink" title="I/O性能问题"></a>I/O性能问题</h1><h2 id="两个途径"><a href="#两个途径" class="headerlink" title="两个途径"></a>两个途径</h2><ul><li>使CPU利用率尽可能不被I/O降低：可以使用缓冲技术减少或缓解速度差异，同时使用异步I/O使CPU不等待 I/O</li><li>使CPU尽可能摆脱I/O：使用DMA、通道等I/O部件让CPU摆脱I/O操作的影响</li></ul><h2 id="I-O操作的两个步骤"><a href="#I-O操作的两个步骤" class="headerlink" title="I/O操作的两个步骤"></a>I/O操作的两个步骤</h2><ol><li>把磁盘数据装载进内核的内存空间</li><li>把内核内存空间的数据copy到用户内存空间中</li></ol><h2 id="五种模型"><a href="#五种模型" class="headerlink" title="五种模型"></a>五种模型</h2><ol><li>阻塞I/O：指I/O调用结果返回之前,当前进程会被挂起(进入睡眠状态) ，只有在得到返回结果后, 才能继续执行。</li><li>I/O多路复用：进程调用一个管理I/O的特殊库函数，此库函数可以接受并管理多个I/O请求，进程则可以同时等待多个I/O请求，可以提高效率。第二阶段依然需要工作进程参与库函数把内核空间数据复制到用户空间，第二阶段依旧阻塞</li><li>非阻塞I/O：进程发起I/O调用，I/O自己知道需过一段时间完成，就立即通知进程进行别的操作，则为非阻塞I/O</li><li>事件（信号）驱动I/O：进程发起调用,通过回调函数, 内核会记住是哪个进程申请的,一旦第一阶段完成了,就可以向这个进程发起通知,这样第一阶段就是非阻塞的,进程不需要忙等了, 但是第二阶段依然是阻塞的</li><li>异步I/O：无论第一第二段, 不再向系统调用提出任何反馈, 只有数据完全复制到服务进程内存中后, 才向服务进程返回ok的信息,其它时间,进程可以随意做自己的事情,直到内核通知ok信息</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>OO第三单元总结</title>
      <link href="/2024/05/16/oo/oo-unit3-summary/"/>
      <url>/2024/05/16/oo/oo-unit3-summary/</url>
      
        <content type="html"><![CDATA[<h1 id="测试过程"><a href="#测试过程" class="headerlink" title="测试过程"></a>测试过程</h1><h2 id="黑箱测试和白箱测试"><a href="#黑箱测试和白箱测试" class="headerlink" title="黑箱测试和白箱测试"></a>黑箱测试和白箱测试</h2><p>黑箱测试注重“结果”，它只关心程序能否完成需求，而不关心代码实现方式、内部架构；白箱测试注重“过程”，需要去关注代码内部结构、实现方法，测试时要关注方法覆盖率、分支覆盖率等。</p><p>互测我一般采用的是黑箱测试，对自己的代码主要采用的是黑箱测试，同时有部分的白箱测试。</p><h2 id="对单元测试、功能测试、集成测试、压力测试、回归测试的理解"><a href="#对单元测试、功能测试、集成测试、压力测试、回归测试的理解" class="headerlink" title="对单元测试、功能测试、集成测试、压力测试、回归测试的理解"></a>对单元测试、功能测试、集成测试、压力测试、回归测试的理解</h2><ol><li>单元测试：对代码的最小单元进行测试，如一个方法、一个类。典型的例子就是Junit，这个单元的作业也要求了用Junit完成一些方法的单元测试，它是一种白箱测试。</li><li>功能测试：关注系统的功能性需求，通常是从使用者的角度出发，观察测试系统的输入和输出是否符合预期，是一种黑箱测试。</li><li>集成测试：在将单个软件模块组合在一起形成完整的系统后，测试这些模块之间的交互是否正常的过程。它的目标是验证不同模块之间的接口是否正确，并且模块之间的通信是否按照预期工作。集成测试可以帮助发现模块之间的兼容性问题和集成错误。</li><li>压力测试：在正常操作条件的前提下，输入具有压力的数据量，以确定程序在超出正常工作负载的情况下的表现，它主要考虑一些极端情况，如高并发、大量数据、极端数据。个人感觉这个单元的重点就是压力测试，强测中大量压力测试，算法稍微设计不合理就会CPU_TIME_LIMIT_EXCEED。</li><li>回归测试：对代码进行修改后重新运行之前的测试样例，以确保没有引入新的错误或者影响到现有的功能。我在测试过程中会经常使用此方法，因为本单元作业很多地方需要对算法进行优化，我一般第一版是没有进行算法优化的，我会保留第一版运行后的输出结果，后续修改算法后会重新运行这些样例，然后与之前的输出结果进行比对。</li></ol><h2 id="数据构造策略"><a href="#数据构造策略" class="headerlink" title="数据构造策略"></a>数据构造策略</h2><p>很明显感觉到这个单元随机生成数据无法很好得保证正确率，尤其是性能方面。所以需要手动构造一些极端数据，比如构造高并发的指令数据，反复执行某个指令，看看运行时间是否超过限制；构造边界数据，避免数据溢出导致的出错。</p><h1 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h1><p>本单元的架构基本是参照着JML实现各个类和方法，在此基础上添加了UnionFind、Dijkstra类做性能的优化，Sender类单纯是为了满足MyNetwork小于500的要求分出去的一个方法。</p><p><img src="/../../images/OO/unit3.jpg" alt="架构图"></p><h1 id="性能改进"><a href="#性能改进" class="headerlink" title="性能改进"></a>性能改进</h1><h2 id="HashMap替代ArrayList"><a href="#HashMap替代ArrayList" class="headerlink" title="HashMap替代ArrayList"></a>HashMap替代ArrayList</h2><p>JML中的描述都是数组形式，所以刚开始很自然地选择了ArrayList，但是这些数据结构有很多查找操作，所以最后都改成了HashMap，以降低查找操作的时间复杂度</p><h2 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h2><p>用于判断两个人是否有联系，抽象出来就是判断图中两个结点是否属于同一个连通子图。</p><p>实现细节方面，首先进行了路径压缩的优化，避免每次查找时都要递归。其次，由于人与人之间的关系会修改，有可能会断开连接（即关系图中结点之间的边可能会被删掉），所以需要重建并查集，这里借鉴了“写时复制”的思想，</p><ol><li>在MyNetwork里设置了一个标记resetUnion，初始为false</li><li>若有添加人的命令，则在并查集中加入该person</li><li>若有添加关系的命令，则调用union把两人放入同一个集合</li><li>若有修改关系的命令<ul><li>若resetUnion为false，则看修改关系是否会导致两个人的连接被删除，若会删除，则resetUnion置为true</li><li>若resetUnion为true，就不要管了(千万别因为没删关系就重新置为false)，这说明之前有命令已经导致并查集需要重建</li></ul></li><li>当要查询两个人之间是否关联时<ul><li>若resetUnion为false，则直接调用isSameSet()判断两人是否连接</li><li>若resetUnion为true，则先调用initialize()清空，然后调用setup()重建并查集，然后查询。同时不要忘了<strong>把restUnion置为false</strong></li></ul></li></ol><h2 id="dijkstra求最短路径"><a href="#dijkstra求最短路径" class="headerlink" title="dijkstra求最短路径"></a>dijkstra求最短路径</h2><p>这个没什么好说的，数据结构课讲过的算法，但是不记得了，然后又重新学了一遍。java可以用优先队列实现，但是注意比较器不要用两个数相减的方式比大小，会爆int。</p><h3 id="动态维护"><a href="#动态维护" class="headerlink" title="动态维护"></a>动态维护</h3><p>在添加人、关系的时候就计算出来结果，每新添加人、关系时都要判断是否需要修改，这个方法用在计算bestAquaintance、tripleSum、valueSum等地方。能动态维护的都要动态维护，事实上这个单元不能出现任何复杂度大于O(n^2)的方法……不然强测一定会寄</p><h1 id="Junit测试"><a href="#Junit测试" class="headerlink" title="Junit测试"></a>Junit测试</h1><p>上学期已经用Junit做过一些单元测试，但是本单元的Junit让我新学会了用Parameters做参数化测试。参数化测试可以生成多组数据一同进行测试，保证了数据的覆盖率，生成过程中我会尽量考虑所有情况，例如生成关系时稠密图、稀疏图都要有一定的生成概率，Message信息每种类型都要有可能出现。</p><p>断言时JML就派上用场了，根据JML的规格信息一一编写断言。注意对于pure方法，要保证属性前后没有发生任何改变，所有可能会有深浅克隆的问题，我的解决办法是在数据生成阶段就进行克隆，用一个shadowNetwork与Network同步生成数据，保证二者一模一样。</p><p>用junit进行测试确实帮助我发现了一些代码的bug，例如sendMessage后message没有正确移除，然后发现我的remove语句有问题。</p><p>不过编写Junit的过程也确实很繁琐，而且要保证数据生成没有问题，有时候还得先改junit的bug<del>(汗流浃背了)</del>。</p><h1 id="学习体会"><a href="#学习体会" class="headerlink" title="学习体会"></a>学习体会</h1><p>本单元了解了JML的语法，训练了根据JML给出的规格编写代码的能力，体会到了JML语言的严谨性，但是理解JML的过程非常痛苦，有时候还会看错括号。</p><p>另外，本单元对性能优化的要求非常高，对我算法的学习有很大帮助，有的算法以前学过但是忘了，所以又重新复习、实践了一遍；有的算法是第一次了解，学到了新知识非常好！<del>(虽然后来可能因为优化不够被替换掉了)</del>。</p><p>这个单元也有很多遗憾，感觉我对高并发的情况考虑还是不够周全，所以每次作业最后都出现了一些CTLE的bug，哎需要好好反思，这个问题在第二单元也出现过，感觉它其实是一个在实际应用中也很需要关注的问题。总之，牢记这次的教训！</p>]]></content>
      
      
      <categories>
          
          <category> OO </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OADP</title>
      <link href="/2024/05/02/dl/ovd/"/>
      <url>/2024/05/02/dl/ovd/</url>
      
        <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><h2 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h2><p>把一个大的模型（教师模型）里面的知识萃取蒸馏出来，并浓缩到一个小的（学生）模型中。是一种用于模型压缩和迁移学习的技术，其主要思想是通过将一个大型模型的知识传递给一个小型模型来提高小型模型的性能。</p><h2 id="软标签和硬标签"><a href="#软标签和硬标签" class="headerlink" title="软标签和硬标签"></a>软标签和硬标签</h2><table><thead><tr><th></th><th>硬标签</th><th>软标签</th></tr></thead><tbody><tr><td>定义</td><td>一种离散的、确定性的类别表示方式。每个样本被赋予一个明确的、唯一的类别标签</td><td>一种连续的、概率分布形式的类别表示方式。每个样本的标签是一个概率向量，表示它属于每个类别的概率。</td></tr><tr><td>应用</td><td>常用于传统的监督学习任务，如分类问题</td><td>常用于<strong>知识蒸馏</strong>等场景，其中<strong>大型模型的输出可以作为软标签传递给小型模型</strong>。</td></tr><tr><td></td><td></td><td>软标签包含了样本与多个类别之间的相似度信息，因此它们可以使模型更加平滑地处理类别之间的边界，并减少对单一类别的过度依赖</td></tr></tbody></table><h2 id="蒸馏温度"><a href="#蒸馏温度" class="headerlink" title="蒸馏温度"></a>蒸馏温度</h2><p>知识蒸馏中的一个超参数，它控制了模型预测的软标签分布的“软化”程度。蒸馏温度的作用是在生成软标签时引入一个温度参数，从而调整标签的相对概率分布。这个温度参数通常是一个正的实数。例如分类问题最后总是用softmax函数将输出转化为概率分布，用上蒸馏温度就变成如下形式：<br>$$<br>softmax(z/T)_i = \frac{e^{z_i/T}}{\sum_j e^{z_j/T}}<br>$$</p><p>作用：</p><p>软标签的平滑性： 增加蒸馏温度会使软标签的概率分布更平滑。较高的温度将导致更平缓的概率曲线，使得模型更容易学到相对均匀的分布。这对于小型模型的训练过程中更容易捕捉大型模型的知识，因为软标签的平滑性有助于抑制训练时的过拟合。</p><p>控制标签的尖峰性： 较低的温度会使软标签的分布更接近硬标签，即更尖锐，更接近确定性。这可能使得模型更注重大型模型中预测概率最高的类别，但相对容易受到噪声的影响。</p><p>控制模型的自信程度： 蒸馏温度还可以影响模型的自信度。较高的温度导致模型更不确定，而较低的温度则会使模型更自信。这可以在训练过程中控制模型的泛化行为。</p><p>控制损失函数的平坦性： 由于知识蒸馏的损失函数通常涉及到交叉熵，温度参数也可以影响损失函数的平坦性。这有助于更稳定地训练小型模型，特别是当大型模型和小型模型结构不同或训练数据较小的情况下。</p><h1 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h1><ol><li><p>Object-Aware Knowledge Extraction：adaptively transforms object proposals and adopts object-aware mask attention to obtain precise and complete knowledge of objects.</p></li><li><p>Distillation Pyramid：introduces global and block distillation for more comprehensive knowledge transfer to compensate for the missing relation information in object distillation</p><ul><li>knowledge extraction</li><li>knowledge transfer</li></ul></li></ol><p>obj token和cls token的区别是什么呢？论文的意思好像是obj关注的是图像的提议框里的对象，cls关注的是整个这张图像，一个是部分，一个是整体，但是obj的生成不也要用“这个图像包含哪个<strong>类别</strong>的目标”的信息，岂不是和cls一样了？</p><h1 id="mmdection"><a href="#mmdection" class="headerlink" title="mmdection"></a>mmdection</h1><ul><li><strong>apis</strong> 为模型推理提供高级 API。</li><li><strong>structures</strong> 提供 bbox、mask 和 DetDataSample 等数据结构。</li><li><strong>datasets</strong> 支持用于目标检测、实例分割和全景分割的各种数据集。<ul><li><strong>transforms</strong> 包含各种数据增强变换。</li><li><strong>samplers</strong> 定义了不同的数据加载器采样策略。</li></ul></li><li><strong>models</strong> 是检测器最重要的部分，包含检测器的不同组件。<ul><li><strong>detectors</strong> 定义所有检测模型类。</li><li><strong>data_preprocessors</strong> 用于预处理模型的输入数据。</li><li><strong>backbones</strong> 包含各种骨干网络。</li><li><strong>necks</strong> 包含各种模型颈部组件。</li><li><strong>dense_heads</strong> 包含执行密集预测的各种检测头。</li><li><strong>roi_heads</strong> 包含从 RoI 预测的各种检测头。</li><li><strong>seg_heads</strong> 包含各种分割头。</li><li><strong>losses</strong> 包含各种损失函数。</li><li><strong>task_modules</strong> 为检测任务提供模块，例如 assigners、samplers、box coders 和 prior generators。</li><li><strong>layers</strong> 提供了一些基本的神经网络层。</li></ul></li><li><strong>engine</strong> 是运行时组件的一部分。<ul><li><strong>runner</strong> 为 <a href="https://mmengine.readthedocs.io/zh_CN/latest/tutorials/runner.html">MMEngine 的执行器</a>提供扩展。</li><li><strong>schedulers</strong> 提供用于调整优化超参数的调度程序。</li><li><strong>optimizers</strong> 提供优化器和优化器封装。</li><li><strong>hooks</strong> 提供执行器的各种钩子。</li></ul></li><li><strong>evaluation</strong> 为评估模型性能提供不同的指标。</li><li><strong>visualization</strong> 用于可视化检测结果。</li></ul><h2 id="faster-RCNN核心代码"><a href="#faster-RCNN核心代码" class="headerlink" title="faster RCNN核心代码"></a>faster RCNN核心代码</h2><p>mmdet/models/backbones/resnet.py</p><p>mmdet/models/necks/fpn.py</p><p>mmdet/models/dense_heads/rpn_head.py</p><p>mmdet/models/roi_heads/standard_roi_head.py</p><h2 id="Q-A"><a href="#Q-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><p>teacher甚至student是指代什么？</p><blockquote><p>知识蒸馏里的术语</p></blockquote><p>embedding和全连接有什么区别？感觉两者都是把高维向量平铺开。</p><blockquote><p>embedding层主要用于处理离散型数据，而全连接层则可以处理任意类型的数据</p></blockquote><p>nn.BatchNorm2d和nn.LayerNorm有什么区别和联系？</p><blockquote><ol><li><p>计算<br>$$<br>nn.BatchNorm2d : \text{E}[x]c = \frac{1}{N \times H \times W} \sum_{n=1}^N \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n,c,h,w} \nn.LayerNorm : \text{E}[x]{c} = \frac{1}{H \times W}\sum_{h=1}^{H} \sum_{w=1}^{W} x_{c,h,w}<br>$$</p></li><li><p>应用场景</p><ul><li>BatchNorm通常用于CNN（卷积神经网络）的卷积层之后，用于调整数据的分布，加速网络训练。由于BatchNorm依赖于mini-batch的统计数据，因此它对于较小的batch size可能不太稳定。</li><li>LayerNorm则常用于RNN（循环神经网络）和Transformer等模型，因为它们在处理序列数据时，每个样本的长度可能不同，而LayerNorm可以对每个样本进行独立的归一化操作。</li></ul></li></ol></blockquote><p>nn.MultiheadAttention和F.multi_head_attention_forward有什么区别?</p><blockquote><p>相比 <code>nn.MultiheadAttention</code>，<code>F.multi_head_attention_forward</code> 提供了更底层的控制，允许用户直接操作多头注意力的各个组成部分。然而，由于它不是一个模块，因此它通常不会被直接嵌入到神经网络模型中。相反，它可能被 <code>nn.MultiheadAttention</code> 或其他高级模块所使用。</p></blockquote><p>ABCMeta是什么？</p><blockquote><p>ABCMeta是一个元类（metaclass），用于定义抽象基类的元信息。通过将ABCMeta作为元类，可以在类定义阶段对类进行检查和修饰。ABCMeta元类提供了一些功能，例如检查子类是否实现了抽象方法、注册具体实现类等。</p><p>在Python中，类是通过类来创建的，而<strong>创建类的类就是元类</strong>。元类的主要目的是控制类的创建行为。Python的特别之处在于可以创建自定义元类，而ABCMeta就是这样一个自定义元类，用于创建自定义的抽象基类。</p></blockquote><p>logits是什么？</p><blockquote><p>logits（或称为分数、原始分数、未校准的输出等）是模型输出的原始、未归一化的预测值</p></blockquote><p>这里为什么要除以embedding向量的开方？一种初始化规则？</p><p><img src="/../../images/tmp/image-20240504102504742.png" alt="image-20240504102504742"></p><p>oake模块下base.py文件中Validator类有什么作用呢？</p><p><img src="/../../images/tmp/image-20240504181309157.png" alt="image-20240504181309157"></p><blockquote><p>OKAE这三文件都可以单独运行，独立的把需要的特征提取出来保存。因为OADP中global、block和object使用的蒸馏特征在提取的时候会特别耗时，所以作者把这三个蒸馏特征的提取过程单独用这三个文件预先生成之后保存起来了，后面DP部分训练时候是直接读取的这些特征</p></blockquote><p>oadp/oake/objects.py里Dataset类的_expand方法有什么用？为什么要扩大bbox，而且还分了很多种扩大模式？</p><blockquote><p>为了把proposal的不规则区域转化成正方形，因为CLIP的输入是224*224的正方形，可以认为这样expand之后有助于减小输入CLIP特征的形变</p></blockquote><p>Compose和nn.Sequential有什么区别和联系？</p><blockquote><ol><li>联系：</li></ol><ul><li>二者都提供了一种顺序执行的方式，可以将多个操作或层按照特定的顺序组合在一起。</li><li>它们都简化了模型的构建过程，使得代码更加清晰和易于管理。</li></ul><ol><li>区别：</li></ol><ul><li>功能和应用领域：Compose主要用于<strong>组合图像预处理操作</strong>，是torchvision.transforms模块中的一个类。而nn.Sequential则用于<strong>构建神经网络模型</strong>，是PyTorch中torch.nn模块的一个容器类。</li><li>组成部分：Compose中的操作通常是对图像的变换，如裁剪、旋转、标准化等。而nn.Sequential中的层通常是神经网络的各种组成部分，如线性层、卷积层、激活函数等。</li><li>使用方式：Compose通常与图像数据集一起使用，用于在将数据输入到神经网络之前进行预处理。而nn.Sequential则直接定义了一个神经网络模型，可以通过前向传播函数将数据传递给模型并获得输出。</li></ul></blockquote><p>oadp/dp/classifiers.py的BaseClassifier类中的bg_embedding是什么？</p><p>mixin是什么意思？</p><blockquote><p>Mixin实质上是一个带有部分或全部实现的接口或类，它可以被其他类继承或混入，从而将这些功能组合到子类中。Mixin模式的主要作用是代码复用，通过减少代码冗余度，使代码更加清晰、可维护和易于扩展。</p></blockquote><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://t.csdnimg.cn/RGdL1">知识蒸馏</a></p><p><code>zip</code> 是 Python 的一个内置函数，用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">for foreground, bbox in zip(foregrounds, bboxes):  </span><br><span class="line">    objects.append(self._object(image, bbox))  </span><br><span class="line">    masks.append(self._mask(foreground, bbox))</span><br><span class="line">//zip 在这里用于并行地遍历两个列表（或任何可迭代对象），确保在每次迭代中，foregrounds 和 bboxes 的对应元素都被一起处理</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Transformer模型</title>
      <link href="/2024/04/27/dl/transformer/"/>
      <url>/2024/04/27/dl/transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h1><p><img src="/../../images/deepLearning/Transformer%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84.png" alt="Transformer整体架构"></p><p>图上全部为训练过程，去掉虚线框里的就是推理过程（没有正确答案输入的部分了）</p><h2 id="Word-Embedding词嵌入"><a href="#Word-Embedding词嵌入" class="headerlink" title="Word Embedding词嵌入"></a>Word Embedding词嵌入</h2><p>将输入(输入句子有m个词)转化为向量，假设嵌入维度为n，则一个词对应一个n维向量，整个向量维度为m x n。</p><h2 id="Positional-Encoding位置编码"><a href="#Positional-Encoding位置编码" class="headerlink" title="Positional Encoding位置编码"></a>Positional Encoding位置编码</h2><p>$$<br>PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d_{model}}}) \<br>PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d_{model}}})<br>$$</p><p>将每个位置信息编码后与对应位置词的编码相加，这样后面的自注意力机制可以同时考虑输入的词本身和顺序信息。</p><h2 id="Encoder-多次反复使用，一般为6次"><a href="#Encoder-多次反复使用，一般为6次" class="headerlink" title="Encoder(多次反复使用，一般为6次)"></a>Encoder(多次反复使用，一般为6次)</h2><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><h3 id="Add-Norm"><a href="#Add-Norm" class="headerlink" title="Add&amp;Norm"></a>Add&amp;Norm</h3><ol><li>残差结构：将输入矩阵X与上一步得到的矩阵相加，这里是经过多头注意力机制得到的矩阵Z，待会Feed Forward后得到的矩阵还会有一个Add&amp;Norm步骤</li><li>LayerNorm：对一个样本的所有特征计算均值和方差，然后归一化</li></ol><h3 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h3><p>就是普通的全连接网络<br>$$<br>FFN(x) = max(0, xW_1 + b_1)W_2 + b_2<br>$$</p><h2 id="Decoder-多次反复使用，一般为6次"><a href="#Decoder-多次反复使用，一般为6次" class="headerlink" title="Decoder(多次反复使用，一般为6次)"></a>Decoder(多次反复使用，一般为6次)</h2><p>与Encoder差不多，但是新增加了一个Masked Multi-Head Attention。</p><h3 id="Masked-Multi-Head-Attention（带掩码的多头注意力机制）"><a href="#Masked-Multi-Head-Attention（带掩码的多头注意力机制）" class="headerlink" title="Masked Multi-Head Attention（带掩码的多头注意力机制）"></a>Masked Multi-Head Attention（带掩码的多头注意力机制）</h3><blockquote><p>Transformer训练过程采用了Teacher Forcing的训练模型，会将原始输入和正确答案都会喂给模型</p></blockquote><p>为了防止模型知道后续输出单词（正确答案）的信息，需要掩码机制掩盖后面词的信息。</p><p>实现方式：构造掩码矩阵（下三角矩阵），将归一化后的注意力分数与掩码矩阵按位相乘。其他部分和多头注意力机制一样。</p><p><img src="/../../images/deepLearning/%E6%8E%A9%E7%A0%81%E7%9F%A9%E9%98%B5.png" alt="掩码矩阵"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="http://t.csdnimg.cn/lire2">自然语言处理Transformer模型最详细讲解（图解版）</a></p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CLIP</title>
      <link href="/2024/04/26/dl/clip/"/>
      <url>/2024/04/26/dl/clip/</url>
      
        <content type="html"><![CDATA[<p>CLIP:Constrastive Language-Image Pre-training</p><h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>embedding：一种将高维数据(如文本或图像)转换为较低维度的向量表示的技术</p><h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><p><img src="/../../images/deepLearning/CLIP.png" alt="CLIP结构"></p><h1 id="推理过程"><a href="#推理过程" class="headerlink" title="推理过程"></a>推理过程</h1><p>把需要分类的图片送入image encoder得到特征，拿图片的特征和所有文本特征算余弦相似性，选最相似的那个文本特征对应的句子，从而完成了分类任务</p><h2 id="余弦相似度cosine-similarity"><a href="#余弦相似度cosine-similarity" class="headerlink" title="余弦相似度cosine similarity"></a>余弦相似度cosine similarity</h2><p>用来度量文本与图像之间的对应关系，值越大表示对应关系越强。其实就是计算两个向量夹角的余弦值。</p><h1 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h1><p>图片和文字配对，分别输入到Image Encoder、Text Encoder得到特征。若每个training batch有n个图像-文本对，就可以得到n个图片-文本对。</p><p>配对的图片-文本对就是正样本，描述的是同一个东西。特征矩阵里对角线上的都是正样本，矩阵中非对角线上的元素都是负样本。有了正负样本，模型就可以通过<strong>对比学习</strong>的方式去训练，不需要任何手工标注。</p><h2 id="Image-Encoder：ViT"><a href="#Image-Encoder：ViT" class="headerlink" title="Image Encoder：ViT"></a>Image Encoder：ViT</h2><p>结构如图，概括一下就是一个Patch+Position Embedding加多个Transformer Encoder</p><p><img src="/../../images/deepLearning/ViT%E7%BB%93%E6%9E%84.png" alt="ViT结构"></p><h3 id="一个Patch-Position-Embedding"><a href="#一个Patch-Position-Embedding" class="headerlink" title="一个Patch+Position Embedding"></a>一个Patch+Position Embedding</h3><p>对输入图像进行卷积，得到的矩阵平铺展开为一个一维向量存储图片的序列信息（mx1)。向量中的每个值代表原图像中的一个卷积核大小的块状区域，代表一个序列单元。当然卷积核会有n个，所有是个mxn的特征矩阵</p><blockquote><p>平铺完成后，我们会在图片序列中添加上Cls Token，<strong>该Token会作为一个单位的序列信息</strong>一起进行特征提取，<strong>图中的这个0*就是Cls Token</strong></p></blockquote><p>添加完成Cls Token后，再<strong>为所有特征添加上位置信息</strong>，<strong>这样网络才有区分不同区域的能力</strong>。添加方式其实也非常简单，我们生成一个<strong>197, 768的参数矩阵</strong>，这个参数矩阵是可训练的，把这个矩阵加上<strong>197, 768的特征层</strong>即可。</p><h3 id="多个Transformer-Block"><a href="#多个Transformer-Block" class="headerlink" title="多个Transformer Block"></a>多个Transformer Block</h3><p>上一步获得的序列信息输入，通过自注意力机制，关注每个块的重要程度。</p><ol><li>Multi-Head Attetion</li><li>两个连接层</li></ol><h2 id="Text-Encoder：Bert"><a href="#Text-Encoder：Bert" class="headerlink" title="Text Encoder：Bert"></a>Text Encoder：Bert</h2><p>和Image Encoder类似</p><blockquote><p>补充：</p><ol><li>Unicode编码几乎可以表示全世界的所有语言字符，常说的ASCII编码是Unicode编码的一个子集。Unicode码点是字符在Unicode字符集中的唯一标识，而UTF-8则是将Unicode码点转换为字节序列的一种编码方式。</li><li>UTF-8是Unicode的一种实现方式，也被称为Unicode转换格式（UTF），是“二进制表示”。它是对Unicode字符集进行编码的一种编码方式，给Unicode字符集加了一个存储类型前缀。</li><li>有时为了不让字典太大,只会把出现频次大于某个阈值的词丢到字典里边,剩下所有的词都统一编码成#UNK</li></ol></blockquote><h2 id="综合图像和文本特征"><a href="#综合图像和文本特征" class="headerlink" title="综合图像和文本特征"></a>综合图像和文本特征</h2><p>$$<br>min(\sum_{i=1}^N\sum_{j=1}^N (I_i \cdot T_j)<em>{i \not= j} - \sum</em>{i=1}^N (I_i \cdot T_i))<br>$$</p><p>得到图像特征和文本特征后，接下来的训练任务转为最大化 N 个正样本的余弦相似度, 最小化$N^2 - N$个负样本的余弦相似度，即最大化对角线中的数值, 最小化其它非对角线的数值</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.zhihu.com/tardis/zm/art/34656727?source_id=1005">zero-shot learning</a></p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Faster RCNN</title>
      <link href="/2024/04/21/dl/fasterrcnn/"/>
      <url>/2024/04/21/dl/fasterrcnn/</url>
      
        <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>bbox(Bouding Box边界框)：包含物体的最小矩形</p><p>NMS(非极大值抑制Non-Maximum Suppression)：选出IoU值最高的框，去掉与它的IoU值较高的框（即重复区域较大），然后再选出IoU次大的框，重复上述过程。</p><p>two-stage方法：如R-CNN系列算法，其主要思路是先通过启发式方法（selective search）或者 CNN 网络（RPN)产生一系列稀疏的候选框，然后对这些候选框进行分类(classification)与回归(bounding box regression)，two-stage方法的优势是准确度高；<br>one-stage方法：如YOLO和SSD，其主要思路是均匀地在图片多个层数的特征图上进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归，整个过程只需要一步，所以其优势是速度快。但是均匀的密集采样的一个重要缺点是训练比较困难，这主要是因为正样本与负样本（背景）极其不均衡，导致模型准确度稍低。</p><h1 id="Fast-RCNN的改进"><a href="#Fast-RCNN的改进" class="headerlink" title="Fast RCNN的改进"></a>Fast RCNN的改进</h1><table><thead><tr><th>RCNN</th><th>Fast RCNN</th></tr></thead><tbody><tr><td>对一张图片提取了大量候选区域，并把它们都输入到CNN进行特征提取，这些候选区域有大量重复，造成特征提取的浪费</td><td>将整个图片归一化后直接送入CNN，<strong>卷积层不进行候选区的特征提取</strong>，在最后一个池化层加入候选区域坐标信息，进行特征提取的计算</td></tr><tr><td>目标分类与候选框的回归是独立的两个操作，并且需要大量特征作为训练样本</td><td>将目标分类与候选框回归统一到CNN网络中来，不需要额外存储特征</td></tr></tbody></table><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/../../images/deepLearning/FastRCNN%E6%A1%86%E6%9E%B6.png" alt="FastRCNN框架"></p><h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>提出背景：<strong>Fast R-CNN通过选择性搜索</strong>（selective search）找出所有的候选框，仍然比较耗时</p><p>解决方法：<strong>加入一个提取边缘的神经网络，把找候选框的工作交给这个神经网络（称作Region Proposal Network，RPN）</strong></p><p>具体做法如下：</p><ol><li>将RPN放在一个卷积层的后面</li><li>RPN直接训练得到候选区域</li></ol><h2 id="结构-检测过程"><a href="#结构-检测过程" class="headerlink" title="结构(检测过程)"></a>结构(检测过程)</h2><p>基本框架如下：<br><img src="/../../images/deepLearning/FasterRCNN%E6%A1%86%E6%9E%B6.png" alt="FasterRCNN框架"></p><p>具体结构如下：</p><p><img src="/../../images/deepLearning/FasterRCNN%E5%85%B7%E4%BD%93%E7%BB%93%E6%9E%84.png" alt="FasterRCNN具体结构"></p><h3 id="conv-layers"><a href="#conv-layers" class="headerlink" title="conv layers"></a>conv layers</h3><p>特征提取网络，用于提取特征。通过一组conv+relu+pooling层来提取图像的feature maps。</p><ol><li>13个conv层：kernel_size=3,  padding=1,  stride=1。因为$n = \frac{n + 2p - f}{s} + 1$，所以经过每个卷积层后feature map维度不变。</li><li>13个relu层：不改变特征图维度</li><li>4个pooling层：kernel_size=2,  padding=0,  stride=2。每池化一次，特征图维度变为原来的1/2，最终变为原来的1/16</li></ol><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><p>区域候选网络，提前做了一部分检测，完成了目标定位，但还没分出具体类别。流程如下：</p><p>![FasterRCNN RPN部分](../../images/deepLearning/FasterRCNN RPN部分.png)</p><p>拿到conv layers的feature map（M/16 x N/16）后，先经过一个3x3卷积，卷积核个数为256，所以通过这个卷积层后feature map的通道数也是256(M/16 x N/16 x 256)。</p><p>之后分成两个任务：</p><ul><li><p>上部分cls layer分类：判断所有预设anchor内是否有目标（二分类），即是正样本（positive）还是负样本（negative）。</p><blockquote><p>补充anchor：对于feature maps的每一个像素点，设置3 x 3种预设anchor，比例3种：1:1、1:2、2:1；边长3种：按原始目标大小灵活设置。这样比例和边长一一配对就可以得到3x3=9种预设锚框。</p></blockquote><ol><li>设anchor种类为k(即上文的9个)。M/16 x N/16 x 256的特征经过1x1卷积就得到了(M/16)x(N/16)x2k的输出.“2”是因为这里做的是一个二分类且用的softmax，所以feature map上每个点的每个anchor对应2个值。</li><li>reshape层对feature map进行维度变换，使得有一个单独的维度为2 ，方便在softmax进行操作</li><li>softmax进行分类</li><li>reshape恢复原状</li></ol></li><li><p>下部分reg layer回归：bbox regression(边界框修正)，修正anchors得到较为准确的proposals</p><p>(M/16)x(N/16)x256的特征通过1x1卷积得到(M/16)x(N/16)x4k的输出，“4”是因为这里是生成每个anchor的坐标偏移量（用于修正anchor），[tx,ty,tw,th]共4个所以是4k。注意，这里输出的是<strong>坐标偏移量</strong>，不是坐标本身，要得到修正后的anchor还要用原坐标和这个偏移量运算一下才行。</p></li></ul><p>两个任务在proposal层合并，proposal层负责综合正样本positive anchor和对应bbox修正后得到的proposals，输出一系列proposals左上角和右下角坐标轴，同时剔除太小和超出边界的proposal。</p><h3 id="RoI-Pooling（兴趣域池化）"><a href="#RoI-Pooling（兴趣域池化）" class="headerlink" title="RoI Pooling（兴趣域池化）"></a>RoI Pooling（兴趣域池化）</h3><ol><li><p>收集RPN生成的proposals，并将每个proposal映射到对应feature map中的区域</p></li><li><p>把这个区域划分成pooled_w x pooled_h个网格</p></li><li><p>对网格的每部分做max pooling</p><blockquote><p>这么做是因为全连接层每次输入特征的维度必须是相同的。这样保证大小不同的proposal最后输出都是相同大小（pooled_w x pooled_h）。</p></blockquote></li><li><p>生成的结果proposals feature maps（pooled_w x pooled_h x 256）送入后续全连接层继续做分类（具体哪一个类别）和回归。</p></li></ol><h3 id="classification-and-regression"><a href="#classification-and-regression" class="headerlink" title="classification and regression"></a>classification and regression</h3><ul><li>利用proposals feature maps计算出具体的类别</li><li>再做一次bbox regression获得检测框最终的精确位置。</li></ul><h2 id="训练过程（two-stage）"><a href="#训练过程（two-stage）" class="headerlink" title="训练过程（two stage）"></a>训练过程（two stage）</h2><p><img src="/../../images/deepLearning/FasterRCNN%E5%8F%8C%E9%98%B6%E6%AE%B5.png" alt="FasterRCNN双阶段检测"></p><ul><li>训练RPN网络</li><li>训练分类网络</li></ul><p><img src="/../../images/deepLearning/FasterRCNN%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.png" alt="训练过程"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://blog.csdn.net/kk123k/article/details/86515513">R-CNN论文详细解读</a></p><p><a href="http://t.csdnimg.cn/PnIKI">Faster RCNN</a></p><p>[超级详细的Faster RCNN解读](一文读懂Faster RCNN - 你再好好想想的文章 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/31426458">https://zhuanlan.zhihu.com/p/31426458</a>)</p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OO第二单元总结</title>
      <link href="/2024/04/20/oo/oo-unit2-summary/"/>
      <url>/2024/04/20/oo/oo-unit2-summary/</url>
      
        <content type="html"><![CDATA[<h2 id="架构分析"><a href="#架构分析" class="headerlink" title="架构分析"></a>架构分析</h2><h3 id="第一次作业"><a href="#第一次作业" class="headerlink" title="第一次作业"></a>第一次作业</h3><p><img src="/../../images/OO/unit2-1.png" alt="第一次作业架构"></p><p>三种线程：输入线程、调度线程、电梯线程</p><p>输入线程和调度线程共享waitQueue队列，调度线程和电梯线程共享processingQueue队列（有六个，放一个列表里统一管理）。</p><p>Passengers类用来放进入电梯的乘客，其实和RequestQueue类差不多，但是因为这部分不需要同步控制，所以很多操作都不需要，于是我新建了一个Passenger类管理。</p><p>Strategy我按指导书的要求建了这么一个类，不过在后面的作业中发现这个类好像没什么用，我就删掉了。</p><h3 id="第二次作业"><a href="#第二次作业" class="headerlink" title="第二次作业"></a>第二次作业</h3><p><img src="/../../images/OO/unit2-2.png" alt="第二次作业架构"></p><p>第二次作业增加了电梯重置请求，我在第一次作业的基础上做出了如下修改：</p><ol><li><p>修改单电梯调度：研讨课发现好多人都用的LOOK算法，比我自己写的逻辑清晰很多，所以第二次作业我也改为Look算法，参考了往年的学长的博客，电梯的Action分为：OPEN、CLOSE、REVERSE、MOVE、WAIT。每次都从T_T中选择一种，然后调整电梯的属性。</p></li><li><p>增加多电梯调度：receive方法拒绝自由竞争，我用的均分方法，但是忽略了高并发的问题，强测别狠狠hack了T_T</p></li><li><p>增加reset处理：本来尝试新增SafeResetRqst类，增加ArrayList&lt;SafeResetRqst&gt;  resetRequests对象，作为InputThread和ElevatorThread之间的一个托盘，但是导致ScheduleThread不太好分配（可能出现检测到某个电梯的resetRqst为空，然后在往该电梯的请求队列中分配请求时该电梯的resetRqst不为空的清空）。</p><p>于是换成新增ResetTable类，每个电梯都可以与InputThread、ScheduleThread共享resetTable对象，但是只改变自己对应的resetRequest，然后ScheduleThread中多电梯调度通过调用resetTable类中的dispatch方法实现。</p></li><li><p>结束条件改变：因为waitQueue会重新接受来自电梯乘客表、等待队列的请求，所以ScheduleThread、ElevatorThread结束条件改变。我参考了往年学长的方法，用了单例模式设置一个Counter类的counter对象来计数，当InputThread结束且总请求数sum==完成请求数finished时，则可以结束ScheduleThread、ElevatorThread。</p></li></ol><h3 id="第三次作业"><a href="#第三次作业" class="headerlink" title="第三次作业"></a>第三次作业</h3><p><img src="/../../images/OO/unit2-3.png" alt="unit2-3"></p><p>第三次作业增加了重置为双轿电梯的请求。我在第二次作业的基础上做了比较大的改动，具体有如下几个方面：</p><ol><li>增加类ElevatorFactory：使用简单工厂模式生产不同的电梯，同时把需要共享的对象waitQueue、processQueues、resetTable作为ElevatorFactory类的属性，这样可以在”生产”电梯时给每个电梯“装配”这些属性。此外，这个类我也使用了单例模式，方便在其他类调用这个类里的方法。</li><li>增加类DoubleElevatorA、DoubleElevatorB管理双轿电梯：SingleElevator、DoubleElevatorA、DoubleElevatorB都是ElevatorThread的子类，其中SingleElevator就是之前作业实现的普通的单轿厢电梯</li><li>新建了一个类Status让双轿电梯共享彼此的状态：同一个井道的两个电梯必须知道互相的状态，避免在换乘楼层发生碰撞。</li><li>DoubleElevator的单电梯调度策略：在Look算法的基础上做了以下两点修改<ol><li>在电梯里没人且电梯请求队列为空时，增加了一个判断“电梯如果在换乘楼层，则继续移动一层”：为了防止两个轿厢相撞，电梯最好不要在换乘楼层久待，所以没请求的时候就赶紧离开</li><li>电梯的Action枚举里增加了一个动作FORCE_OPEN_AND_REVERSE，即电梯到达换乘层时必须强制开门把电梯里的乘客仍回waitQueue，然后转向：这个动作放在LOOK算法的实现方法getAction()的最开始判断。为什么要把FORCE_OPEN和REVERSE这个动作放一起呢？最开始没理清，没把这两个放一起，导致之前的代码其他的动作判断全都得加判断“电梯如果在换乘楼层，根据方向判断是否需要反向，如果不需要转向再判断…(原本的LOOK算法逻辑)”。后来突然意识到电梯到达换乘楼层就肯定不能继续MOVE了，一定要转向的（REVERSE），那干脆这两个动作就放一起好了（感觉这个就很像OS里说的原语，不可分割）。这样之前的代码逻辑除了上面说的那一点，其它都不用改动了。</li></ol></li><li>删除strategy类：第三次作业了，我仍不知道这个类有什么作用，所有就把它删掉然后把单电梯调度策略写在电梯线程里了。</li></ol><h3 id="协作图"><a href="#协作图" class="headerlink" title="协作图"></a>协作图</h3><p><img src="/../../images/OO/unit2-%E5%8D%8F%E4%BD%9C%E5%9B%BE.png" alt="unit2-协作图"></p><p>三个线程组成两个“生产者-消费者”模式，其中电梯线程重置时需要将未完成的请求重新传递回总请求队列，再次由调度线程分配。</p><h3 id="变与不变"><a href="#变与不变" class="headerlink" title="变与不变"></a>变与不变</h3><p>三次作业中，请求队列表和电梯里的乘客表没有特别大变化，三个线程的结构没变，但是每个线程内部易变</p><ol><li>输入线程：请求类型增加，逻辑修改简单，只需略微修改</li><li>调度线程：分配策略需要改动</li><li>电梯线程：随着电梯类型的改变，电梯的调度策略有略微变化</li></ol><h2 id="同步块的设置和锁的选择"><a href="#同步块的设置和锁的选择" class="headerlink" title="同步块的设置和锁的选择"></a>同步块的设置和锁的选择</h2><p>同步块我全部使用的方法同步块，听到好几个同学代码块同步造成“缝隙”出现难以复现的bug，感觉还是在方法里同步不容易出问题（虽然可能会造成一些不必要的同步影响性能）。大多数共享对象的类里的方法我都是用synchronized修饰来同步，最后一次作业为了防止两个轿厢相撞新建的类Status里，我用的是读写锁。因为感觉大部分的共享对象还是写操作为主，读操作不多，用读写锁收益不大（<del>但是好歹学了读写锁，还是想用用，所以最后一次作业尝试了一下</del>），而且读写锁不能直接用wait()，实现等待需要新建一个Condition类的对象与锁关联，有点麻烦。</p><h2 id="调度器设计"><a href="#调度器设计" class="headerlink" title="调度器设计"></a>调度器设计</h2><p>调度器单开一个线程ScheduleThread，当一个“中间商”，从Input Thread的总请求池拿请求，然后根据自己的调度策略分配给特定电梯的请求队列。</p><h3 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h3><p>第一次作业不需要自己实现调度策略。</p><p>第二次作业用的均分策略，设置一个circleNum，从0-5循环，每次分配完一个请求就+1，等于6就归零。如果当前circleNum对应的电梯在重置，就跳过。但是这么分其实并不完全平均，所以互测被hack了（详见bug分析）</p><p>第三次作业的调度策略可能是估算等待时间和权重赋值的结合吧。因为第二次作业性能挺差的，然后第三次作业的时间较为充裕，所有还是想改改调度策略。调度逻辑如下：</p><ul><li>对于每个请求者，遍历所有电梯</li><li>对于每一个电梯<ul><li><p>查看电梯的当前状态，计算电梯移动到请求者所在楼层需要多久。</p></li><li><p>考虑一些特殊情况：</p><ul><li>对于单轿电梯，如果当前电梯在<strong>重置</strong>，score翻倍。</li><li>如果当前电梯的<strong>请求队列人数过多</strong>（这个“多”的权值自己设置，我设置的是超过电梯最大限乘人数就算多），score翻倍。</li><li>对于双轿电梯，如果该请求者能进去该电梯但是该电梯<strong>不能直接把它运送到目的地</strong>，score翻倍。</li></ul></li></ul></li><li>找出score分最小的电梯，准备把请求者分配给它。<ul><li>如果该电梯还在重置，就等它重置完再把请求者加入到它的等待队列。</li><li>如果该电梯不在重置或者是双轿电梯，就直接把请求者加入到它的等待队列。</li></ul></li></ul><p>可以看出来，这个分配策略非常粗糙，处处透着不严谨：时间计算不严谨，不考虑同步问题，只看开始计算这一瞬间电梯状态如何；权重赋值不严谨，对于重置、请求队列人数过多、不能直接运送到目的地这些特殊情况我都直接当成效果差不多的不利条件，都把score分翻倍，没有赋予不同的权值。但是最终分配策略的效果似乎还可以？反正强测性能分还不错，挺让我意外的。但是其实这个分配策略对高并发的处理依旧不完善（见bug分析）</p><h2 id="bug分析"><a href="#bug分析" class="headerlink" title="bug分析"></a>bug分析</h2><p>第一次作业没用LOOK写单电梯调度策略，然后自己设计的策略逻辑又非常复杂，导致电梯出现移动到非法楼层的问题，bug出现在中测，强测、互测没出问题。</p><p>第二次作业在互测被高并发的数据点hack到了，因为如果某一时刻重置的电梯很多，并且这一时刻来了大量请求，就会出现所有请求都被分配给那个没重置的电梯的情况。于是修改成每个电梯请求队列设置一个上限，超过了就不再分配，同时让重置的电梯可以接受请求者。</p><p>第三次作业再次被高并发的数据点hack（<del>有时候真的被自己蠢笑了</del>），因为我调度策略大改，然后这个地方又没考虑周全，当时想着我重置状态的电梯可以分配请求应该就没问题吧，就没细想，然后再次被hackT_T。原因就出在我的score在“请求队列人数多”这个特殊情况只是简单的翻倍，bug修复我把这里改成score *= 请求队列人数 / 该电梯最大限乘人数。</p><h2 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h2><p>其实很喜欢这个单元结合实际、贴近生活的题目，虽然刚开始写多线程非常地痛苦，因为无法单步调试，debug十分困难，尤其是在第二次作业，需要把未完成的请求者扔回总请求队列，并且ScheduleThread、ElevatorThread线程的结束条件要改变，稍有不甚就死锁了，我反复删删改改才比较完美地实现了线程控制。不过虽然过程很痛苦，结果还是不错的，至少这个单元我在强测没有因为同步控制不当出现线程不安全的bug。</p><p>以下是一些个人实现同步控制过程中困惑/不理解的概念的总结。</p><p>synchronized修饰方法的理解：</p><ul><li>如果一个类A里有多个synchronized修饰的方法，对于这个类的一个实例对象a，同一时刻只有一个synchronized方法可能被访问。因为锁的是这个实例对象。</li><li>如果一个类A里有个synchronized修饰的方法，对于这个类的多个实例对象a1,a2,a3…每个实例的这个方法都可以被不同的线程同时访问。</li><li>对于一类的对象，在synchronized修饰的方法里调用此对象的另一个synchronized修饰的方法是可行的。同样因为这个对象已经获得了锁。</li></ul><p>传给方法的参数是对对象的引用，在方法里改变这个对象是在改变这个对象本身</p><p>wait和sleep的使用：</p><p>synchronized修饰的代码块里(假设对象叫a)如果需要等待应该用a.wait(); 方法里需要线程sleep可以写成Thread.sleep()</p><table><thead><tr><th></th><th>直接调用run</th><th>线程调度执行run</th></tr></thead><tbody><tr><td></td><td>不会创建新的线程。因此，它不会表现出任何并发性。</td><td><code>start</code>方法会启动一个新的线程，并在该新线程的上下文中异步执行<code>run</code>方法</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> OO </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>视觉开发认知和检测</title>
      <link href="/2024/04/20/dl/yolo-world/"/>
      <url>/2024/04/20/dl/yolo-world/</url>
      
        <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>zero-shot learning(零样本学习)：让机器具有推理能力，例如在目标检测中，希望模型可以对从未见过的类别进行分类</p><p>开放世界目标检测：在每一个场景中检测每一个类别，应该有能力利用具有异构标签空间的多个来源的图像用于训练和推广到开放世界进行推理</p><p>开放的含义：一般情况下，模型是根据预先打好的标签来检测目标，但是这样的模型能够检测的目标类型是有限的。“开放”就是希望模型不受预先定义好的标签类别限制。</p><h2 id="多模态"><a href="#多模态" class="headerlink" title="多模态"></a>多模态</h2><p>概念：多种形态的信息（例如声音、文字、图像等）一起协作推理</p><p>结合计算机视觉和自然语言处理领域的多模态任务：让机器通过构建能够联合多种模态信息的模型来捕捉不同模态之间的对应关系和语义特征，从而能够同时处理多种形式的数据（图像、音频、文本等），加深机器对现实世界的感知。例如图像描述，我们不仅希望机器识别出实体对象及其标签，还希望它能够描述图像中实体之间的关系，以文本的形式描述出来。</p><h2 id="yolo-world：轻量级的开放词汇检测器"><a href="#yolo-world：轻量级的开放词汇检测器" class="headerlink" title="yolo-world：轻量级的开放词汇检测器"></a>yolo-world：轻量级的开放词汇检测器</h2><p><a href="https://www.bilibili.com/read/cv31740783/">参考</a></p><p>没有使用在线词汇，而是提出了一种“先提示后检测”的推理范式。具体如下：</p><ul><li><p>使用在线词汇进行训练。在训练过程中，为每个包含4张图像的mosaic样本构建一个在线词汇T。具体做法是，从mosaic图像中抽样所有涉及的正面名词，并从相应的数据集中随机抽样一些负面名词。</p></li><li><p>随后使用离线词汇进行推理。在推理阶段，采用一种”先提示-再检测”的策略，使用离线词汇以提高效率。用户可以定义一系列自定义提示，然后利用文本编码器对这些提示进行编码，并获得离线词汇嵌入。</p></li></ul><p>架构：</p><ol><li>文本编码器对输入文本进行编码。</li><li>图像编码器将输入图像编码为多尺度图像特征</li><li>图像和文本特征利用RepVL-PAN实现多级跨模态融合。</li><li>YOLO-World预测回归的边界框和对象嵌入，以匹配输入文本中出现的类别或名词。</li></ol>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>进程管理</title>
      <link href="/2024/04/10/os/os-jin-cheng/"/>
      <url>/2024/04/10/os/os-jin-cheng/</url>
      
        <content type="html"><![CDATA[<h1 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h1><h2 id="进程概念的引入"><a href="#进程概念的引入" class="headerlink" title="进程概念的引入"></a>进程概念的引入</h2><h3 id="两个基本概念：并发与并行"><a href="#两个基本概念：并发与并行" class="headerlink" title="两个基本概念：并发与并行"></a>两个基本概念：并发与并行</h3><p>并发：宏观同时，微观交替</p><p>并行：同时</p><h3 id="并发性的确定–Bernstein条件"><a href="#并发性的确定–Bernstein条件" class="headerlink" title="并发性的确定–Bernstein条件"></a>并发性的确定–Bernstein条件</h3><p>定义：</p><ul><li>R(Si)：Si的读子集, 其值在Si中被引用的变量的集合</li><li>W(Si)：Si的写子集, 其值在Si中被改变的变量的集合</li></ul><p>Bernstein条件：</p><p>两个进程S1和S2可并发，当且仅当下列条件同时成</p><p>立：</p><ul><li>R(S1) ∩ W(S2) = Φ</li><li>W(S1) ∩ R(S2) = Φ</li><li>W(S1) ∩ W(S2) = Φ</li></ul><p>Bernstein条件是判断程序并发执行结果是否可再现的充分条件。</p><p>进程的定义和特征</p><ul><li>进程是程序的一次执行；</li><li>进程是可以和别的计算并发执行的计算；</li><li>进程可定义为一个数据结构，及能在其上进行操作的一个程序；</li><li>进程是一个程序及其数据在处理机上顺序执行时所发生的活动；</li><li>进程是程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。</li></ul><p>一个进程应该包括：</p><ul><li>程序的代码</li><li>程序的数据</li><li>PC中的值</li><li>一组通用的寄存器的当前值，堆、栈</li><li>一组系统资源（如打开的文件）</li></ul><p>进程与程序的区别</p><table><thead><tr><th></th><th>进程</th><th>程序</th></tr></thead><tbody><tr><td></td><td>动态</td><td>静态</td></tr><tr><td></td><td>暂时</td><td>永久</td></tr><tr><td>组成</td><td>程序、数据和进程控制块</td><td></td></tr><tr><td>两者之间的对应关系</td><td>通过多次执行，一个程序可以对应多个进程</td><td>通过调用关系，一个进程可包括多个程序</td></tr></tbody></table><h2 id="进程状态与控制"><a href="#进程状态与控制" class="headerlink" title="进程状态与控制"></a>进程状态与控制</h2><h3 id="进程控制"><a href="#进程控制" class="headerlink" title="进程控制"></a>进程控制</h3><p>进程控制的主要任务是创建和撤销进程，以及实现进程的状态转换。<strong>由内核来实现</strong></p><h4 id="原语"><a href="#原语" class="headerlink" title="原语"></a>原语</h4><p>由若干条指令所组成的指令序列，来实现某个特定操作功能。</p><ul><li>指令序列是连续的，不可分割</li><li>是操作系统核心组成部分</li><li>必须在管态（内核态）下执行，且常驻内存</li></ul><p>与系统调用的区别：不可中断</p><p>创建原语：fork, exec</p><p>在fork函数执行完毕后，如果创建新进程成功，则出现两个进程，一个是子进程，一个是父进程。在子进程中，fork函数返回0，在父进程中，fork返回新创建<strong>子进程的进程ID</strong>。我们可以通过fork返回的值来判断当前进程是子进程还是父进程。如果出现错误，fork返回一个负值。</p><p>撤消原语：kill</p><h3 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h3><p>就绪状态：进程已获得除处理机外的所需资源，等待分配处理机资源；只要分配CPU就可执行。</p><p>执行状态：占用处理机资源；处于此状态的进程的数目小于等于CPU的数目。在没有其他进程可以执行时（如所有进程都在阻塞状态），通常会自动执行系统的idle进程（相当于空操作）。</p><p>阻塞状态：正在执行的进程，由于发生某种事件而暂时无法执行，便放弃处理机处于暂停状态</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">graph TB;</span><br><span class="line">A[执行状态] --&gt; |等待某个事件|B[阻塞状态]</span><br><span class="line">B --&gt; |事件发生|C[就绪状态]</span><br><span class="line">A --&gt; |时间片到|C</span><br><span class="line">C --&gt; |调度|A</span><br></pre></td></tr></tbody></table></figure><h3 id="进程控制块"><a href="#进程控制块" class="headerlink" title="进程控制块"></a>进程控制块</h3><p>系统为每个进程定义了一个数据结构：进程控制块PCB（Process Control Block）。</p><p>作用：</p><ul><li>进程创建、撤消；</li><li>进程唯一标志；</li></ul><p>进程控制块是进程管理和控制的最重要的数据结构，每一个进程均有一个PCB，在创建进程时，建立PCB，伴随进程运行的全过程，直到进程撤消而撤消</p><p>内容：</p><ul><li>进程标识符：唯一。可以是字符串，也可以是一个数字（Linux系统中是一个整型数）。在进程创建时由系统赋予。</li><li>程序和数据地址</li><li>当前状态：为了管理的方便，系统设计时会将相同的状态的进程组成一个队列，如就绪进程队列，等待进程则要根据等待的事件组成多个等待队列，如等待打印机队列、等待磁盘I/O完成队列等等</li><li>现场保留区</li><li>互斥和同步机制：用于实现进程间互斥、同步和通信所需的信号量等</li><li>进程通信机制</li><li>优先级：反映进程的紧迫程度，通常由用户指定和系统设置。</li><li>资源清单：列出所拥有的除CPU外的资源记录，如拥有的I/O设备、打开的文件列表等。</li><li>链接字：根据进程所处的执行状态，进程相应的PCB加入到不同队列中。PCB链接字指出该进程所在队列中下一个进程PCB的首地址。</li><li>家族关系 …</li></ul><h3 id="PCB组织方式"><a href="#PCB组织方式" class="headerlink" title="PCB组织方式"></a>PCB组织方式</h3><p>线性表：适用于系统中进程数目不多的情况</p><p>索引方式：是线性表的改进，系统按照进程的状态分别建立就绪索引表、阻塞索引表。</p><h3 id="辨析：进程上下文切换vs陷入内核"><a href="#辨析：进程上下文切换vs陷入内核" class="headerlink" title="辨析：进程上下文切换vs陷入内核"></a>辨析：进程上下文切换vs陷入内核</h3><table><thead><tr><th></th><th>进程上下文切换（Process Context Switch</th><th>陷入/退出内核（也称为模态切换，Mode Switch）</th></tr></thead><tbody><tr><td></td><td>通常由调度器执行，保存进程执行断点，切换内存映射(页表基址，flush TLB)</td><td>CPU状态改变，由中断、异常、Trap指令（系统调用）引起，需要保存执行现场（寄存器、堆栈等）</td></tr><tr><td></td><td>进程上下文切换过程一定会陷入内核</td><td>陷入内核不一定会导致进程切换</td></tr><tr><td></td><td></td><td>系统调用涉及到进程从用户态到内核态的切换（mode switch），这个时候涉及到的切换主要是寄存器上下文的切换，和通常所说的进程上下文切换不同，mode switch的消耗相对要小得多</td></tr></tbody></table><h2 id="线程概念的引入"><a href="#线程概念的引入" class="headerlink" title="线程概念的引入"></a>线程概念的引入</h2><h3 id="线程的提出"><a href="#线程的提出" class="headerlink" title="线程的提出"></a>线程的提出</h3><p>进程的不足：</p><ul><li>进程只能在一个时间处理一个任务，不能同时处理多个任务。</li><li>如果进程在执行时阻塞，整个进程都无法继续执行。</li></ul><p>需要一种新的实体，满足：</p><ul><li>实体之间可以并发地执行</li><li>实体之间共享相同的地址空间</li></ul><h3 id="进程与线程-1"><a href="#进程与线程-1" class="headerlink" title="进程与线程"></a>进程与线程</h3><p>实际上，进程包含了两个概念：资源拥有者和可执行单元。现代操作系统<strong>将资源拥有者称为进程，将可执行单元称为线程</strong>。线程将资源与计算分离，提高并发效率。</p><table><thead><tr><th></th><th>进程</th><th>线程</th></tr></thead><tbody><tr><td>基本概念</td><td>程序的一次执行</td><td>进程中的可执行单元，进程中的一个实体，可以与其他同进程的线程共享进程拥有的所有资源，同时也拥有栈、PC等私有资源</td></tr><tr><td>资源共享</td><td>资源分配的单位，拥有运行程序所需要的全部资源</td><td>CPU调度的单位，只拥有必不可少的少量资源</td></tr><tr><td>系统开销</td><td>创建/撤销/切换/同步进程的开销大</td><td>系统开销小</td></tr><tr><td>并发程度</td><td>低</td><td>高</td></tr></tbody></table><p>创建一个线程比一个进程快19-100倍。对于存在大量计算和大量I/O处理的应用，大幅度提高性能。在多CPU/多核CPU系统中更有优势。</p><h2 id="线程的实现方式"><a href="#线程的实现方式" class="headerlink" title="线程的实现方式"></a>线程的实现方式</h2><h3 id="用户级线程"><a href="#用户级线程" class="headerlink" title="用户级线程"></a>用户级线程</h3><p>线程在用户空间,通过library模拟的thread,不需要或仅需要极少的kernel支持。上下文切换比较快,因为不用更改page table等,使用起来较为轻便快速。提供操控视窗系统的较好的解决方案</p><p>优点：</p><ul><li>线程切换与内核无关</li><li>线程的调度由应用决定，容易进行优化。</li><li>可运行在任何操作系统上，只需要线程库的支持</li></ul><p>不足：</p><ul><li>很多系统调用会引起阻塞，内核会因此而阻塞所有相关的线程。</li><li>内核只能将处理器分配给进程，即使有多个处理器，也无法实现一个进程中的多个线程的并行执行</li></ul><h3 id="内核级线程"><a href="#内核级线程" class="headerlink" title="内核级线程"></a>内核级线程</h3><p>内核级线程就是kernel有好几个分身，一个分身可以处理一件事。这用来处理非同步事件很有用，kernel可以对每个非同步事件产生一个分身来处理。支持内核线程的操作系统内核称作多线程内核。</p><p>优点：</p><ul><li>内核可以在多个处理器上调度一个进程的多个线程实现同步并行执行</li><li>阻塞发生在线程级别</li><li>内核中的一些处理可以通过多线程实现</li></ul><p>缺点：</p><ul><li>一个进程中的线程切换需要内核参与，线程的切换涉及到两个模式的切换（进程-进程、线程-线程）</li><li>降低效率</li></ul><h3 id="用户级线程和内核级线程的比较"><a href="#用户级线程和内核级线程的比较" class="headerlink" title="用户级线程和内核级线程的比较"></a>用户级线程和内核级线程的比较</h3><ul><li>用户级线程执行系统调用指令时将导致其所属进程的执行被暂停，而内核级线程执行系统调用指令时，只导致该线程被暂停。</li><li>在只有用户级线程的系统内，CPU调度还是以进程为单位，处于运行状态的进程中的多个线程，由用户程序控制线程的轮换运行；在有内核级线程的系统内，CPU调度则以线程为单位，由OS的线程调度程序负责线程的调度</li><li>用户级线程的程序实体是运行在用户态下的程序，而内核级线程的程序实体则是可以运行在任何状态下的程序</li></ul><h3 id="混合实现方式"><a href="#混合实现方式" class="headerlink" title="混合实现方式"></a>混合实现方式</h3><p>线程在用户空间创建和管理，需要实现从用户空间的线程到内核空间线程（轻量级进程）的映射。</p><h1 id="同步与互斥"><a href="#同步与互斥" class="headerlink" title="同步与互斥"></a>同步与互斥</h1><h2 id="同步与互斥问题"><a href="#同步与互斥问题" class="headerlink" title="同步与互斥问题"></a>同步与互斥问题</h2><p>临界资源：一次仅允许一个进程访问的资源。</p><p>临界区：每个进程中访问临界资源的那段<strong>代码</strong>称为临界区。</p><p>进程互斥：某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。互斥无法限制访问者对资源的访问顺序，即访问是<strong>无序访问</strong>。</p><p>进程同步：指<strong>在互斥的基础上</strong>，通过其他机制实现访问者对资源的<strong>有序访问</strong>。在大多数情况下，同步已经实现了互斥，特别是所有对资源的写入的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源。</p><p>临界区管理应满足的条件：</p><ul><li><strong>空闲让进</strong>：临界资源处于空闲状态，允许进程进入临界区。临界区内仅有一个进程运行。</li><li><strong>忙则等待</strong>：临界区有正在执行的进程，所有其他进程则不可以进入临界区。</li><li><strong>有限等待</strong>：对要求访问临界区的进程，应保证在有限时间内进入自己的临界区，避免死等。</li><li><strong>让权等待</strong>：当进程（长时间）不能进入自己的临界区时，应立即释放处理机，尽量避免忙等。</li></ul><h2 id="基于忙等待的互斥方法"><a href="#基于忙等待的互斥方法" class="headerlink" title="基于忙等待的互斥方法"></a>基于忙等待的互斥方法</h2><h3 id="软件方法"><a href="#软件方法" class="headerlink" title="软件方法"></a>软件方法</h3><h4 id="Dekker算法"><a href="#Dekker算法" class="headerlink" title="Dekker算法"></a>Dekker算法</h4><p>第一个不需要严格轮换的互斥算法。</p><p>引入变量turn，以便在竞争时选出进入临界区的进程</p><p>pturn/qturn表示“我”想进入临界区，turn表示该哪个进程进入临界区了（轮流进）</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">P:</span><br><span class="line">......</span><br><span class="line">pturn = true;//I want to enter</span><br><span class="line">while(qturn) {// other wants to enter</span><br><span class="line">if (turn == 1) {//it's not my turn</span><br><span class="line">pturn = false;//I (temporarily) don't want to enter</span><br><span class="line">while(turn == 1); // The other is automatically allowed to enter</span><br><span class="line">// other don't want to enter</span><br><span class="line">pturn = true;// I want to enter</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">// other doesn't want to enter</span><br><span class="line">//临界区</span><br><span class="line">turn = 1;// it's the other's turn</span><br><span class="line">pturn = false;// I don't want to enter</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">Q:</span><br><span class="line">......</span><br><span class="line">qturn = true;</span><br><span class="line">while(pturn) {</span><br><span class="line">if (turn == 0) {</span><br><span class="line">pturn = false;</span><br><span class="line">while(turn == 0);</span><br><span class="line">qturn = true;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">//临界区</span><br><span class="line">turn = 0;</span><br><span class="line">qturn = false;</span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure><h4 id="Peterson算法"><a href="#Peterson算法" class="headerlink" title="Peterson算法"></a>Peterson算法</h4><p>解决了互斥访问的问题，而且克服了强制轮流法的缺点，可以完全正常地工作。</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">#define FALSE O</span><br><span class="line">#define TRUE 1</span><br><span class="line">#define N 2 //进程的个数</span><br><span class="line">int turn; //轮到谁?</span><br><span class="line">int interested[N]; //兴趣数组，初始值均为FALSE</span><br><span class="line">void enter_region(int process)//process=0或1</span><br><span class="line">{</span><br><span class="line">//另外一个进程的进程号</span><br><span class="line">    int other;</span><br><span class="line">    other = 1 - process;</span><br><span class="line">    //表明本进程感兴趣</span><br><span class="line">    interested[process]=TRUE;</span><br><span class="line">    turn = other;//设置标志位</span><br><span class="line">    //other也可以换成process，这其实是两种方式：谦让式vs争夺式</span><br><span class="line">    while(turn == other &amp;&amp; interested[other]==TRUE);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">void leave_region(int process) {</span><br><span class="line">interested[process] = FALSE;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="Bakery-Algorithm-面包店算法"><a href="#Bakery-Algorithm-面包店算法" class="headerlink" title="Bakery Algorithm(面包店算法)"></a>Bakery Algorithm(面包店算法)</h4><ul><li>在进入临界区之前，进程收到一个数字，具有最小数字的进程被允许进入临界区</li><li>如果进程 Pi 和 Pj 接收到相同数字, if i &lt; j, then Pi 进入临界区；否则，Pj 进入临界区</li><li>产生的数字总是递增的，例如：1,2,3,3,3,3,4,5…</li></ul><h3 id="硬件方法"><a href="#硬件方法" class="headerlink" title="硬件方法"></a>硬件方法</h3><h4 id="中断屏蔽方法"><a href="#中断屏蔽方法" class="headerlink" title="中断屏蔽方法"></a>中断屏蔽方法</h4><p>使用“开关中断”指令。执行“关中断”指令，进入临界区操作；退出临界区之前，执行“开中断”指令。</p><p>优点：简单</p><p>缺点：</p><p>不适用于多CPU系统</p><p>往往会带来很大的性能损失。很多日常任务都是靠中断机制触发的，比如时钟，如果屏蔽中断，会影响时钟和系统效率。</p><p>用户进程的使用可能很危险，例如：关中断之后，不再打开中断，会导致整个系统无法继续运行。所以，使用范围一般为内核进程（少量使用）。</p><h4 id="使用test-and-set指令"><a href="#使用test-and-set指令" class="headerlink" title="使用test and set指令"></a>使用test and set指令</h4><p>TS（test-and-set ）是一种不可中断的基本原语（指令）。它会把“1”写到某个内存位置并传回其旧值。在多进程可同时访问内存的情况下，如果一个进程正在执行TS指令，在它执行完成前，其它的进程不可以执行TS指令。</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">TestAndSet(boolean_ref *lock) { </span><br><span class="line">    boolean initial = *lock; </span><br><span class="line">    *lock = true; </span><br><span class="line">    return initial; </span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="自旋锁Spinlocks"><a href="#自旋锁Spinlocks" class="headerlink" title="自旋锁Spinlocks"></a>自旋锁Spinlocks</h4><p>利用test_and_set硬件原语提供互斥支持。通过对总线的锁定实现对某个内存位置的原子读与更新。</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">acquire(int *lock) {</span><br><span class="line">    while(test_and_set(lock) == 1)</span><br><span class="line">    /* do nothing */;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">release(int *lock) {</span><br><span class="line">*lock = 0;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>当lock == 0时，线程执行完acquire获得锁，lock被修改为1。其他线程想获得锁时，执行acquire会被卡在while循环里，直到获得线程的锁执行完释放锁。</p><h3 id="几个算法的共性问题"><a href="#几个算法的共性问题" class="headerlink" title="几个算法的共性问题"></a>几个算法的共性问题</h3><p>无论是软件还是硬件方法，解法都是正确的，但它们都有一个特点：当一个进程想进入临界区时，先检查是否允许进入，若不允许，则该进程将原地等待，直到允许为止。造成：</p><ul><li>忙等待，浪费CPU时间</li><li>优先级反转：低优先级进程先进入临界区，高优先级进程一直忙等。如果使用用户级线程，低优先级线程不会被高优先级线程抢占(进入临界区一般需要系统调用，其他线程也同时会被阻塞)，因为抢占发生在进程级别。但是对于内核级线程的实现，这个是可能发生的。</li></ul><h4 id="补充：优先级反转"><a href="#补充：优先级反转" class="headerlink" title="补充：优先级反转"></a>补充：优先级反转</h4><p>是指一个低优先级的任务持有一个被高优先级任务所需要的共享资源。高优先任务由于因资源缺乏而处于受阻状态，一直等到低优先级任务释放资源为止。而低优先级获得的CPU时间少，如果此时有优先级处于两者之间的任务，并且不需要那个共享资源，则该中优先级的任务反而超过这两个任务而获得CPU时间。如果高优先级等待资源时不是阻塞等待，而是忙循环，则可能永远无法获得资源，因为此时低优先级进程无法与高优先级进程争夺CPU时间，从而无法执行，进而无法释放资源，造成的后果就是高优先级任务无法获得资源而继续推进。</p><ol><li>优先级置顶：给临界区一个高优先级，进入临界区的进程都将获得这个高优先级</li><li>优先级继承：当一个高优先级进程等待一个低优先级进程持有的资源时，低优先级进程将暂时获得高优先级进程的优先级别，在释放共享资源后，低优先级进程回到原来的优先级别</li><li>中断屏蔽：通过禁止中断来保护临界区，采用此种策略的系统只有两种优先级——可抢占优先级和中断禁止优先级。前者为一般进程运行时的优先级，后者为运行于临界区的优先级。</li></ol><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>将忙等变为阻塞。见下节</p><h2 id="基于信号量的同步方法"><a href="#基于信号量的同步方法" class="headerlink" title="基于信号量的同步方法"></a>基于信号量的同步方法</h2><p>同步中，进程经常需要等待某个条件的发生，如果使用忙等待的解决方案，势必浪费大量CPU时间。</p><p>解决方法：将忙等变为阻塞，可使用两条进程间的通信原语：Sleep和Wakeup。Sleep原语将引起调用进程的阻塞，直到另外一个进程用Wakeup原语将其唤醒。很明显，wakeup原语的调用需要一个参数——被唤醒的进程ID。</p><h3 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h3><p>是Dijkstra提出，它使用一个整型变量来累计唤醒次数，供以后使用。在他的建议中引入了一个新的变量类型，称作信号量（semaphore）。建议设立两种操作，P（也叫semWait）、V（也叫semSignal）。PV操作属于进程的低级通信。</p><p>信号量是一类特殊的变量，程序对其访问都是原子操作，且只允许对它进行P(信号变量)和V(信号变量)操作。</p><h3 id="信号量的定义"><a href="#信号量的定义" class="headerlink" title="信号量的定义"></a>信号量的定义</h3><p>一个确定的二元组(s, q)，其中s是一个具有非负初值的整型变量，q是一个初始状态为空的队列. 当发出P操作时：</p><ul><li>s为正，则该值等于可立即执行的进程的数量；s &lt;= 0，那么发出P操作后的进程被阻塞（即发出P操作后s先减一，此时若s&lt;0，则该进程被阻塞），│s │是被阻塞的进程数。</li><li>q是一个初始状态为空的队列，当有进程被阻塞时就会进入此队列。</li></ul><h3 id="信号量的分类"><a href="#信号量的分类" class="headerlink" title="信号量的分类"></a>信号量的分类</h3><p>二元信号量和一般信号量</p><ul><li>二元信号量：取值仅为“0”或“1”，主要用作实现互斥；</li><li>一般信号量：初值为<strong>可用物理资源的总数</strong>，用于进程间的协作同步问题。</li></ul><p>强信号量和弱信号量</p><ul><li><p>强信号量：进程从被阻塞队列释放时采取FIFO</p><p>–不会出现“饥饿”（某个进程长时间被阻塞）</p></li><li><p>弱信号量：没有规定进程从阻塞队列中移除顺序</p><p>–可能出现“饥饿“</p></li></ul><h3 id="信号量的操作"><a href="#信号量的操作" class="headerlink" title="信号量的操作"></a>信号量的操作</h3><ul><li>一个信号量可能被初始化为一个非负整数.</li><li>semWait 操作（P操作）使信号量减1。若值为负，则执行semWait的进程被阻塞。否则进程继续执行。</li><li>semSignal操作（V操作）使信号量加1。若值小于或等于零，则被semWait操作阻塞的进程被解除阻塞。</li></ul><h3 id="信号量在并发中的典型应用"><a href="#信号量在并发中的典型应用" class="headerlink" title="信号量在并发中的典型应用"></a>信号量在并发中的典型应用</h3><p>互斥：可以用初始值为1的信号量实现。一个进程在进入临界区之前执行P操作，退出临界区后执行V操作。这是实现临界区资源互斥使用的一个二元信号量。</p><p>有限并发：可以用初始值为c的信号量实现n（1&lt;=n&lt;=c)个进程的并发执行一个函数或一个资源。</p><p>进程同步：指当⼀个进程Pi想要执⾏⼀个ai操作时，它只在进程Pj执⾏完aj后，才会执⾏ai操作。可以⽤信号量如下实现：将信号量初始为0，Pi执⾏ai操作前执⾏⼀个semWait操作；⽽Pj执⾏aj操作后，执⾏⼀个semSignal操作。</p><p>屏障Barriers：只有当该线程/进程组中所有线程到达屏障点（可称之为同步点）时，整个程序才得以继续执行。信号量实现如下：n个进程就有n个信号量，屏障进入前对除本进程对应信号量以外的其他进程进行V操作，然后对本进程对应信号量进行P操作。</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">//两个进程</span><br><span class="line">semaphore s0 = 0</span><br><span class="line">semaphore s1 = 0</span><br><span class="line">P0:</span><br><span class="line">//code before barrier</span><br><span class="line">V(s0)</span><br><span class="line">P(s1)</span><br><span class="line">//code after barrier</span><br><span class="line"></span><br><span class="line">P1:</span><br><span class="line">//code before barrier</span><br><span class="line">V(s1)</span><br><span class="line">P(s0)</span><br><span class="line">//code after barrier</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">//多个进程</span><br><span class="line">n = the number of threads</span><br><span class="line">count = 0//到达汇合点的线程数</span><br><span class="line">semaphore mutex = 1//互斥对count的访问</span><br><span class="line">semaphore barrier = 0 //线程到达之前都是0或者负值，到达后取正值</span><br><span class="line"></span><br><span class="line">P:</span><br><span class="line">P(mutex)</span><br><span class="line">count++</span><br><span class="line">if count == n:</span><br><span class="line">V(barrier) //唤醒一个线程</span><br><span class="line">V(mutex)</span><br><span class="line"></span><br><span class="line">P(barrier)</span><br><span class="line">V(barrier)//一旦线程被唤醒，有责任唤醒下一个线程</span><br><span class="line"></span><br><span class="line">P(mutex)</span><br><span class="line">count--</span><br><span class="line">if count == 0:</span><br><span class="line">P(barrier)//“关门”</span><br><span class="line">V(mutex)</span><br></pre></td></tr></tbody></table></figure><h3 id="P、V操作的优缺点"><a href="#P、V操作的优缺点" class="headerlink" title="P、V操作的优缺点"></a>P、V操作的优缺点</h3><p>优点：简单，而且表达能力强（用P、V操作可以<strong>解决任何同步、互斥问题</strong>）</p><p>缺点：不够安全，PV操作使用不当会出现死锁，遇到复杂同步互斥问题时实现复杂。</p><h2 id="基于管程的同步与互斥"><a href="#基于管程的同步与互斥" class="headerlink" title="基于管程的同步与互斥"></a>基于管程的同步与互斥</h2><h3 id="管程的定义和组成"><a href="#管程的定义和组成" class="headerlink" title="管程的定义和组成"></a>管程的定义和组成</h3><p>管程（Monitor）是在程序设计语言当中引入的一种高级同步机制。</p><p>一个管程定义了一个数据结构和能（在该数据结构上）被并发进程所执行的一组操作，这组操作能同步进程和改变管程中的数据。（挺像Java里一个类的组成）</p><p>管程由四部分组成：</p><ol><li>管程的名称</li><li>局部于管程内部的共享数据结构（变量）说明</li><li>对该数据结构进行操作的一组互斥执行的过程（可以理解为函数 / Java里被synchronized的方法）</li><li>对局部于管程内部的共享数据设置初始值的语句</li></ol><h3 id="条件变量与信号量的区别"><a href="#条件变量与信号量的区别" class="headerlink" title="条件变量与信号量的区别"></a>条件变量与信号量的区别</h3><table><thead><tr><th>条件变量</th><th>信号量</th></tr></thead><tbody><tr><td>值不可增减</td><td>可增减</td></tr><tr><td>wait操作一定会阻塞当前进程</td><td>P操作只有当信号量的值小于0时才会阻塞</td></tr><tr><td>如果没有等待的进程，signal将会丢失</td><td>V操作增加了信号量的值，不会丢失</td></tr><tr><td>访问条件变量必须拥有管程的锁</td><td></td></tr></tbody></table><h2 id="进程通信（Interprocess-Communication）的主要方法"><a href="#进程通信（Interprocess-Communication）的主要方法" class="headerlink" title="进程通信（Interprocess Communication）的主要方法"></a>进程通信（Interprocess Communication）的主要方法</h2><ul><li><p>低级通信：只能传递状态和整数值（控制信息），包括进程互斥和同步所采用的信号量和管程机制。</p><p>缺点：传送信息量小；编程复杂</p></li><li><p>高级通信：适用于分布式系统、基于共享内存的多处理机系统、单处理机系统。</p><p>主要包括三类：管道、共享内存、消息系统</p></li></ul><h3 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h3><h4 id="无名管道（Pipe）"><a href="#无名管道（Pipe）" class="headerlink" title="无名管道（Pipe）"></a>无名管道（Pipe）</h4><ul><li>管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立起两个管道；</li><li>只能用于父子进程或者兄弟进程之间（具有亲缘关系的进程）；</li><li>单独构成一种独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但它不是普通的文件，它不属于某种文件系统，而是单独构成一种文件系统，并且<strong>只存在于内存</strong>中。</li><li>数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，并且每次都是从缓冲区的头部读出数据。</li></ul><h4 id="有名管道（Named-Pipe，又称FIFO）"><a href="#有名管道（Named-Pipe，又称FIFO）" class="headerlink" title="有名管道（Named Pipe，又称FIFO）"></a>有名管道（Named Pipe，又称FIFO）</h4><p>FIFO不同于无名管道之处在于它提供一个路径名与之关联，以FIFO的文件形式存在于文件系统中。这样，即使与FIFO的创建进程不存在亲缘关系的进程，只要可以访问该路径，就能够彼此通过FIFO相互通信（能够访问该路径的进程以及FIFO的创建进程之间），因此，通过FIFO不相关的进程也能交换数据。</p><p>注意：FIFO严格遵循先进先出（first in first out），对管道及FIFO的读总是从开始处返回数据，对它们的写则把数据添加到末尾</p><h3 id="消息传递"><a href="#消息传递" class="headerlink" title="消息传递"></a>消息传递</h3><ul><li>管程：过度依赖编译器；适用于单机环境。</li><li>消息传递——两个通信原语（OS系统调用）<ul><li>send (destination, &amp;message)</li><li>receive(source, &amp;message)</li></ul></li></ul><p>调用方式</p><ul><li>阻塞调用</li><li>非阻塞调用</li></ul><p>主要问题：</p><ul><li>解决消息丢失、延迟问题（TCP协议）</li><li>编址问题：mailbox</li></ul><h3 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h3><p>共享内存是最有用的进程间通信方式，也是最快的IPC形式（因为它避免了其它形式的IPC必须执行的开销巨大的缓冲复制）。</p><p>两个不同进程A、B共享内存的意义是，同一块物理内存被映射到进程A、B各自的进程地址空间。当多个进程共享同一块内存区域，由于共享内存可以同时读但不能同时写，则需要同步机制约束（互斥锁和信号量都可以）。</p><p>共享内存通信的效率高（因为进程可以直接读写内存）。</p><p>进程之间在共享内存时，保持共享区域直到通信完毕。</p><h2 id="经典同步与互斥问题"><a href="#经典同步与互斥问题" class="headerlink" title="经典同步与互斥问题"></a>经典同步与互斥问题</h2><h3 id="生产者-消费者问题"><a href="#生产者-消费者问题" class="headerlink" title="生产者-消费者问题"></a>生产者-消费者问题</h3><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">semaphore empty = N//空闲数量</span><br><span class="line">semaphore full = 0//产品数量</span><br><span class="line">semaphore mutex = 1</span><br><span class="line">producer() {</span><br><span class="line">while(true) {</span><br><span class="line">P(empty);</span><br><span class="line">P(mutex);</span><br><span class="line">...</span><br><span class="line">V(mutex);</span><br><span class="line">V(full)</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">consumer() {</span><br><span class="line">P(full)</span><br><span class="line">P(mutex)</span><br><span class="line">...</span><br><span class="line">V(mutex)</span><br><span class="line">V(empty)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="读者-写者"><a href="#读者-写者" class="headerlink" title="读者-写者"></a>读者-写者</h3><p>读者优先</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">int readcount=0;//正在读的进程数</span><br><span class="line">semaphore rmutex=1;//用户readcount的互斥访问</span><br><span class="line">semaphore mutex=1;//用户数据访问的互斥</span><br><span class="line"></span><br><span class="line">read() {</span><br><span class="line">P(rmutex)</span><br><span class="line">if readcount == 0 then P(mutex)</span><br><span class="line">readcount++</span><br><span class="line">V(rmutex)</span><br><span class="line">...</span><br><span class="line">P(rmutex)</span><br><span class="line">readcount--</span><br><span class="line">if readcount == 0 then V(mutex)</span><br><span class="line">V(rmutex)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">write(){</span><br><span class="line">P(mutex)</span><br><span class="line">...</span><br><span class="line">V(mutex)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>读写公平(先来先服务)</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">int readcount=0;//正在读的进程数</span><br><span class="line">semaphore rmutex=1;//用户readcount的互斥访问</span><br><span class="line">semaphore mutex=1;//用户数据访问的互斥</span><br><span class="line">semaphore rwmutex = 1;</span><br><span class="line">read() {</span><br><span class="line">P(rwmutex)</span><br><span class="line">P(rmutex)</span><br><span class="line">if readcount == 0 then P(mutex)</span><br><span class="line">readcount++</span><br><span class="line">V(rmutex)</span><br><span class="line">V(rwmutex)</span><br><span class="line">...</span><br><span class="line">P(rmutex)</span><br><span class="line">readcount--</span><br><span class="line">if readcount == 0 then V(mutex)</span><br><span class="line">V(rmutex)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">write(){</span><br><span class="line">P(rwmutex)</span><br><span class="line">P(mutex)</span><br><span class="line">    V(rwmutex)</span><br><span class="line">...</span><br><span class="line">V(mutex)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>写者优先(TODO</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">readSwitch = Lightswitch() </span><br><span class="line">writeSwitch = Lightswitch() </span><br><span class="line">noReaders = Semaphore(1) </span><br><span class="line">noWriters = Semaphore(1)</span><br><span class="line"></span><br><span class="line">Reader：</span><br><span class="line">noReaders.wait() </span><br><span class="line">readSwitch.lock(noWriters)</span><br><span class="line">noReaders.signal()</span><br><span class="line"># critical section for readers</span><br><span class="line">readSwitch.unlock(noWriters)</span><br><span class="line"></span><br><span class="line">Writer：</span><br><span class="line">writeSwitch.lock(noReaders) </span><br><span class="line">noWriters.wait() </span><br><span class="line"># critical section for writers </span><br><span class="line">noWriters.signal() </span><br><span class="line">writeSwitch.unlock(noReaders) </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>最初信号量都是解锁态。如果reader在临界区，会给noWriter上锁。但是不会给noReader上锁。如果这时候writer到来，会给noReader加锁，会让后续读者排队在noReader。当最后一个读者离开，他会signal noWriter，这时写者可以进入。</p><p>当写者进入临界区，他同时拿着noreader和nowriter两个锁。一方面，其他读者和写者不能同时访问临界区。另一方面，writeSwitch 允许其他写者通过，并在noWriter等待。但是读者只能在noReader等待。这样，所有排队的写者能够通过临界区，而不需要signal noreader。当最后一个写者离开，noreader才解锁。写者才能进入。</p><h3 id="哲学家就餐"><a href="#哲学家就餐" class="headerlink" title="哲学家就餐"></a>哲学家就餐</h3><p>求解思路</p><ol><li>至多只允许四个哲学家同时（尝试）进餐,以保证至少有一个哲学家能够进餐,最终总会释放出他所使用过的两支筷子,从而可使更多的哲学家进餐。设置信号量room=4。（破除资源互斥）</li><li>对筷子进行编号，每个哲学家按编号从低到高拿起筷子。或者对哲学家编号，奇数号哲学家先拿左，再拿右；偶数号相反。（破除循环等待）</li><li>同时拿起两根筷子，否则不拿起。（破除保持等待）</li></ol><h1 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="调度的类型"><a href="#调度的类型" class="headerlink" title="调度的类型"></a>调度的类型</h3><ul><li>高级调度：又称为“宏观调度”、“作业调度”。时间单位通常是分钟、小时、天</li><li>中级调度：又称为“内外存交换”</li><li>低级调度：又称为“微观调度”、“进程或线程调度”。时间单位通常是毫秒</li></ul><h3 id="面向用户的调度性能准则"><a href="#面向用户的调度性能准则" class="headerlink" title="面向用户的调度性能准则"></a>面向用户的调度性能准则</h3><ul><li><p>周转时间（批处理系统）：作业从提交到完成所经历的时间</p><p>带权周转时间 = 周转时间 / 服务时间（即执行时间）</p></li><li><p>响应时间（分时系统）：用户输入一个请求到系统给出首次响应的时间</p></li><li><p>截止时间（实时系统）：开始截止时间和完成截止时间，与周转时间有些相似</p></li><li><p>优先级</p></li><li><p>公平性</p></li></ul><h3 id="面向系统的调度性能准则"><a href="#面向系统的调度性能准则" class="headerlink" title="面向系统的调度性能准则"></a>面向系统的调度性能准则</h3><ul><li>吞吐量（批处理系统）：单位时间完成的作业数</li><li>处理机利用率（大中型主机）</li><li>各种资源的均衡利用（大中型主机）：如CPU繁忙的作业和I/O繁忙的作业搭配</li></ul><h3 id="调度算法本身的调度性能准则"><a href="#调度算法本身的调度性能准则" class="headerlink" title="调度算法本身的调度性能准则"></a>调度算法本身的调度性能准则</h3><ul><li>易于实现</li><li>执行/开销比小</li></ul><h2 id="设计调度算法要考虑的问题"><a href="#设计调度算法要考虑的问题" class="headerlink" title="设计调度算法要考虑的问题"></a>设计调度算法要考虑的问题</h2><ol><li><p>进程优先级（数）</p><p>优先级和优先数是不同的，优先级表现了进程的重要性和紧迫性，优先数实际上是一个数值，反映了某个优先级。</p><p>有静态、动态优先级之分，主要看优先级会不会在运行过程中改变。</p></li><li><p>进程优先级就绪队列的组织</p></li><li><p>抢占式调度与非抢占式调度</p></li><li><p>进程的分类</p><ol><li><p>I/O Bound（密集型）与CPU Bound</p></li><li><p>批处理进程、交互式进程、实时进程</p></li></ol></li><li><p>时间片</p></li></ol><h2 id="批处理系统的调度算法"><a href="#批处理系统的调度算法" class="headerlink" title="批处理系统的调度算法"></a>批处理系统的调度算法</h2><h3 id="先来先服务（FCFS）"><a href="#先来先服务（FCFS）" class="headerlink" title="先来先服务（FCFS）"></a>先来先服务（FCFS）</h3><p>特点：有利于长作业，不利于短作业；有利于CPU繁忙的作业，不利于I/O繁忙的作业</p><h3 id="最短作业优先（SJF）-短进程优先（SPN）"><a href="#最短作业优先（SJF）-短进程优先（SPN）" class="headerlink" title="最短作业优先（SJF）/短进程优先（SPN）"></a>最短作业优先（SJF）/短进程优先（SPN）</h3><p>这是对FCFS算法的改进，其目标是减少平均周转时间。做法是对预计执行时间短的作业（进程）优先分派处理机。<strong>通常后来的短作业不抢占正在执行的作业</strong>。</p><p>优点：与FCFS相比改善了平均（&amp;带权）周转时间、作业的等待时间；提高系统的吞吐量。</p><p>缺点：长作业可能长时间得不到执行；未能依据作业的紧迫程度来划分执行的优先级；难以准确估计作业的执行时间，从而影响调度性能。</p><h3 id="最短剩余时间优先（SRTF）"><a href="#最短剩余时间优先（SRTF）" class="headerlink" title="最短剩余时间优先（SRTF）"></a>最短剩余时间优先（SRTF）</h3><p>上面的方法SJF修改条件“通常后来的短作业不抢占正在执行的作业”为<strong>抢占式</strong></p><h3 id="最高响应比优先（HRRF）"><a href="#最高响应比优先（HRRF）" class="headerlink" title="最高响应比优先（HRRF）"></a>最高响应比优先（HRRF）</h3><p>实际上是FCFS算法和SJF算法的折衷既考虑作业等待时间，又考虑作业的运行时间，既照顾短作业又不使长作业的等待时间过长，改善了调度性能。</p><p>在每次选择作业投入运行时，先计算后备作业队列中每个作业的响应比RP，然后选择其值最大的作业投入运行。<br>$$<br>RP = \frac{作业已等待时间 + 作业的服务时间}{作业的服务时间} = 1 + \frac{作业已等待时间}{作业的服务时间}<br>$$<br>响应比的计算时机：每当调度一个作业运行时，都要计算后备作业队列中每个作业的响应比，选择响应比最高者投入运行。</p><h2 id="交互式系统的调度算法"><a href="#交互式系统的调度算法" class="headerlink" title="交互式系统的调度算法"></a>交互式系统的调度算法</h2><h3 id="时间片轮转-RR：Round-Robin）"><a href="#时间片轮转-RR：Round-Robin）" class="headerlink" title="时间片轮转(RR：Round Robin）"></a>时间片轮转(RR：Round Robin）</h3><h3 id="多级队列（MQ：Multi-level-Queue）"><a href="#多级队列（MQ：Multi-level-Queue）" class="headerlink" title="多级队列（MQ：Multi-level Queue）"></a>多级队列（MQ：Multi-level Queue）</h3><p>本算法引入多个就绪队列，通过各队列的区别对待，达到综合的调度目标；</p><p>不同队列可有不同的优先级、时间片长度、调度策略等；在运行过程中还可改变进程所在队列。如：系统进程、用户交互进程、批处理进程等</p><h3 id="多级反馈队列（MFQ：-Multi-level-Feedback-Queue-）"><a href="#多级反馈队列（MFQ：-Multi-level-Feedback-Queue-）" class="headerlink" title="多级反馈队列（MFQ： Multi-level Feedback Queue ）"></a>多级反馈队列（MFQ： Multi-level Feedback Queue ）</h3><p>是时间片轮转算法和优先级算法的综合和发展</p><ol><li>设置多个就绪队列，分别赋予不同的优先级（如逐级降低），队列1的优先级最高。每个队列执行时间片的长度也不同，规定<strong>优先级越低则时间片越长</strong>（如逐级加倍）。</li><li>新进程进入内存后，先投入队列1的末尾，按FCFS算法调度；若按队列1一个时间片未能执行完，则降低投入到队列2的末尾，同样按FCFS算法调度；如此下去，降低到最后的队列，则按“时间片轮转”算法调度直到完成。</li><li>仅当较高优先级的队列为空，才调度较低优先级的队列中的进程执行。如果进程执行时有新进程进入较高优先级的队列，则抢先执行新进程，并把被抢先的进程投入原队列的末尾</li><li>如果进程执行时有新进程进入较高优先级的队列，则<strong>抢先</strong>执行新进程，并把被抢先的进程投入<strong>原</strong>队列的末尾</li></ol><p><strong>优先级反转问题</strong></p><h2 id="实时系统的调度算法"><a href="#实时系统的调度算法" class="headerlink" title="实时系统的调度算法"></a>实时系统的调度算法</h2><p>实时系统是一种时间起着主导作用的系统。分为硬实时（绝对满足截止时间要求，如汽车和飞机的控制系统）和软实时（可以偶尔不满足）</p><h3 id="实时调度的前提条件"><a href="#实时调度的前提条件" class="headerlink" title="实时调度的前提条件"></a>实时调度的前提条件</h3><ul><li>任务集（S）是已知的；</li><li>所有任务都是<strong>周期性</strong>（T）的，</li><li>必须在限定的时限D内完成；</li><li>任务之间都是独立的，每个任务不依赖于其他任务；</li><li>每个任务的运行时间（c）是不变的；</li><li>调度, 任务切换的时间忽略不计</li></ul><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul><li><p>静态表调度（Static table-driven scheduling）：通过对所有周期性任务的分析预测，事先确定一个固定的调度方案。</p></li><li><p>单调速率调度（RMS：Rate Monotonic Scheduling） 静态最优调度算法：任务的周期越小，其优先级越高，优先级最高的任务最先被调度；如果两个任务的优先级一样，当调度它们时，RM算法将随机选择一个调度。静态、<strong>抢先式调度</strong>。前提<br>  $$<br>  \sum_{i=1}^n \frac{C_i}{T_i} &lt;= n(\sqrt[n]{2} - 1) \to ln2 \approx 0.69 (n \to \infty)<br>  $$</p></li><li><p>最早截止时间优先算法（EDF：Earliest Deadline First）：任务的绝对截止时间越早，其优先级越高，优先级最高的任务最先被调度。如果两个任务的优先级一样，当调度它们时，RM算法将随机选择一个调度。<strong>抢占式</strong>。任务集可调度的充分必要条件(C是执行时间，T是周期)：<br>  $$<br>  \sum_{i=1}^n \frac{C_i}{T_i} &lt;= 1<br>  $$</p></li><li><p>最低松弛度优先算法（LLF：Least Laxity First）：根据任务紧急/松弛程度，来确定任务的优先级，使优先级高的优先执行。</p><p>  $$<br>  松弛度Laxity = 任务截止时间 - 本身剩余运行时间 - 当前时间<br>  $$</p><p>  调度时机：有任务执行完时，或有进程Laxity为0时（直接抢占）</p></li></ul><h2 id="多处理机调度"><a href="#多处理机调度" class="headerlink" title="多处理机调度"></a>多处理机调度</h2><p>与单处理机调度的区别：</p><ul><li>注重整体运行效率，而不是个别处理机的利用率</li><li>更多样的调度算法</li><li>多处理访问os数据结构时的互斥（对于共享内存的系统）</li></ul><p>调度单位广泛采用线程</p><ol><li><p>非对称式多处理系统(AMP：Asymmetric Multi-Processor)：指多处理器系统中各个处理器的地位不同。</p><ul><li><p>主－从处理机系统，由主处理机管理一个公共就绪队列，并分派进程给从处理机执行。</p></li><li><p>各个处理机有固定分工，如执行OS的系统功能，I/O处理，应用程序。</p></li><li><p>有潜在的不可靠性（主机故障造成系统崩溃）。</p></li></ul></li><li><p>对称式多处理系统（SMP）</p><ul><li><p>集中控制</p><ul><li><p>静态调度：每个CPU设立一个就绪队列，进程从开始执行到完成，都在同一个CPU上。</p><p>优点：调度算法开销小。</p><p>缺点：容易出现忙闲不均</p></li><li><p>动态调度：各个CPU采用一个公共就绪队列，<strong>队首</strong>进程每次分派到当前空闲的CPU上执行。可防止系统中多个处理器忙闲不均。</p></li></ul></li><li><p>分散控制</p><ul><li>自调度：所有CPU采用一个公共就绪队列，每个处理机都可以从队列中<strong>选择适当进程</strong>来执行。需要对就绪队列的数据结构进行<strong>互斥访问控制</strong>。最常用的算法，实现时易于移植，采用单处理机的调度技术</li></ul></li></ul></li></ol><h1 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h1><h2 id="死锁的概念"><a href="#死锁的概念" class="headerlink" title="死锁的概念"></a>死锁的概念</h2><h3 id="死锁发生的四个必要条件"><a href="#死锁发生的四个必要条件" class="headerlink" title="死锁发生的四个必要条件"></a>死锁发生的四个必要条件</h3><ol><li>互斥条件：在一段时间内某资源只由一个进程占用</li><li>请求和保持条件：进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放</li><li>不可剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放</li><li>环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源</li></ol><h3 id="活锁livelock"><a href="#活锁livelock" class="headerlink" title="活锁livelock"></a>活锁livelock</h3><p>是指任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。</p><p>活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，即所谓的“活” ， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。避免活锁的简单方法是采用先来先服务的策略</p><h3 id="饥饿（starvation-："><a href="#饥饿（starvation-：" class="headerlink" title="饥饿（starvation)："></a>饥饿（starvation)：</h3><p>某些进程可能由于资源分配策略的不公平导致长时间等待。当等待时间给进程推进和响应带来明显影响时，称发生了进程饥饿，当饥饿到一定程度的进程所赋予的任务即使完成也不再具有实际意义时称该进程被饿死</p><h2 id="处理死锁的基本方法"><a href="#处理死锁的基本方法" class="headerlink" title="处理死锁的基本方法"></a>处理死锁的基本方法</h2><ul><li><p>不允许死锁发生</p><p>预防死锁（静态）：防患于未然，破坏死锁的产生条件</p><p>避免死锁（动态）：在资源分配前进行判断</p></li><li><p>允许死锁发生</p><p>检测与接触死锁</p><p>无所作为：鸵鸟算法</p></li></ul><h3 id="避免死锁"><a href="#避免死锁" class="headerlink" title="避免死锁"></a>避免死锁</h3><h4 id="安全序列"><a href="#安全序列" class="headerlink" title="安全序列"></a>安全序列</h4><p>安全序列的定义：一个序列{P1，P2，…，Pn}安全的，是指若对于每一个进程Pi，它需要的资源可以被系统中当前可用资源加上所有进程Pj（j &lt; i）当前占有资源之和所满足，则{P1，P2，…，Pn}为一个安全序列。</p><p>如果系统不存在这样一个安全序列，则系统是不安全的。</p><p>系统进入不安全状态也未必会产生死锁。产生死锁后系统一定处于不安全状态</p><h4 id="银行家算法-避免死锁算法"><a href="#银行家算法-避免死锁算法" class="headerlink" title="银行家算法(避免死锁算法)"></a>银行家算法(避免死锁算法)</h4><p>设n为进程数量，m为资源类型数量</p><ul><li>可利用资源向量Available：m维向量</li><li>最大需求矩阵Max：n x m 矩阵</li><li>分配矩阵Allocation：n x m矩阵</li><li>需求矩阵Need：n x m矩阵</li></ul><p>$$<br>Need(i, j) = Max(i,j) - Allocation(i,j)<br>$$</p><p><img src="/../../images/OS/%E9%93%B6%E8%A1%8C%E5%AE%B6%E7%AE%97%E6%B3%95.png" alt="银行家算法"></p><h3 id="死锁检测"><a href="#死锁检测" class="headerlink" title="死锁检测"></a>死锁检测</h3><h4 id="资源分配图-进程-资源图"><a href="#资源分配图-进程-资源图" class="headerlink" title="资源分配图/进程-资源图"></a>资源分配图/进程-资源图</h4><p>用有向图描述系统资源和进程的状态。二元组G=（ N， E），N： 结点的集合，N=P∪R。</p><p>P为进程， R为资源，P = {p1, p2, … , pn}，R = {r1, r2, … , rm}，两者为互斥资源。E：有向边的集合，e属于E，e = (pi , rj ) 或e = (rj , pi )。</p><ul><li>e = (pi, rj)是请求边，进程pi请求一个单位的rj资源；</li><li>e = (rj, pi)是分配边，为进程pi分配了一个单位的rj资源</li></ul><h4 id="资源分配图-RAG-算法"><a href="#资源分配图-RAG-算法" class="headerlink" title="资源分配图(RAG)算法"></a>资源分配图(RAG)算法</h4><p>资源分配图的化简：首先，找到一个非孤立点进程结点且只有分配边，去掉分配边，将其变为孤立结点。接着，将相应的资源分配给一个等待该资源的进程，即将某进程的申请边变为分配边。重复以上步骤，直到所有进程化简完成，即进程与资源间无连线，则系统无死锁</p><p>在经过一系列的简化后，若能消去图中的所有边，使所有的进程都成为孤立结点，则称该图是<strong>可完全化简的</strong>；反之的是<strong>不可完全化简的</strong></p><p><strong>死锁定理</strong>：系统中某个时刻t为死锁状态的充要条件是t时刻系统的资源分配图是不可完全化简的。</p><h3 id="死锁解除"><a href="#死锁解除" class="headerlink" title="死锁解除"></a>死锁解除</h3><p>剥夺资源</p><p>撤销进程</p>]]></content>
      
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>内存管理</title>
      <link href="/2024/03/22/os/os-nei-cun-guan-li/"/>
      <url>/2024/03/22/os/os-nei-cun-guan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="预备知识：链接与装载"><a href="#预备知识：链接与装载" class="headerlink" title="预备知识：链接与装载"></a>预备知识：链接与装载</h1><p>gcc调用包含的几个工具</p><ul><li>cc1: 预处理和编译器</li><li>as: 汇编器</li><li>collect2: 链接器</li></ul><p>ELF(Executable and Linkable Format)——可执行文件格式</p><p><img src="/../../images/OS/ELF%E7%BB%93%E6%9E%84.png" alt="ELF结构图"></p><p><strong>几个重要节头：</strong></p><ul><li>.bss: 存储未初始化的全局变量和静态变量。此节类型是SHT_NOBITS,因此不占⽂件空间</li><li>.data: 存储已初始化的全局变量和静态变量。</li><li>.text: 存放正文，也称程序的执行指令</li></ul><p>.bss、.data节是数据段的组成部分</p><p>ELF文件头</p><ul><li>e_ident： 这一部分是文件的标志，用于表明该文件是一个ELF文件。ELF文件的头四个字节为magic number。</li><li>e_type： 用于标明该文件的类型，如可执行文件、动态连接库、可重定位文件等。</li><li>e_machine： 表明体系结构，如x86，x86_64，MIPS，PowerPC等等。</li><li>e_version： 文件版本</li><li>e_entry： 程序入口的虚拟地址</li><li>e_phoff： 程序头表在该ELF文件中的位置(具体地说是偏移)。ELF文件可以没有程序头表。</li><li>e_shoff： 节头表的位置。</li><li>e_eflags： 针对具体处理器的标志。</li><li>e_ehsize： ELF 头的大小。</li><li><strong>e_phentsize： 程序头表（段头表）每项的大小。</strong></li><li><strong>e_phnum： 程序头表项的个数。</strong></li><li><strong>e_shentsize： 节头表每项的大小。</strong></li><li><strong>e_shnum： 节头表项的个数。</strong></li><li>e_shstrndx： 与节名字符串表相关的节头表。</li></ul><p>节头表更关注于文件内部各个节的<strong>属性</strong>信息，而程序（段）头表则关注于程序<strong>执行时</strong>的段信息。</p><p>段是由多个节组成的，多个节经过重定位后形成一个段。段主要用于描述程序在内存中的布局和加载方式。例如，在创建两个汇编文件时，每个文件都有自己的数据节（.data）。当这两个文件链接在一起时，它们的数据节会组合在一起形成一个数据段。同样，代码节也会组合形成代码段。</p><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><p>将.o文件链接到一起，形成最终的可执行文件</p><ul><li>E重定位目标文件</li><li>U未解析符号</li><li>D已定义符号</li></ul><p>程序入口点：_start函数</p><p>三种链接方式</p><ul><li>静态链接：装入前连接成一个完整装入模块</li><li>装入时动态链接：运行前边装入边链接</li><li>运行时动态链接：运行时需要目标模块才装入并链接</li></ul><p>三种装入方式</p><ol><li>绝对装入：编译时产生绝对地址（单道程序阶段，无操作系统）</li><li>可重定位装入：装入时将逻辑地址转换为物理地址（早期多道批处理阶段）</li><li>动态运行时装入：运行时将逻辑地址转换为物理地址，需设置重定位寄存器（现代操作系统）</li></ol><h2 id="装载和运行"><a href="#装载和运行" class="headerlink" title="装载和运行"></a>装载和运行</h2><ul><li>shell调用fork()系统调用，创建出一个子进程（装载前的工作）</li><li>子进程调用execve()加载program（开始装载）</li><li>读取ELF头部的魔数(Magic Number)，以确认该文件确实是ELF文件</li><li>找到段表项</li><li>对于每个段表项解析出各个段应当被加载的虚地址，在文件中的偏移。以及在内存中的大小和在文件中的大小。（段在文件中的大小 &lt;= 内存中的大小）</li><li>对于每一个段，根据其在内存中的大小，为其分配足够的物理页，并映射到指定的虚地址上。再将文件中的内容拷贝到内存中</li><li>若ELF中记录的段在内存中的大小大于在文件中的大小，则多出来的部分用0进行填充。</li><li>设置进程控制块中的PC为ELF文件中记载的入口地址</li><li>控制权交给进程开始执行！</li></ul><h1 id="存储管理基础"><a href="#存储管理基础" class="headerlink" title="存储管理基础"></a>存储管理基础</h1><h2 id="存储保护"><a href="#存储保护" class="headerlink" title="存储保护"></a>存储保护</h2><p>保证各进程在自己的内存空间内运行，不会越界访问</p><p>两种方法：</p><ol><li><p>界限寄存器方法</p><p>两种方式：</p><ul><li>设置上下限寄存器</li><li>利用重定位寄存器（基址寄存器）、界地址寄存器（限长寄存器）进行判断</li></ul></li><li><p>存储保护键方法<br>给每个存储块分配一个单独的保护键，它相当于一把锁。进入系统的每个作业也赋予一个保护键，它相当于一把钥匙。当作业运行时，检查钥匙和锁是否匹配，如果不匹配，则系统发出保护性中断信号，停止作业运行</p></li></ol><h2 id="覆盖与交换：解决分区管理问题"><a href="#覆盖与交换：解决分区管理问题" class="headerlink" title="覆盖与交换：解决分区管理问题"></a>覆盖与交换：解决分区管理问题</h2><h3 id="覆盖技术（主要用于早期的OS）"><a href="#覆盖技术（主要用于早期的OS）" class="headerlink" title="覆盖技术（主要用于早期的OS）"></a>覆盖技术（主要用于早期的OS）</h3><ol><li>一个固定区：存放最活跃的程序段，固定区中的程序段在运行中不会调入调出</li><li>若干个覆盖区：不可能同时被访问程序段可共享一个覆盖区，覆盖区中的程序段在运行过程中会根据需要调入调出</li></ol><p>必须由程序员声明覆盖结构，操作系统完成自动覆盖</p><p>缺点：对用户不透明，增加了用户编程负担</p><h4 id="补充：计算机领域“透明”的含义"><a href="#补充：计算机领域“透明”的含义" class="headerlink" title="补充：计算机领域“透明”的含义"></a>补充：计算机领域“透明”的含义</h4><p>在计算机术语中，”透明”通常指的是一种操作或过程对用户或其他系统的影响被隐藏或减轻到最小程度，以使其表现为<strong>无缝、不可察觉或无需用户干预</strong>。这种透明性的目标是使系统<strong>更易于使用、更具可靠性，并减少对终端用户或其他系统组件的干扰</strong>。</p><p>通俗来讲：看不见，不用管它（系统管理员或者开发者无需显式干预和管理）。那么“不透明”就是指开发人员要考虑怎么用它。</p><h3 id="交换技术"><a href="#交换技术" class="headerlink" title="交换技术"></a>交换技术</h3><p>内存紧张时，换出某些进程以腾出内存空间，再换入某些进程。</p><p>磁盘分为文件区和对换区，换出的进程放在对换区。</p><h3 id="覆盖与交换的区别"><a href="#覆盖与交换的区别" class="headerlink" title="覆盖与交换的区别"></a>覆盖与交换的区别</h3><ul><li>覆盖：同一个程序或进程中的</li><li>交换：不同进程（或作业）之间的</li></ul><h2 id="连续分配管理"><a href="#连续分配管理" class="headerlink" title="连续分配管理"></a>连续分配管理</h2><h3 id="单一连续分配：只支持单道程序，内存分为系统区和用户区，用户程序放在用户区。"><a href="#单一连续分配：只支持单道程序，内存分为系统区和用户区，用户程序放在用户区。" class="headerlink" title="单一连续分配：只支持单道程序，内存分为系统区和用户区，用户程序放在用户区。"></a>单一连续分配：只支持单道程序，内存分为系统区和用户区，用户程序放在用户区。</h3><p>无外部碎片，有内部碎片</p><h3 id="固定分区分配：支持多道程序，内存用户空间分为若干个固定大小的分区，每个分区只能装一道作业"><a href="#固定分区分配：支持多道程序，内存用户空间分为若干个固定大小的分区，每个分区只能装一道作业" class="headerlink" title="固定分区分配：支持多道程序，内存用户空间分为若干个固定大小的分区，每个分区只能装一道作业"></a>固定分区分配：支持多道程序，内存用户空间分为若干个固定大小的分区，每个分区只能装一道作业</h3><p>无外部碎片，有内部碎片</p><p>两种分区方式</p><ul><li>分区大小相同</li><li>分区大小不同</li></ul><p>两种分配方式</p><ul><li>单一队列的分配方式：当需要加载程序时，选择一个当前闲置且容量足够大的分区进行加载，可采用共享队列的固定分区（多个用户程序排在一个共同的队列里面等待分区）分配</li><li>多队列分配方式：给每个分区一个队列，程序按照所需内存的大小排在相应的队列里。避免小程序占用大分区，导致大程序无法加载。</li></ul><h3 id="可变式分区（动态分区分配）：支持多道程序，在进程装入内存时，根据进程动态地建立分区"><a href="#可变式分区（动态分区分配）：支持多道程序，在进程装入内存时，根据进程动态地建立分区" class="headerlink" title="可变式分区（动态分区分配）：支持多道程序，在进程装入内存时，根据进程动态地建立分区"></a>可变式分区（动态分区分配）：支持多道程序，在进程装入内存时，根据进程动态地建立分区</h3><p>无内部碎片，有外部碎片。外部碎片可用<strong>紧凑</strong>技术解决</p><h2 id="闲置空间管理"><a href="#闲置空间管理" class="headerlink" title="闲置空间管理"></a>闲置空间管理</h2><p>在管理内存时，OS需要知道内存空间有多少空闲。这就必须跟踪内存的使用，跟踪的办法有两种：</p><ul><li><p>位图表示法（分区表）</p><ul><li><p>空间开销固定：不依赖于内存中的程序数量。</p></li><li><p>时间开销低：操作简单，直接修改其位图值即可。</p></li><li><p>没有容错能力：如果一个分配单元对应的标志位为1，不能确定是否因错误变成1。</p></li></ul></li><li><p>链表表示法（分区链表）</p><ul><li><p>空间开销：取决于程序的数量。</p></li><li><p>时间开销：链表扫描速度较慢，还要进行链表项的插入、删除和修改</p></li><li><p>有一定容错能力：因为链表有被占空间和闲置空间的表项，可相互验证</p></li></ul></li></ul><h2 id="分配内存"><a href="#分配内存" class="headerlink" title="分配内存"></a>分配内存</h2><p>若m.size（空闲分区的大小）- u.size（请求的分区大小）≤size （规定当剩余空闲分区≤ size时，不再进一步分割），将整个分区分配给请求者。否则，从该分区中按请求的大小划分出一块内存空间分配出去，余下的部分仍留在空闲分区表/链中</p><h2 id="回收内存"><a href="#回收内存" class="headerlink" title="回收内存"></a>回收内存</h2><ol><li>待回收分区与前一个空闲分区邻接：合并后首地址为空闲分区的首地址，大小为二者之和。</li><li>待回收分区与后一个空闲分区邻接：合并后首地址为回收分区的首地址，大小为二者之和。</li><li>待回收分区与前后两个空闲分区邻接：合并后首地址为前一个空闲分区的首地址，大小为三者之和。</li><li>待回收分区不与空闲分区邻接：在空闲分区表中新建</li></ol><h2 id="分配算法"><a href="#分配算法" class="headerlink" title="分配算法"></a>分配算法</h2><h3 id="基于顺序搜索的分配算法：适合于较小的系统"><a href="#基于顺序搜索的分配算法：适合于较小的系统" class="headerlink" title="基于顺序搜索的分配算法：适合于较小的系统"></a>基于顺序搜索的分配算法：适合于较小的系统</h3><ul><li><p>首次适应：每个空闲区按其在存储空间中地址递增的顺序连在一起。为作业分配存储空间时，从这个空闲区域链始端开始查找，选择第一个满足的空白块</p><p>缺点：留下碎片，查找时间开销大</p></li><li><p>下次适应：把空闲区构建成一个循环链表。每次从上次查找结束的地方开始，只要找到能装的就分配出去</p><p>优点：存储空间利用更均衡</p><p>缺点：缺乏大的空闲分区</p></li><li><p>最佳适应：找大小与作业所需存储区域最接近的</p><p>缺点：留下碎片</p></li><li><p>最坏适应：每次都找最大空闲区</p><p>优点：分给一个作业后剩下的空闲分区也较大，可以装下其他作业</p><p>缺点：大作业的空间申请难以满足</p></li></ul><h3 id="基于索引搜索的分配算法：大中型系统，提高搜索空闲分区的速度"><a href="#基于索引搜索的分配算法：大中型系统，提高搜索空闲分区的速度" class="headerlink" title="基于索引搜索的分配算法：大中型系统，提高搜索空闲分区的速度"></a>基于索引搜索的分配算法：大中型系统，提高搜索空闲分区的速度</h3><ul><li><p>快速适应算法（/分类搜索法）：把空闲分区按容量大小进行分类，常用大小的空闲区设立单独的空闲区链表。系统为多个空闲链表设立一张管理索引表</p><p>优点：查找效率高；保留大分区，不会产生内存碎片</p><p>缺点：分区回收算法复杂</p></li><li><p><strong>伙伴系统</strong>（Linux系统采用）：在分配存储块时将一个大的存储块分裂成两个大小相等的小块，这两个小块就称为“伙伴”。介于固定分区与可变分区之间的动态分区技术</p></li></ul><h3 id="紧凑技术"><a href="#紧凑技术" class="headerlink" title="紧凑技术"></a>紧凑技术</h3><p>实现支撑：动态重定位（作业在内存中的位置发生了变化，这就必须对其地址加以修改或变换。）</p><h3 id="可重定位分区分配"><a href="#可重定位分区分配" class="headerlink" title="可重定位分区分配"></a>可重定位分区分配</h3><p>结合紧凑技术的动态分区技术</p><p><img src="/../../images/OS/%E5%8F%AF%E9%87%8D%E5%AE%9A%E4%BD%8D%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AE%97%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="算法示意图"></p><h3 id="多重分区分配"><a href="#多重分区分配" class="headerlink" title="多重分区分配"></a>多重分区分配</h3><p>为了支持结构化程序设计，操作系统往往把一道作业分成若干片段（如子程序、主程序、数据组等）。这样，片段之间就不需要连续了。只要增加一些重定位寄存器，就可以有效地控制一道作业片段之间的调用</p><h1 id="页式内存管理"><a href="#页式内存管理" class="headerlink" title="页式内存管理"></a>页式内存管理</h1><p>基本思想：把一个逻辑地址连续的程序分散存放到若干个不连续的内存区域，并保证程序的正确执行。</p><p>纯分页/基本分页存储管理方式：在调度一个作业时，必须把它的所有页一次装到主存的页框内；如果当时页框数不足，则该作业必须等待，系统再调度另外作业。不具备页面对换功能，不支持虚拟存储器功能。</p><h2 id="补充知识：作业、进程和程序"><a href="#补充知识：作业、进程和程序" class="headerlink" title="补充知识：作业、进程和程序"></a>补充知识：作业、进程和程序</h2><p>作业通常包括程序、数据和操作说明书三部分。</p><p>每一个进程由进程控制块PCB、程序和数据集合组成。这说明程序是进程的一部分，是进程的实体。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul><li>页面/页：逻辑地址空间</li><li>页框/存储块：物理内存的存储空间，与页面相同大小的片</li><li>页表项：每个页面到页框的映射</li><li>页表：分页系统为每个进程配置的一张表，页表项的集合。存放在内存中</li></ul><p>页面大小</p><ul><li>小：页内碎片少，利于提高内存利用率；页表占用内存较大；页面换进换出速度降低</li><li>大：与小相反</li></ul><h2 id="地址变换机构"><a href="#地址变换机构" class="headerlink" title="地址变换机构"></a>地址变换机构</h2><ol><li>内存系统区的PCB（进程控制块）将页表信息传给页表寄存器（存储<u>页表起始地址+页表长度</u>信息）</li><li>根据逻辑地址计算页号、页内偏移量</li><li>判断是否越界：页号 &lt;= 页表长度 ？</li><li>查询页表，找到页号对应的页表项，确定页号对应的页框号</li><li>用页框号+页内偏移量计算出物理地址</li><li>访问目标内存单元</li></ol><h2 id="二级页表"><a href="#二级页表" class="headerlink" title="二级页表"></a>二级页表</h2><p>目录-页表-偏移量</p><h3 id="多级页表为什么省空间"><a href="#多级页表为什么省空间" class="headerlink" title="多级页表为什么省空间"></a>多级页表为什么省空间</h3><p>因为多级页表允许操作系统根据进程实际使用的内存量来动态分配页表空间（想想实验课的MOS）</p><p>多级页表的设计允许操作系统在创建进程时只需为每个进程分配一个一级页表，然后根据进程申请的内存空间，再为进程分配二级页表。这种按需分配的方式避免了单级页表在进程创建时为可能用到的所有页表项分配空间，从而减少了不必要的内存占用。</p><h2 id="快表-TLB（转换表查找缓冲区）-联想存储器"><a href="#快表-TLB（转换表查找缓冲区）-联想存储器" class="headerlink" title="快表/TLB（转换表查找缓冲区）/联想存储器"></a>快表/TLB（转换表查找缓冲区）/联想存储器</h2><p>为了解决页表机制带来的内存访问效率严重下降问题</p><p>快表一种特殊的cache，内容是页表中的一部分或全部内容</p><p>结构：Valid + Virtual page + Modified + Protection + Page frame</p><h2 id="哈希页表：处理超过32位地址空间的一种常用方法"><a href="#哈希页表：处理超过32位地址空间的一种常用方法" class="headerlink" title="哈希页表：处理超过32位地址空间的一种常用方法"></a>哈希页表：处理超过32位地址空间的一种常用方法</h2><p>将虚拟页号转换为哈希值，据此访问哈希表的表项（链表）。用虚拟页号与链表中的元素的第一个域相比较。如果匹配，那么相应的页框号（第二个域）就用来形成物理地址；如果不匹配，那么就进一步比较链表的下一个节点，以找到匹配</p><h2 id="反置页表"><a href="#反置页表" class="headerlink" title="反置页表"></a>反置页表</h2><p>页表按页框号排序，页表项内容是逻辑页号+隶属进程标识符</p><p>优点：页表占用的内存空间小</p><h2 id="页共享与保护"><a href="#页共享与保护" class="headerlink" title="页共享与保护"></a>页共享与保护</h2><p>保护：</p><ul><li>地址越界保护</li><li>在页表中设置保护位（定义操作权限：只读，读写，执行等）</li></ul><p>共享：分段存储管理（解决共享数据和不共享数据出现在同一个页框的问题）</p><h1 id="段式内存管理（类比页式）"><a href="#段式内存管理（类比页式）" class="headerlink" title="段式内存管理（类比页式）"></a>段式内存管理（类比页式）</h1><p>将地址空间按照程序自身的逻辑关系划分为若干个段（类比页式的”分成若干页面“）。</p><p>段表项：段号（隐含在地址下标）、<strong>段长</strong>+基址（区别于页式，段的长度不固定，页的大小固定）</p><p>地址变换（类比页式的地址变换，需增加一步：根据段表内的段长检查段内地址/偏移是否越界）</p><h2 id="分段和分页对比"><a href="#分段和分页对比" class="headerlink" title="分段和分页对比"></a>分段和分页对比</h2><table><thead><tr><th></th><th>分段</th><th>分页</th></tr></thead><tbody><tr><td>对用户</td><td>可见</td><td>不可见</td></tr><tr><td>地址空间</td><td>二维</td><td>一维</td></tr><tr><td>信息共享和保护实现难易程度</td><td>更容易实现</td><td>更难实现</td></tr><tr><td>长度</td><td>不固定</td><td>固定</td></tr><tr><td>都需要两次访存，都可引入快表机构</td><td></td><td></td></tr></tbody></table><h2 id="段页式内存管理"><a href="#段页式内存管理" class="headerlink" title="段页式内存管理"></a>段页式内存管理</h2><p>段里再分页</p><p>逻辑地址结构：段号+页号+页内偏移量</p><p>段表项：段号（隐含在地址下标）、页表长度（一个段里有几页）+页表存放块号（页表起始地址）</p><p>页表项：页号（隐含在地址下标）、页框号</p><p>地址变换机构：段式和页式的结合，三次访存，可引入快表机构（段号+页号）</p><h1 id="虚拟内存管理"><a href="#虚拟内存管理" class="headerlink" title="虚拟内存管理"></a>虚拟内存管理</h1><h2 id="局部性原理"><a href="#局部性原理" class="headerlink" title="局部性原理"></a>局部性原理</h2><p>时间局部性：程序中存在大量循环</p><p>空间局部性：很多数据在内存中是连续存放的</p><p>基于局部性原理产生了高速缓存技术</p><h2 id="虚拟内存的定义和特征"><a href="#虚拟内存的定义和特征" class="headerlink" title="虚拟内存的定义和特征"></a>虚拟内存的定义和特征</h2><p>定义：程序不需全部装入即可运行，运行时根据需要动态调入数据，若内存不够，还需换出一些数据</p><table><thead><tr><th>特征比较</th><th>虚拟内存</th><th>传统存储管理方式</th></tr></thead><tbody><tr><td></td><td>多次性</td><td>一次性</td></tr><tr><td></td><td>对换性</td><td>驻留性</td></tr><tr><td></td><td>虚拟性</td><td></td></tr><tr><td></td><td>离散性</td><td></td></tr></tbody></table><h2 id="虚拟内存的实现"><a href="#虚拟内存的实现" class="headerlink" title="虚拟内存的实现"></a>虚拟内存的实现</h2><ul><li>请求分页式存储</li><li>请求分段式存储</li><li>请求段页式存储</li></ul><p>操作系统要提供请求调页（调段）功能，页面置换（段置换）功能</p><h2 id="请求分页管理方式"><a href="#请求分页管理方式" class="headerlink" title="请求分页管理方式"></a>请求分页管理方式</h2><h3 id="一些基本概念"><a href="#一些基本概念" class="headerlink" title="一些基本概念"></a>一些基本概念</h3><ol><li><p>进程的逻辑空间：一个进程的逻辑空间的建立是通过链接器（Linker），将构成进程所需要的所有程序及运行所需环境，按照某种规则装配链接而形成的一种规范格式(布局)，这种格式按字节从0开始编址所形成的空间也称为该进程的逻辑地址空间</p></li><li><p>虚拟地址空间和虚拟存储空间：进程的虚拟地址空间即为进程在内存中存放的逻辑视图。因此，一个进程的虚拟地址空间的大小与该进程的虚拟存储空间的大小相同，都从0开始编址。</p></li><li><p>交换分区（交换文件）：一段连续的磁盘空间（按页划分的），并且对用户不可见。</p><p>功能：在物理内存不够的情况下，OS先把内存中暂时不用的数据，存到磁盘的交换空间，腾出物理内存来让别的程序运行。</p><p>在Linux系统中，交换分区为swap；在Windows系统中则以文件的形式存在（pagefile.sys)。</p><p>交换分区的大小：应当与系统物理内存（M）的大小保持线性比例关系(Linux中）：</p><p>If (M &lt; 2GB) swap = 2*M</p><p>else swap = M + 2GB</p></li></ol><h3 id="请求式分页管理的页表项"><a href="#请求式分页管理的页表项" class="headerlink" title="请求式分页管理的页表项"></a>请求式分页管理的页表项</h3><ul><li>逻辑页号（隐含）</li><li>访问位（用于页面置换算法）</li><li>修改位（表明此页在内存中是否被修改过）</li><li>保护位（只读、可写、可执行）</li><li>驻留位（1：该页位于内存中；2：该页还在外存中）</li><li>物理页帧号</li></ul><h2 id="页面调入策略"><a href="#页面调入策略" class="headerlink" title="页面调入策略"></a>页面调入策略</h2><ol><li><p>按需调页/请求式调页：当且仅当需要某页时才将其调入内存的技术</p></li><li><p>预调页：同时将所需要的所有页一起调入内存</p><p>优缺点：与按需调页相比，阻止了大量的页错误（也叫缺页异常）；但若程序执行的局部性较差，则预先装入的很多页面不会很快被引用，并会占用大量的内存空间，反而降低系统的效率。</p><p>实际应用中，可以为每个进程维护一个当前工作集（<strong>进程运行正在使用的页面集合</strong>）中的页的列表，如果进程在暂停之后需要重启时，根据这个列表使用预调页将所有工作集合中的页一次性调入内存</p></li></ol><h2 id="页面置换策略"><a href="#页面置换策略" class="headerlink" title="页面置换策略"></a>页面置换策略</h2><table><thead><tr><th>算法</th><th>算法规则</th><th>优缺点</th></tr></thead><tbody><tr><td>最优置换（OPT）</td><td>从主存中移出永远不再需要的页面，如这样的页面不存在，则应选择最长时间不需要访问的页面。</td><td>所有页置换算法中页错误率最低<br>但它需要引用串（即页面访问序列）的先验知识，因此无法实现。但通常用于比较性研究，衡量其他页置换算法的效果</td></tr><tr><td>先进先出（FIFO）</td><td>优先淘汰最先进入的页面</td><td>性能较差，较早调入的页往往是经常被访问的页，导致它们被反复调入调出，可能出现belady异常（在使用FIFO算法作为缺页置换算法时，随着分配的页框增多，缺页率反而提高）</td></tr><tr><td>Second Chance（改进的FIFO算法）</td><td></td><td></td></tr><tr><td>Clock（改进的FIFO算法&amp;Second Chance）</td><td>通过一个环形队列，避免将数据在FIFO队列中移动</td><td></td></tr><tr><td>最近最少使用（LRU）</td><td>选择最近一段时间最久不用的页面去置换</td><td>是局部性原理的合理近似，性能接近OPT算法<br>但需记录页面使用的先后关系，实现开销大</td></tr><tr><td>*老化算法（AGING，LRU近似）</td><td>为每个页面设置一个移位寄存器，每隔一段时间（clocktick），所有寄存器右移1位，并将访问位R值从左移入。淘汰寄存器中数值最小的页面</td><td></td></tr><tr><td>*工作集算法</td><td>给定一个进程，记录其工作集，当需要进行页面替换时，选择不在工作集中的页面进行替</td><td></td></tr><tr><td>WSClock工作集时钟页面置换算法</td><td>和时钟算法很像，将页框组成一个循环表，每个表项包含来自基本工作集算法的上次使用时间</td><td></td></tr></tbody></table><hr><p>前面是从一个进程的角度讨论虚存管理的相关问题，下面是从系统管理者（OS）的角度讨论多个进程情况下虚存管理的问题</p><h3 id="概念：工作集和驻留集"><a href="#概念：工作集和驻留集" class="headerlink" title="概念：工作集和驻留集"></a>概念：工作集和驻留集</h3><ul><li>进程的工作集（working set）：当前正在使用的页面的集合。</li><li>进程的驻留集（resident set ）：虚拟存储系统中，每个进程驻留在内存的页面集合，或进程分到的物理页框集合。</li></ul><h2 id="多进程页面分配策略"><a href="#多进程页面分配策略" class="headerlink" title="多进程页面分配策略"></a>多进程页面分配策略</h2><ul><li>固定：每个进程分配固定数量的页框</li><li>可变</li></ul><h2 id="多进程页面置换策略"><a href="#多进程页面置换策略" class="headerlink" title="多进程页面置换策略"></a>多进程页面置换策略</h2><ul><li>局部：系统在进程自身的驻留集中判断当前是否存在空闲页框，并在其中进行置换</li><li>全局</li></ul><h2 id="分配与置换的搭配"><a href="#分配与置换的搭配" class="headerlink" title="分配与置换的搭配"></a>分配与置换的搭配</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">    A[固定分配策略]--&gt;C[局部置换策略];</span><br><span class="line">    B[可变分配策略]--&gt;C;</span><br><span class="line">    B--&gt;D[全局置换策略];</span><br></pre></td></tr></tbody></table></figure><h2 id="抖动问题（thrashing）"><a href="#抖动问题（thrashing）" class="headerlink" title="抖动问题（thrashing）"></a>抖动问题（thrashing）</h2><p>随着驻留内存的进程数目增加，即进程并发程度的提高，处理器利用率先上升，然后下降。因为每个进程的驻留集不断减小，当驻留集小于工作集后，缺页率急剧上升，频繁调页使得调页开销增大。</p><h3 id="抖动的消除与预防"><a href="#抖动的消除与预防" class="headerlink" title="抖动的消除与预防"></a>抖动的消除与预防</h3><p>局部置换策略：使抖动局限在一个小的范围内，但是并未消除抖动的方式。</p><p>引入工作集算法</p><p>预留部分页面</p><p>挂起若干进程</p><h3 id="负载控制"><a href="#负载控制" class="headerlink" title="负载控制"></a>负载控制</h3><p>主要解决系统应当保持多少个活动进程驻留在内存的问题，即控制多道程序系统的度。当内存中的活动进程数太少时，负载控制将增加新进程或激活一些挂起进程进入内存；反之，当内存中的进程数太多时，负载控制将暂时挂起一些进程，减少内存中的活动进程数。</p><h3 id="页面缓冲算法"><a href="#页面缓冲算法" class="headerlink" title="页面缓冲算法"></a>页面缓冲算法</h3><p>对FIFO算法的发展</p><p>被置换页面的选择和处理：用FIFO算法选择被置换页，把被置换的页面放入两个链表之一。即：如果页面未被修改，就将其归入到空闲页面链表的末尾，否则将其归入到已修改页面链表。</p><h3 id="写时复制技术"><a href="#写时复制技术" class="headerlink" title="写时复制技术"></a>写时复制技术</h3><p>两个进程共享同一块物理内存，每个页面都被标志成了写时复制。共享的物理内存中每个页面都是只读的。如果某个进程想改变某个页面时，就会与只读标记冲突，而系统在检测出页面是写时复制的，则会在内存中复制一个页框，然后进行写操作。新复制的页框对执行写操作的进程是私有的，对其他共享写时复制页面的进程是不可见的。</p><p>优点：</p><p>传统的fork()系统调用直接把所有的资源复制给新创建的进程。这种实现过于简单而效率低下，因为它拷贝的数据也许并不共享，如果新进程打算立即执行一个新的映像，那么所有的拷贝都将前功尽弃。</p><p>Linux的fork()使用写时复制实现，它<strong>可以推迟甚至免除拷贝数据的技术</strong>。内核此时并不复制整个进程地址空间，而是让父进程和子进程共享同一个拷贝。只有在需要写入的时候，数据才会复制，从而使各个进程都拥有各自的拷贝。也就是说，资源的复制只有在需要写入的时候才进行。</p><h1 id="页目录自映射"><a href="#页目录自映射" class="headerlink" title="页目录自映射"></a>页目录自映射</h1><p>以二级页表为例：设页表基址$ PT_{base} $（4MB对齐，即低22位为0，非必须，这是为了让页目录表在一个单独页表内，也可以简化计算），则页目录表基址$ PD_{base}=PT_{base} | (PT_{base} &gt;&gt; 10) $，自映射目录表项$ PDE_{self-mapping}=PT_{base} | (PT_{base} &gt;&gt; 10) | (PT_{base} &gt;&gt; 20) $。可推广到多级页表。</p><p>自映射的数学意义：手持北京地图在北京，必有地图上一点与其表示的地理位置与该点的实际地理位置重合。(压缩映像)</p>]]></content>
      
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OO第一单元总结</title>
      <link href="/2024/03/19/oo/oo-unit1-summary/"/>
      <url>/2024/03/19/oo/oo-unit1-summary/</url>
      
        <content type="html"><![CDATA[<h2 id="第一次作业架构与心得分享"><a href="#第一次作业架构与心得分享" class="headerlink" title="第一次作业架构与心得分享"></a>第一次作业架构与心得分享</h2><h3 id="一、架构设计"><a href="#一、架构设计" class="headerlink" title="一、架构设计"></a>一、架构设计</h3><p>如下图：</p><p><img src="/../../images/OO/unit1-1.png"></p><h3 id="二、总体思路"><a href="#二、总体思路" class="headerlink" title="二、总体思路"></a>二、总体思路</h3><p>最开始觉得<code>'(表达式)^n'</code>去括号这块很难解决，后来发现其实可以把题目看作一个多项式化简的问题，而**化简后的每一项都可以表示为<code>系数*x^非负指数</code>**，所以在训练题第二题的基础上增加了幂函数因子类及预处理多项式相乘、合并同类项等过程，具体流程如下：</p><ol><li>输入表达式预处理：删掉所有空白项，去掉连续加减号 </li><li>解析输入表达式：方法类似训练题第二题，采用递归下降的方法，不过训练题只有‘+’，而这里‘+’、‘-’都有，所以需要存储运算符号，而我后续要将Expr化简成Term的集合，所以就用Term的系数coefficient记录‘+’、‘-’</li><li>化简表达式：采用多项式相乘的方法去掉括号，Term类增加了coefficient、exponent属性，将表达式转化成若干项的集合</li><li>合并同类项：将指数相同的项合并</li><li>输出：第一项系数若非负则不输出符号，后续项按照项的sign输出符号及项</li></ol><h3 id="三、bug分析"><a href="#三、bug分析" class="headerlink" title="三、bug分析"></a>三、bug分析</h3><p>本次作业未被hack。我测试别人的方法是造一些和0、1相关的数据，如(0)^0, 1*x, 2-2等等</p><h2 id="第二次作业架构"><a href="#第二次作业架构" class="headerlink" title="第二次作业架构"></a>第二次作业架构</h2><h3 id="一、架构设计-1"><a href="#一、架构设计-1" class="headerlink" title="一、架构设计"></a>一、架构设计</h3><p><img src="/../../images/OO/unit1-2.png" alt="类图"></p><p>与第一次相比：</p><ul><li>增加了Poly、Mono类</li><li>增加了SelfFunction类处理函数</li></ul><h3 id="二、总体思路-1"><a href="#二、总体思路-1" class="headerlink" title="二、总体思路"></a>二、总体思路</h3><p>第二次作业新增的主要任务如下：</p><ol><li>支持嵌套多层括号。</li><li>新增指数函数因子，指数函数括号内部包含任意因子。</li><li>新增自定义函数因子，但自定义函数的函数表达式中不会调用其他自定义函数。</li></ol><p>第一个任务我在第一次作业就已经完成了（事实上感觉只要用了递归下降，这块不需要刻意实现）对于第二个任务，我新建了ExpFunction类来处理。</p><p>第三个任务，我原本尝试了一下在解析表达式的时候处理函数，但是发现很麻烦，再加上我第一次作业的架构不是很好，把标准项放在Term里一并处理，导致解析表达式非常混乱，结构很不清晰，最后还有深浅拷贝的问题，以上种种原因造成了这条路走不通。于是我请教了同学，在交流中发现了自己架构的不足，最后决定重构。</p><p>首先，添加了Poly、Mono类，然后把解析完的Expr转化为Poly来管理。Poly、Mono的主要作用就是存储标准项（标准项的形式为 $${coef*x^{xexpo}*\exp (eexpo)}$$ ,最终的表达式一定能转化为若干标准项相加的形式）,这么做可以把解析和化简分离开，结构更清晰，同时规避了一些深浅拷贝的问题。</p><p>其次，对于自定义函数的处理我选择了在预处理阶段进行字符串替换。这么做解析阶段就不用处理函数了，而且对于处理第三次作业新增的“函数定义式可以调用已经定义的函数”这一新任务意外地地方便，不过需要注意的是：</p><ul><li>函数定义式里的形参替换成因子时因子外面要加一层括号</li><li>函数表达式替换表达式里的函数调用的时候表达式外面要加一层括号<br>这么做是为了避免运算优先级出错，例如：<figure class="highlight ruby"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line">f(x)=x+<span class="number">1</span></span><br><span class="line">f(x)*<span class="number">2</span></span><br><span class="line">不加括号的结果是x+<span class="number">1</span>*<span class="number">2</span>=x+<span class="number">2</span>,显然不对</span><br></pre></td></tr></tbody></table></figure></li></ul><figure class="highlight ruby"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">1</span></span><br><span class="line">f(x)=x^<span class="number">2</span>;</span><br><span class="line">f(-<span class="number">2</span>)</span><br><span class="line">不加括号的结果是-<span class="number">2</span>^<span class="number">2</span>=-<span class="number">4</span>,显然也不对</span><br></pre></td></tr></tbody></table></figure><h3 id="三、bug分析-1"><a href="#三、bug分析-1" class="headerlink" title="三、bug分析"></a>三、bug分析</h3><p>这次作业在写的时候遇到了很多bug：</p><ul><li>深、浅拷贝问题：第一次作业我把标准项放到Term里，第二次我本来打算沿用的，然而对深、浅拷贝认识不透彻导致我第二次作业debug出现了“闹鬼”，有的数据明明没有动它但它自己变了，由于这个变动的数据在很深层，很难找出哪里的拷贝出现了问题，我决定直接新建两个类Poly、Mono来管理处理完的表达式，并在Poly类中专门写了一个方法<code>public Poly copy()</code>来进行拷贝，避免出现“闹鬼”事件</li><li>指数函数化简问题：我的做法是把表达式转化成Poly类的时候把指数函数的指数因子乘进去，作为指数函数的指数的一部分（即Poly中的属性ArrayList&lt;Monos&gt;数组的一个Mono管理），如何最后输出时，如果<code>exp(因子)</code>的因子是<code>coef*x^xexpo</code>形式或者<code>coef*exp(...)^eexpo</code>形式，就把系数coef提到括号外，减少一个括号的使用,即<code>exp(...)^coef</code>。但是最开始忽略了一点：如果系数coef是负数，不能提出来，否则指数就是负数，不符合指数<strong>非负</strong>的要求，即<code>exp(...)^coef</code>(coef不能是负数)。（利用这一点，我成功hack了房间里一位同学，不过ta的问题是只把<code>|coef|</code>提出来，负号<code>-</code>留里面，但是同样不行。例如<code>exp((-3*x)) -&gt; exp(-x)^3</code>，变量因子是不能有负号的，这里提取来之后里面还是一个表达式，需要再加一层括号，<code>exp((-x))^3</code>才是正确的）</li></ul><p>本次作业未被hack，但是强测有一个数据点幂函数指数爆int了，并且由此发现自己合并同类项的方法时间复杂度较高，进行了优化。互测阶段我造数据的方法就是拿我自己写代码时候遇到的bug点去测，比如指数函数化简问题（如上）。</p><h2 id="第三次作业架构"><a href="#第三次作业架构" class="headerlink" title="第三次作业架构"></a>第三次作业架构</h2><h3 id="一、架构设计-2"><a href="#一、架构设计-2" class="headerlink" title="一、架构设计"></a>一、架构设计</h3><p>最终总体架构如下：</p><p><img src="/../../images/OO/unit1-3.png" alt="第三次作业架构图"></p><p>与之前相比，新增了Derivative类管理求导因子，主要修改了Poly、Mono里的一些方法用来求导</p><h3 id="二、基本思路"><a href="#二、基本思路" class="headerlink" title="二、基本思路"></a>二、基本思路</h3><p>这次作业相比第二次作业增加了两点：</p><ol><li>自定义函数支持调用其他“已定义的”函数</li><li>支持求导操作</li></ol><p>对于第一点，由于我第二次作业采用的是在预处理阶段字符串替换掉函数，即：读入函数的名称、参数和表达式，放在ArrayList&lt;SelfFunction&gt;数组中存储，然后在预处理阶段替换掉表达式里出现函数名的地方，所以本次作业我直接把替换阶段的函数列表反着遍历就可以了。</p><p>对于第二点，我的解决办法是新开一个Derivative类，在解析表达式的时候遇到“dx”就把它括号后面的表达式解析完后存储在Derivative中的expr属性中，然后在Expr转换为Poly时再求导</p><p>由于我把Derivative里的expr转化为Poly类再求导，所以求导公式非常固定，即：</p><p>$$dx\left( {coef<em>x^{xexpo}<em>\exp \left(eexpo\right)} \right) = coef</em>xexpo</em>x^{xexpo - 1}<em>\exp (eexpo) + coef</em>dx(ab)*x^{xexpo}*\exp (eexpo)$$</p><p>求导中也用到了递归下降的思想，对Poly求导，即对Poly的每个Mono求导；对Mono求导，应用上述公式，得到的是Poly/ArrayList&lt;Mono&gt;</p><h3 id="三、bug分析-2"><a href="#三、bug分析-2" class="headerlink" title="三、bug分析"></a>三、bug分析</h3><p>本次作业无强测bug，未被hack。第二次作业的架构调整对第三次作业帮助很大，在其上迭代开发非常清晰，不容易出现问题</p><h2 id="度量分析"><a href="#度量分析" class="headerlink" title="度量分析"></a>度量分析</h2><p>方法复杂度</p><p><img src="/../../images/OO/%E6%96%B9%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6.png" alt="方法复杂度"></p><p>类复杂度</p><p><img src="/../../images/OO/%E7%B1%BB%E5%A4%8D%E6%9D%82%E5%BA%A6.png" alt="类复杂度"></p><p>各类代码行数</p><p><img src="/../../images/OO/%E7%B1%BB%E4%BB%A3%E7%A0%81%E9%87%8F.png" alt="代码行数"></p><h2 id="架构设计体验"><a href="#架构设计体验" class="headerlink" title="架构设计体验"></a>架构设计体验</h2><p>我的架构是基于第一次实验提供的递归下降方法，主要用Lexer、Parser类解析表达式，用Expr、Term、Factor保存，然后根据每次迭代增加相应的类与方法（见每次作业的架构设计部分）。在第二次作业经历了一次重构，主要是增加了Poly、Mono类，将表达式解析和化简分离。</p><p>新的可能迭代场景：表达式中增加三角函数。需要新增三角函数类Trigo，标准项变成$$coef*x^{xexpo}*\exp \left(eexpo\right)*sin\left(poly\right)^{triExpo}$$, 可以将cos化为sin一并处理。</p><h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><ul><li>在第二次作业强测由于爆int发现自己合并同类项的复杂度太高（因为我是找出x的指数最大值max，然后从max到0遍历每个指数，再遍历每个项找到满足当前条件的指数项合并，但是事实上可能表达式中根本就没有或很少比max小的指数项，比如表达式为$x^{100000000000}$, 我要从100000000000遍历到0，非常费时）。于是优化后的化简方式为：遍历多项式（Poly）里的每个单项式（Mono），以当前单项式为基准，遍历它后面的单项式看是否有它的同类项。</li><li>最开始去括号化简我是采用先全部乘开再合并同类项，后来发现边乘开边合并同类项不仅速度更快，而且指数函数的指数多项式比较起来更方便。</li></ul><h2 id="心得体会"><a href="#心得体会" class="headerlink" title="心得体会"></a>心得体会</h2><ul><li>对深、浅拷贝的问题有了很深刻的认识（de这种bug的感觉实在是印象深刻）</li><li>对继承和接口的作用有了更清晰的了解：继承是数据层面的抽象（有相同属性），接口是行为层面的抽象（有类似的方法，但是可能实现过程不太一样）</li></ul><h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><p>个人认为性能分的计算方式可以适当调整一下，寻找更合理的计算方式。以这次作业为例，在实际生活中可能人们更习惯于看到多项式按一定规律排列（比如按x的降次/升次排列），但是为了性能分不得不把最高次系数为负数的项挪到后面，感觉不是很美观。如果换种方式，比如可以让输出表达式少于某一长度就得多少分，而不是以所有人作业的输出最短长度为基准（或者其他更好的办法），或许也不错。</p>]]></content>
      
      
      <categories>
          
          <category> OO </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OOpre反思总结</title>
      <link href="/2024/03/19/oo/oopre-summary/"/>
      <url>/2024/03/19/oo/oopre-summary/</url>
      
        <content type="html"><![CDATA[<h2 id="作业最终架构"><a href="#作业最终架构" class="headerlink" title="作业最终架构"></a>作业最终架构</h2><p>如图</p><p><img src="/../../images/OO/OOpre%E4%BD%9C%E4%B8%9A%E7%B1%BB%E5%9B%BE.png" alt="OOpre作业类图"></p><h2 id="迭代中的架构调整及考虑"><a href="#迭代中的架构调整及考虑" class="headerlink" title="迭代中的架构调整及考虑"></a>迭代中的架构调整及考虑</h2><ul><li>operate指令执行调整：可以说是我花时间最久，改动最大的一次架构调整了。最初我使用if-else处理指令输入(<del>因为switch-case要消耗更多行，所有没用</del>)，但是每次迭代指令种类都在增加，无法满足“方法不超过60行“的限制，最后是看了老师分享的博客以及自己学习了一下<em>表驱动法</em>，并用此方法优化了自己的代码。<figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">// 表驱动法</span></span><br><span class="line"><span class="title class_">Map</span>&lt;<span class="title class_">Integer</span>, <span class="title class_">Function</span>&lt;<span class="title class_">ArrayList</span>&lt;<span class="title class_">String</span>&gt;, <span class="title class_">Void</span>&gt;&gt; actionsMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"><span class="comment">// 初试配置对应动作</span></span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">1</span>, operate.<span class="title function_">getOp1</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">2</span>, operate.<span class="title function_">getOp2</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">3</span>, operate.<span class="title function_">getOp3</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">4</span>, operate.<span class="title function_">getOp4</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">5</span>, operate.<span class="title function_">getOp5</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">6</span>, operate.<span class="title function_">getOp6</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">7</span>, operate.<span class="title function_">getOp7</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">8</span>, operate.<span class="title function_">getOp8</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">9</span>, operate.<span class="title function_">getOp9</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">10</span>, operate.<span class="title function_">getOp10</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">11</span>, operate.<span class="title function_">getOp11</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">12</span>, operate.<span class="title function_">getOp12</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">13</span>, operate.<span class="title function_">getOp13</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">14</span>, operate.<span class="title function_">getOp14</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">15</span>, operate.<span class="title function_">getOp15</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">16</span>, operate.<span class="title function_">getOp16</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">17</span>, operate.<span class="title function_">getOp17</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">18</span>, operate.<span class="title function_">getOp18</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">19</span>, operate.<span class="title function_">getOp19</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">20</span>, operate.<span class="title function_">getOp20</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">21</span>, operate.<span class="title function_">getOp21</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">22</span>, operate.<span class="title function_">getOp22</span>());</span><br><span class="line">        actionsMap.<span class="title function_">put</span>(<span class="number">23</span>, operate.<span class="title function_">getOp23</span>());</span><br><span class="line">        <span class="comment">// 省略 null 判断</span></span><br></pre></td></tr></tbody></table></figure></li><li>指令输入处理和执行分离：这个是参考了PPT里给出的架构，用两个类MyScanner和Manager分别进行输入处理和执行（也是从这里开始真正领悟到了第一节课反复提到的”万物皆可为对象“的含义）。</li><li>特别复杂的指令，如14号指令，将其根据步骤分成若干个函数，分步处理，结构更清晰。</li></ul><h1 id="使用Junit的心得体会"><a href="#使用Junit的心得体会" class="headerlink" title="使用Junit的心得体会"></a>使用Junit的心得体会</h1><p>Junit能够很好地进行单元测试，检查每个方法写的有没有问题。遇到bug时，通过编写单元测试，可以更方便地定位错误位置。</p><p>讲真，最开始总感觉我的Junit没啥用，完全是为了覆盖率而编写测试代码。但是随着迭代，Junit真的帮我de出了很多错误，例如一些访问到空指针的错误，逐渐体会到它的妙用。不过我感觉我对Junit的使用还不太成熟，没有发挥它最大的作用，还有许多要反思的地方。以下是我使用Junit后的总结与反思：</p><ul><li>尽量<strong>不要使用Junit的输出人工判断对错</strong>，交给断言去判断。这是一个我没太注意的点，以后需要注意！</li><li>编写单元测试注意代码覆盖率，尽量都测试到。</li><li>考虑一些极端情况，如数据范围边界处。</li></ul><h1 id="学习oopre的心得体会"><a href="#学习oopre的心得体会" class="headerlink" title="学习oopre的心得体会"></a>学习oopre的心得体会</h1><ul><li>”万物皆可为对象“！前面也提到过，老师第一节课反复提到的话我在经历了几次迭代作业后才真正体会到，尤其是调整自己代码架构的时候，经常会感叹”原来这也可以写一个类啊“。最开始几次的迭代，几乎每次都把代码大改了一遍，因为每次上完课都会觉得“自己之前写的什么玩意儿”&gt;_&lt;（泪目)，虽然耗时特别长，但是在修改过程中还是挺有收获的，对“对象”的理解也更深入了，看着自己的代码结构逐渐清晰非常开心！</li><li>代码模块化。跟第一条有相似之处，最开始我把很多东西都放在Main类里面，但是随着迭代，main方法越来越冗长，看着很不优雅（<del>还有方法不超过60行的限制</del>），所以不得不做出大改动。通过将处理过程分步骤，代码架构更清晰，debug的时候也方便定位错误。</li><li>变量的保护，即关于权限修饰符private，public，protected使用区别的体会。Java是面向对象编程，涉及到顾客使用时的问题，为了不让其他人随意修改一些变量值，必须限制一些变量、方法的访问权限，这在面向过程编程里是没有的。</li></ul><h1 id="对OOpre课程的一点点小建议"><a href="#对OOpre课程的一点点小建议" class="headerlink" title="对OOpre课程的一点点小建议"></a>对OOpre课程的一点点小建议</h1><p>1.每次的作业指导书对我学习这门课帮助挺大的，<strong>但是</strong>第一次作业指导书真的有点难看懂&gt;_&lt;，结构很混乱，甚至不知道从何看起，感觉可以改进一下。</p>]]></content>
      
      
      <categories>
          
          <category> OO </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov5训练总结</title>
      <link href="/2024/03/12/dl/yolov5-xun-lian-zong-jie/"/>
      <url>/2024/03/12/dl/yolov5-xun-lian-zong-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="yolov5前期配置和运行"><a href="#yolov5前期配置和运行" class="headerlink" title="yolov5前期配置和运行"></a>yolov5前期配置和运行</h2><p>参考资料：<a href="http://t.csdnimg.cn/MjIs3">Yolov5训练自己的数据集（详细完整版）</a></p><p>注：VOCData文件夹（可以自己命名）下的images和labels文件夹不能叫别的名字！</p><p>使用镜像源安装库：</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pip install -r requirements.txt -i  https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></tbody></table></figure><p>运行train.py的命令</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">python train.py --weights weights/yolov5s.pt  --cfg models/yolov5s.yaml  --data data/myvoc.yaml --epoch 200 --batch-size 8 --img 640   --device cpu --resume True</span><br></pre></td></tr></tbody></table></figure><h2 id="yolov5项目目录结构理解"><a href="#yolov5项目目录结构理解" class="headerlink" title="yolov5项目目录结构理解"></a>yolov5项目目录结构理解</h2><p>参考资料：<a href="https://zhuanlan.zhihu.com/p/669304006">YOLOv5系列(二) 解析项目目录结构(详尽)</a></p><ol><li>models/yolov5_.yaml:模型的配置文件，有n、s、m、l、x版本，逐渐增大（随着架构的增大，训练时间也是逐渐增大）</li><li>data/hyps/hyp.scratch-_.yaml:超参数配置文件，有low、med、high版本，数据增强效果递增</li></ol><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>我用git拉取了yolov5源代码（如下图，右边有点点的就是我修改了的文件）</p><p><img src="/../../images/%E7%9B%AE%E5%BD%95.png" alt="dir"></p><p>就按这个顺序依次说明</p><ol><li>data文件夹：新建了myvoc.yaml文件</li><li>models文件夹：新建了yolo5s_well.yaml文件，复制了yolo5s里的内容，仅修改了类</li><li>VOCData文件夹：存储自己的数据集，两个py文件顾名思义：用来生成训练集和验证集以及将数据集由xml转化为txt供模型训练</li><li>random_search.py:没有用，可忽略</li><li>train.py:修改了其中的一些参数，主要是路径修改<br><img src="/../../images/deepLearning/resume%E5%8F%82%E6%95%B0.png" alt="resume"><br>在九天上我把resume参数default改为true了，这样可以在训练意外中断后，能在之前训练的基础上继续训练，具体做法如下：</li></ol><ul><li>命令行里输入cd yolov5进入文件夹</li><li>输入conda activate pytorch激活我配置好的虚拟环境</li><li>输入nohup python train.py在原来的基础上继续训练，nohup可以把输入放到nohup.txt文件里，保证不在该界面的时候训练依然可以继续（官方文档的ReadMe有写），可以继续训练是因为我改了train.py里的参数，这么输入就可以了</li></ul><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h3 id="yolov5源代码中自带的数据增强方法：需激活"><a href="#yolov5源代码中自带的数据增强方法：需激活" class="headerlink" title="yolov5源代码中自带的数据增强方法：需激活"></a>yolov5源代码中自带的数据增强方法：需激活</h3><p>参考资料：<a href="http://t.csdnimg.cn/9WOJp">YOLOv5-6.x源码分析（七）—- 数据增强之augmentations.py</a> </p><p>此篇博客详细介绍了yolov5中使用的数据增强方法，包括：</p><ol><li>归一化和反规范化</li><li>hsv 色调-饱和度-亮度的图像增强</li><li>直方图均衡化增强</li><li>图像框的平移复制增强</li><li>图片缩放letterbox</li><li>随机透视变换</li><li>cutout</li><li>mixup</li><li>box_candidates</li></ol><p>相关文件：</p><ul><li>utils/augmentations.py</li><li>utils/dataloaders.py</li><li>models/yolov5s_well.py</li><li>data/hyps/hyp.scrath-high.yaml</li></ul><p><strong>使用方法（每个方法启用的数据增强不一样）</strong></p><p><strong>1. 安装步骤</strong></p><ul><li>激活虚拟环境:<code>activate pytorch</code></li><li>下载库:<code>pip install albumentations -i https://pypi.tuna.tsinghua.edu.cn/simple</code></li></ul><p>_补充说明_：</p><p>“Albumentations——强大的数据增强库（图像分类、分割、关键点检测、目标检测）” <br>  “YOLOv5集成Albumentations，添加新的数据增强方法”</p><p>  根据augmentations.py的代码：<br>  </p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">class Albumentations:</span><br><span class="line">  # YOLOv5 Albumentations class (optional, only used if package is installed)</span><br></pre></td></tr></tbody></table></figure><br>  可推断必须安装albumentations库才能启动数据增强<br>  <figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">T = [</span><br><span class="line">        A.RandomResizedCrop(height=size, width=size, scale=(0.8, 1.0), ratio=(0.9, 1.11), p=0.0),</span><br><span class="line">        A.Blur(p=0.01),</span><br><span class="line">        A.MedianBlur(p=0.01),</span><br><span class="line">        A.ToGray(p=0.01),</span><br><span class="line">        A.CLAHE(p=0.01),</span><br><span class="line">        A.RandomBrightnessContrast(p=0.0),</span><br><span class="line">        A.RandomGamma(p=0.0),</span><br><span class="line">        A.ImageCompression(quality_lower=75, p=0.0),</span><br><span class="line">]  # transforms</span><br></pre></td></tr></tbody></table></figure><br>  这部分可以看到具体应用了哪些数据增强方法<p></p><p><strong>2. hyp.scratch.yaml调整（可启用mixup）</strong></p><ul><li>方法一：直接用官方的配置文件，将train.py里–hyp参数的默认使用文件修改一下<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">parser.add_argument("--hyp", type=str, default=ROOT / "data/hyps/hyp.scratch-low.yaml", help="hyperparameters path")</span><br></pre></td></tr></tbody></table></figure>把这里的<code>hyp.scratch-low.yaml</code>换成<code>hyp.scratch-med.yaml</code>或者<code>hyp.scratch-high.yaml</code></li><li>方法二：自己调整hyp.scratch.yaml文件里的参数（不太敢瞎调，或许可以找找文章参考一下别人的？）</li></ul><p> **<em>补充说明：</em>**Mixup是指将两张图片和其标签，按权重进行叠加，生成新的数据集和其所对应的标签。</p><p>Mixup的步骤如下：</p><ol><li>从训练数据中随机选择两个样本，记作样本A和样本B。</li><li>随机选择一个介于0和1之间的权重值λ。</li><li>将样本A和样本B的特征按照权重值λ进行线性组合：mixed_feature = λ * feature_A + (1 - λ) * feature_B。</li><li>将样本A和样本B的标签按照权重值λ进行线性组合：mixed_label = λ * label_A + (1 - λ) * label_B。</li><li>使用mixed_feature作为新的训练样本，使用mixed_label作为对应的标签</li></ol><p><strong>3. 启用cutout</strong></p><p>把utils/dataloaders.py的被注释掉的cutout部分相关代码取消注释(光标处的下两行)<br><img src="/../../images/cutout%E4%BB%A3%E7%A0%81.png" alt="cutout代码"></p><p><strong><em>补充说明：</em></strong><br>Cutout是指随机的将样本中的部分区域cut掉，并且填充0像素值，分类的结果不变</p><h3 id="其他数据增强方法"><a href="#其他数据增强方法" class="headerlink" title="其他数据增强方法"></a>其他数据增强方法</h3><ol><li>Cutmix (参考资料：<a href="http://t.csdnimg.cn/iNl8H">数据增强方法Mixup、Cutout、CutMix、ClassMix</a>)</li></ol><p>  Cutmix综合了Mixup和Cutout的想法，把一张图片上的某个随机矩形区域剪裁到另一张图片上生成新图片。标签的处理和mixUp是一样的，都是按照新样本中两个原样本的比例确定新的混合标签的比例。</p><h2 id="用自己训练的模型预测-推理"><a href="#用自己训练的模型预测-推理" class="headerlink" title="用自己训练的模型预测/推理"></a>用自己训练的模型预测/推理</h2><p>修改detect.py</p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">parse_opt</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">"--weights"</span>, nargs=<span class="string">"+"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=ROOT / <span class="string">"runs/train/exp1/weights/best.pt"</span>, <span class="built_in">help</span>=<span class="string">"model path or triton URL"</span>) <span class="comment">#</span></span><br><span class="line">    parser.add_argument(<span class="string">"--source"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=ROOT / <span class="string">"testData/images"</span>, <span class="built_in">help</span>=<span class="string">"file/dir/URL/glob/screen/0(webcam)"</span>) <span class="comment">#</span></span><br><span class="line">    parser.add_argument(<span class="string">"--data"</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=ROOT / <span class="string">"data/myvoc.yaml"</span>, <span class="built_in">help</span>=<span class="string">"(optional) dataset.yaml path"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--imgsz"</span>, <span class="string">"--img"</span>, <span class="string">"--img-size"</span>, nargs=<span class="string">"+"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=[<span class="number">640</span>], <span class="built_in">help</span>=<span class="string">"inference size h,w"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--conf-thres"</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.25</span>, <span class="built_in">help</span>=<span class="string">"confidence threshold"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--iou-thres"</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.45</span>, <span class="built_in">help</span>=<span class="string">"NMS IoU threshold"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--max-det"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1000</span>, <span class="built_in">help</span>=<span class="string">"maximum detections per image"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--device"</span>, default=<span class="string">""</span>, <span class="built_in">help</span>=<span class="string">"cuda device, i.e. 0 or 0,1,2,3 or cpu"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--view-img"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"show results"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--save-txt"</span>,  default=<span class="literal">True</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"save results to *.txt"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--save-csv"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"save results in CSV format"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--save-conf"</span>,default=<span class="literal">True</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"save confidences in --save-txt labels"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--save-crop"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"save cropped prediction boxes"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--nosave"</span>, action=<span class="string">"store_true"</span>, <span class="built_in">help</span>=<span class="string">"do not save images/videos"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--classes"</span>, nargs=<span class="string">"+"</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, <span class="built_in">help</span>=<span class="string">"filter by class: --classes 0, or --classes 0 2 3"</span>)</span><br></pre></td></tr></tbody></table></figure><p>weights: 修改为自己训练出的模型,即runs/train/exp/weights目录下的.pt文件</p><p>source: 修改为自己想检测的图片集所在的目录</p><p>data: 修改为data目录下自己训练时新建的.yaml(里面有训练集、验证集的文件位置和分类信息)</p><p>save-txt: 因为我需要把检测结果存为txt文件，所以在这里添加了<code>default=True</code></p><p>save-conf:同样因为检测结果需要输出置信度,所以在这里添加了<code>default=True</code></p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>conda命令</title>
      <link href="/2024/03/04/dl/conda/"/>
      <url>/2024/03/04/dl/conda/</url>
      
        <content type="html"><![CDATA[<ul><li>查看已创建的环境。使用命令 <code>conda info --envs</code> 或 <code>conda env list</code> 可以显示所有已创建的环境。</li><li>创建新环境。使用命令 <code>conda create -n env_name python=3.7</code> 可以创建一个名为env_name的新环境，并安装指定版本的Python。</li><li>激活环境。使用命令 <code>conda activate env_name</code> 可以激活一个特定的环境。</li><li>停用环境。使用命令 <code>conda deactivate</code> 可以退出当前激活的环境。</li><li>列出当前环境中的所有软件包。使用命令 <code>conda list</code> 可以显示当前环境中已安装的所有包及其版本。</li><li>查找包信息。使用命令 <code>conda search package_name</code> 可以在Conda的源中搜索指定包的信息。</li><li>安装包。使用命令 <code>conda install package_name</code> 可以在当前活跃环境中安装指定的包。</li><li>更新包。使用命令 <code>conda update package_name</code> 可以更新指定的包到最新版本。</li><li>删除包。使用命令 <code>conda remove package_name</code> 可以从当前环境中删除指定的包。</li><li>删除环境。使用命令 <code>conda remove --name env_name --all</code> 可以删除一个完整的环境及其所有包。</li><li>更新Conda本身。使用命令 <code>conda update conda</code> 可以将Conda更新到最新版本。</li><li>更新<a href="https://www.baidu.com/s?wd=Anaconda&amp;tn=15007414_5_dg&amp;usm=1&amp;ie=utf-8&amp;rsv_pq=befb7000004e24f6&amp;oq=conda%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4&amp;rsv_t=1f0dKfK3WIypazc9aGRul9QdDPMiRPpeAng+JalQIddV0MmCY2I2wqFxm5YfdXd/oIayQg&amp;sa=re_dqa_zy&amp;icon=1">Anaconda<em></em></a>集合包。使用命令 <code>conda update anaconda</code> 可以更新Anaconda的集合包。</li><li>降级Conda版本。使用命令 <code>conda install -nbase conda==4.6.7</code> 可以将Conda降级到指定版本。</li></ul><p>tmux常用命令</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">tmux new -s my_session //新建</span><br><span class="line">tmux rename-session -t 0 my_session //重命名</span><br><span class="line">tmux kill-session -t my_session //删除会话</span><br></pre></td></tr></tbody></table></figure><p>图片顺序</p><p>97568</p><p>411178</p><p>370135</p><p>22759</p><p>vild:391895 522418 184613 318219 554625</p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>卷积神经网络CNN</title>
      <link href="/2024/03/01/dl/cnn/"/>
      <url>/2024/03/01/dl/cnn/</url>
      
        <content type="html"><![CDATA[<h1 id="卷积神经网络构成部分"><a href="#卷积神经网络构成部分" class="headerlink" title="卷积神经网络构成部分"></a>卷积神经网络构成部分</h1><p>$$n_H^{[l]} = [\frac{ {n_H^{[l - 1]} + 2{p^{[l]}} - {f^{[l]}}}}{s^{[l]}}] + 1$$<br>$$n_W^{[l]} = [\frac{ {n_W^{[l - 1]} + 2{p^{[l]}} - {f^{[l]}}}}{s^{[l]}}] + 1$$</p><ul><li>输入：$n_H^{[l-1]} \times n_W^{[l-1]} \times n_C^{[l-1]}$</li><li>权重参数$w^{[l]} : f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$ （$n_c^{[l]}$表示过滤器总数量）</li><li>偏差参数$b^{[l]}: 1 \times 1 \times 1 \times n_c^{[l]}$</li></ul><p>$$\begin{gathered}<br>  {a^{[0]}} = x \<br>  a^{[1]} = g({z^{[1]}}) \<br>  {z^{[1]}} = {w^{[1]}}{a^{[0]}} + {b^{[1]}}\<br>\end{gathered}$$</p><p>${w^{[1]}}{a^{[0]}}$卷积计算，$+{b^{[1]}}$加上偏差</p><p>卷积网络通常有三层：</p><h2 id="卷积层（convolution-layer，简称Conv）"><a href="#卷积层（convolution-layer，简称Conv）" class="headerlink" title="卷积层（convolution layer，简称Conv）"></a>卷积层（convolution layer，简称Conv）</h2><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>提取输入图片的特征，使图像从具体到抽象</p><h3 id="填充-padding"><a href="#填充-padding" class="headerlink" title="填充(padding)"></a>填充(padding)</h3><p>如果有一个$n \times n$的图像，用$f \times f$的过滤器做卷积，那么输出的维度为$(n-f+1)\times (n-f+1)$。这样存在缺点：1. 输出缩小；2. 图像边缘的大部分信息都丢失了</p><p>padding：在图像四周均填充p个像素点，这样输出维度变成了（一般向下取整）$(n+2p-f+1)\times (n+wp-f+1)$</p><ul><li>Valid卷积：不填充</li><li>Same卷积：填充后输入大小和输出大小相同，即$n+2p-f+1=n -&gt; p = (f-1)/2$</li></ul><p>在计算机视觉中，f通常是奇数，因为：</p><ul><li>如果f为偶数，只能使用不对称填充</li><li>奇数维过滤器有中心像素点，便于指出过滤器的位置</li></ul><h3 id="卷积步长-strided-convolutions"><a href="#卷积步长-strided-convolutions" class="headerlink" title="卷积步长(strided convolutions)"></a>卷积步长(strided convolutions)</h3><p>设步长为s，输出大小为$[\frac{n + 2p - f}{s} + 1] \times [\frac{n + 2p - f}{s} + 1]$(向下取整)</p><h3 id="三维卷积"><a href="#三维卷积" class="headerlink" title="三维卷积"></a>三维卷积</h3><p>$$<br>输入图像（n\times n \times {n_c}） * 过滤器(f\times f \times {n_c})\xrightarrow{s = 1,p = 0}输出((n-f+1)\times (n-f+1))<br>$$</p><p>${n_c}$表示通道数，对应位置相乘后全部数相加</p><blockquote><p>对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量</p></blockquote><h2 id="池化层-pooling-layer，简称Pool"><a href="#池化层-pooling-layer，简称Pool" class="headerlink" title="池化层(pooling layer，简称Pool)"></a>池化层(pooling layer，简称Pool)</h2><h3 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h3><ol><li>挑选不受位置干扰的图像信息。</li><li>对特征进行降维，提高后续特征的感受野，也就是让池化后的一个像素对应前面图片中的一个区域。</li><li>因为池化层是不进行反向传播的，而且池化层减少了特征图的变量个数，所以池化层可以减少计算量。</li></ol><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul><li>最大池化：将输入矩阵划分成若干小矩阵，选取每个小矩阵中的最大值</li><li>平均池化：将输入矩阵划分成若干小矩阵，选取每个小矩阵中的平均值</li></ul><p>超参数：f，s</p><h2 id="全连接层-fully-connected-layer，简称FC"><a href="#全连接层-fully-connected-layer，简称FC" class="headerlink" title="全连接层(fully connected layer，简称FC)"></a>全连接层(fully connected layer，简称FC)</h2><p>作用：将前一层得到的矩阵平整化为一个一维向量输出（类似普通的神经层）</p><h1 id="卷积神经网络常见模式"><a href="#卷积神经网络常见模式" class="headerlink" title="卷积神经网络常见模式"></a>卷积神经网络常见模式</h1><p>一个或多个Conv + 一个Pool + 一个或多个Conv + 一个Pool + 几个全连接层 + softmax</p><p>注：</p><ul><li>池化层和最大池化层没有参数</li><li>卷积层的参数相对较少，许多参数存在于神经网络的全连接层</li></ul><p><strong>尽量不要自己设置超参数，而是查看文献中别人采用了哪些超参数</strong></p><p>卷积的优点：</p><ul><li>参数共享 parameter sharing</li><li>稀疏链接 sparsity of connections</li></ul><h1 id="对抗攻防"><a href="#对抗攻防" class="headerlink" title="对抗攻防"></a>对抗攻防</h1><p>概念：训练样本中隐藏了微小噪声，人眼无法识别出来，但是会造成模型产生错误预测。</p><p>解决办法：强化学习，延迟奖励机制</p><p>监督学习试图基于训练数据预测其标签，并正确泛化至未经过训练的数据；但在强化学习中，由于延迟奖励，当前状态下的最优动作往往难以定义，且在智能体与环境交互的过程中，足以代表训练环境的数据往往难以获取。</p><p>对抗攻击：白盒设置、黑盒设置（主要）</p><p>目标：增强模型的鲁棒性</p><h1 id="多智能体"><a href="#多智能体" class="headerlink" title="多智能体"></a>多智能体</h1><p>把博弈论运用在人机对抗、机器与机器的对抗中。这些智能体都拥有自己的优化目标，比如最大化自身收益。</p><p>传统无生命的物理对象通过机器学习等方法正在逐渐被赋予如生命体一样的智能性。多智能体和自动控制的差距缩小</p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>OS概述</title>
      <link href="/2024/03/01/os/os-gai-shu/"/>
      <url>/2024/03/01/os/os-gai-shu/</url>
      
        <content type="html"><![CDATA[<h1 id="操作系统的概念和功能"><a href="#操作系统的概念和功能" class="headerlink" title="操作系统的概念和功能"></a>操作系统的概念和功能</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>负责管理协调硬件、软件等计算机资源的工作，为上层用户、应用服务提供简单易用的服务，是一种系统软件</p><ul><li>UI</li><li>ABI：可以通过相应的编程语言使用这些接口，以操作计算机系统来完成某项特定任务</li><li>API：将程序与操作系统、硬件平台之间紧密协作需要遵守的特定规则保留出来</li><li>ISA：工业标准体系结构</li></ul><h2 id="功能和目标"><a href="#功能和目标" class="headerlink" title="功能和目标"></a>功能和目标</h2><ul><li>资源管理者<ul><li>处理机管理</li><li>存储器管理</li><li>文件管理</li><li>设备管理</li></ul></li><li>向上层提供服务<ul><li>直接给用户使用的<ul><li>GUI（图形用户界面）</li><li>命令接口：联机命令接口（一句一句执行），脱机命令接口（一批一起执行）</li></ul></li><li>给软件/程序员使用的：程序接口（即系统调用）</li></ul></li><li>对硬件机器的扩展：虚拟机</li></ul><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><ul><li>并发性：宏观上同时，微观上交替（区别于并行：同一时刻同时发生）<br>注：单核CPU各程序只能并发执行，多核CPU可以并行执行</li><li>共享性：并发和共享互为存在条件<ul><li>互斥共享方式（如对摄像头设备的共享使用）</li><li>同时共享方式（如对硬盘资源的共享使用）</li></ul></li><li>虚拟性：把一个物理上的实体变为若干个逻辑上的对应物<ul><li>空分复用技术（如虚拟存储技术）</li><li>时分复用技术（如虚拟处理器技术）</li></ul></li><li>异步性</li></ul><h2 id="发展与分类"><a href="#发展与分类" class="headerlink" title="发展与分类"></a>发展与分类</h2><p>冯·诺伊曼体系结构：存储程序式，指令与数据共同存储，通过一个总线访问</p><p>哈佛结构：将指令存储与数据存储分离</p><ol><li><p>手工操作阶段</p></li><li><p>批处理阶段</p><ol><li>单道批处理系统（引入脱机输入输出技术，即输入/输出脱离主机控制）</li><li>多道批处理系统（操作系统开始出现）<br>多道程序设计技术，就是指允许多个程序同时进入内存并运行。即同时把多个程序放入内存中（前提是内存放的下），并允许它们交替在CPU中运行，它们共享系统中的各种硬、软件资源。当一道程序因<strong>I/O请求</strong>而暂停运行时，CPU便立即转去运行另一道程序。<br>引入多道程序设计技术后形成多道批处理系统<br>优点：系统吞吐量大，资源利用率高<br>缺点：平均周转时间长，不能交互（上面所说的<strong>I/O请求</strong>是指请求“读取文件”之类的操作，需要等待直到I/O操作完成，所以不是在和用户交互）</li></ol></li><li><p>分时操作系统</p><p>分时系统：将CPU处理时间分割为多个时间片，将时间片分给不同程序，达到多个程序“同时”运行的效果<br>两种典型分时操作系统：(1) Multics/Unix (1968/1970) (2) IBM VM 360/370</p></li><li><p>实时操作系统：能优先处理紧急任务，一般用于嵌入式</p><ul><li>硬实时系统：必须在绝对严格的限定时间内完成处理</li><li>软实时系统：能接受偶尔违反时间规定</li></ul></li><li><p>网络操作系统</p><p>在传统单机OS上加单独软件层，主要提供联网功能和资源的远程访问，实现多机互联、分布式与嵌入式系统</p></li><li><p>分布式操作系统</p><p>多台机器统一管理形成单一系统，相比于网络操作系统，对用户和应用高度透明</p></li><li><p>个人计算机操作系统</p></li></ol><p>注：</p><table><thead><tr><th>多道批处理</th><th>分时技术</th></tr></thead><tbody><tr><td>不可交互</td><td>可交互</td></tr><tr><td>侧重于作业的批量处理和长处理时间</td><td>强调用户与计算机的实时交互和独占使用</td></tr></tbody></table><h1 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h1><h2 id="程序运行原理"><a href="#程序运行原理" class="headerlink" title="程序运行原理"></a>程序运行原理</h2><p>高级语言编写的代码 –&gt; 机器指令<br>程序运行的过程就是CPU执行指令的过程</p><h2 id="两类程序"><a href="#两类程序" class="headerlink" title="两类程序"></a>两类程序</h2><ul><li>内核程序</li><li>应用程序</li></ul><h2 id="两类指令"><a href="#两类指令" class="headerlink" title="两类指令"></a>两类指令</h2><ul><li>特权指令</li><li>非特权指令</li></ul><h2 id="两种处理器状态"><a href="#两种处理器状态" class="headerlink" title="两种处理器状态"></a>两种处理器状态</h2><ul><li>内核态/核心态/管态</li><li>用户态/目态</li></ul><h2 id="内核"><a href="#内核" class="headerlink" title="内核"></a>内核</h2><p>是操作系统最重要最核心的部分，由很多内核程序组成操作系统内核</p><p>如何改变CPU状态？</p><ul><li>内核态 –&gt; 用户态 ：一条修改PSW（程序状态字，也叫程序状态寄存器）的特权指令</li><li>用户态 –&gt; 内核态 ：由中断引起，硬件自动完成</li></ul><h1 id="中断和异常"><a href="#中断和异常" class="headerlink" title="中断和异常"></a>中断和异常</h1><ol><li><p>作用：<br>让操作系统内核强行夺回CPU控制权，使CPU从用户态变为内核态</p></li><li><p>分类</p><ul><li>异常（内中断）<ul><li>陷阱(trap)</li><li>故障</li><li>终止</li></ul></li><li>中断（外中断）<ul><li>时钟中断</li><li>I/O中断请求</li></ul></li></ul></li><li><p>中断机制的基本实现原理</p><ol><li>检查中断信号<ul><li>异常：CPU在执行指令会检查是否有异常发生</li><li>中断：每个指令周期末尾，CPU都会检查是否有中断信号需要处理</li></ul></li><li>找到相应的中断处理程序：通过中断向量表实现</li></ol></li></ol><h1 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h1><ol><li><p>定义：操作系统对应用程序/程序员提供的接口</p></li><li><p>系统调用和库函数的关系：有的库函数是对系统调用的进一步封装，有的库函数没有使用系统调用</p></li><li><p>什么功能要用系统调用实现：设备管理、文件管理、进程控制、进程通信、内存管理</p></li><li><p>系统调用的过程：</p><ol><li>传参</li><li>陷入(trap)指令</li><li>由操作系统内核程序处理系统调用请求</li><li>返回应用程序</li></ol></li></ol><h1 id="OS体系结构"><a href="#OS体系结构" class="headerlink" title="OS体系结构"></a>OS体系结构</h1><table><thead><tr><th></th><th>大内核</th><th>微内核</th></tr></thead><tbody><tr><td>特点</td><td>将操作系统的主要功能模块都作为系统内核，运行在核心态</td><td>只把最基本的功能保留在内核</td></tr><tr><td>例子</td><td>如Linux、UNIX</td><td>如Windows NT</td></tr></tbody></table><h1 id="系统引导"><a href="#系统引导" class="headerlink" title="系统引导"></a>系统引导</h1><p><img src="D:/mynotes/images/OS/系统引导.jpg" alt="系统引导过程图"></p><p>BootLoader(引导加载程序)：系统加电后运行的第一段软件代码，是在OS内核运行之前的一小段程序。（也就是说存放在图中PBR所在区域）</p><ul><li>boot：初始化嵌入式系统硬件使之运行起来，至少是部分运行起来。</li><li>load：将OS映像加载到内存中，并跳转到OS的代码运行</li></ul><table><thead><tr><th>处理器类型</th><th>常用bootloader</th></tr></thead><tbody><tr><td>MIPS处理器（大多用于嵌入式系统）</td><td>U-boot</td></tr><tr><td>x86处理器</td><td>LILO、GRUB</td></tr></tbody></table><p><strong>BootLoader的实现严重依赖于具体硬件</strong></p><h2 id="计算机的启动过程（MIPS）"><a href="#计算机的启动过程（MIPS）" class="headerlink" title="计算机的启动过程（MIPS）"></a>计算机的启动过程（MIPS）</h2><h3 id="U-boot"><a href="#U-boot" class="headerlink" title="U-boot"></a>U-boot</h3><p>大多数BootLoader都分为stage1和stage2两大部分，U-boot也不例外</p><ul><li>stage1：依赖于cpu体系结构的代码（如设备初始化代码等）通常都放在stage1且可以用汇编语言来实现</li><li>stage2：通常用C语言来实现，这样可以实现复杂的功能，而且有更好的可读性和移植性</li></ul><h3 id="MIPS基本地址空间"><a href="#MIPS基本地址空间" class="headerlink" title="MIPS基本地址空间"></a>MIPS基本地址空间</h3><p><img src="/../../images/OS/MIPS%E5%9F%BA%E6%9C%AC%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4.png" alt="MIPS基本地址空间"></p><ul><li>kuseg：用户态可用的地址,在有MMU的机器里,这些地址将一概被MMU作转换,除非MMU的设置被建立好,否则这2G的地址是不可用的</li><li>kseg0：将他们的最高位清零,即可映射到物理地址段512M(0x00000000 – 0x1fffffff).这种映射关系很简单,通常称之为”非转换的”地址区域,几乎全部对这段地址的存取都会通过cache,因此cache设置好之前,不能随便使用这段地址.</li><li>kseg1: 将这些地址的高三位清零可映射到相应的物理地址上,与kseg0映射的物理地址一样,但kseg1是非cache存取的. <strong>kseg1是唯一在系统重启时能正常工作的地址空间</strong></li><li>kseg2: 这块区域只能在核心态下使用并且要经过MMU的转换. 在MMU设置好之前,不要存取该区域. 除非在写一个真正的操作系统,否则没有理由用kseg2. 有时会看到该区域被分为kseg2和kseg3,意在强调低半部分(kseg2)可供运行在管理态的程序使用.</li></ul><h3 id="MIPS启动过程"><a href="#MIPS启动过程" class="headerlink" title="MIPS启动过程"></a>MIPS启动过程</h3><ol><li><p>start.S: 从_start开始执行</p><ul><li>初始化中断向量</li><li>寄存器清零</li><li>配置寄存器的CP0_STATUS, 设置所使用的协处理器，中断以及CPU运行级别（核心级）</li><li>配置gp寄存器</li></ul></li><li><p>lowlevel_init.S: 从lowlevel_init开始执行，工作频率配置（e.g: CPU主频、总线、DDR工作频率）</p></li><li><p>cache.S</p><ul><li>mips_cache_rest: 对cache进行初始化</li><li>mips_cache_lock: 设置堆栈</li></ul><p>由于此时ddr ram并没有配置好，而如果直接调用c语言的函数必须完成栈的设置，栈必定要在ram中。所以，只有先把一部分cache拿来当ram用。做法就是把一部分cache配置为栈的地址并锁定。这样，当读写栈的内存空间时，只会访问cache，而不会访问真的ram地址了</p></li></ol><p>第一阶段：三个汇编代码</p><hr><ol start="4"><li><p>board.c: </p><p>board_init_f</p><ul><li>time_init </li><li>env_init 环境变量初始化</li><li>init_baudrate 串口速率</li><li>serial_init  串口初始化</li><li>console_init_f 配置控制台</li><li>display_banner  显示U-boot启动信息，版本号等</li><li>init_func_ram 初始化内存，配置DDR controller</li></ul><p>board_init_r: 初始化flash、PCI以及外设（e.g: 网口），进入命令行直接启动Linux Kernel</p></li></ol><h2 id="MIPS下Linux系统引导过程"><a href="#MIPS下Linux系统引导过程" class="headerlink" title="MIPS下Linux系统引导过程"></a>MIPS下Linux系统引导过程</h2><p>BootLoader将Linux内核映像拷贝到RAM中</p><p>/arch/mips/kernel/head.s  kernel_entry(): 初始化内核堆栈段，为创建系统中的第一个进程进行准备，接着用另一段循环将内核映像的未初始化数据段清零，最后跳转到start_kernel()</p><p>/init/main.c  start_kernel(): 初始化硬件平台相关代码</p><h2 id="计算机的启动过程（x86"><a href="#计算机的启动过程（x86" class="headerlink" title="计算机的启动过程（x86)"></a>计算机的启动过程（x86)</h2><p><img src="/../../images/OS/x86%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B.png" alt="x86启动过程"></p><ol><li>加载BIOS   BIOS v.s. UEFI(统一可扩展固件接口)</li><li>读取MBR(主引导记录，即磁盘上第0磁头第0磁道第一个扇区)：启动代码及数据、分区表、幻数</li></ol><h2 id="x86下Linux系统引导过程"><a href="#x86下Linux系统引导过程" class="headerlink" title="x86下Linux系统引导过程"></a>x86下Linux系统引导过程</h2><ol start="3"><li>Boot Loader</li><li>加载内核</li><li>init进程执行</li><li>用户层init依据initab文件来设定运行等级</li><li>init进程执行rc.sysinit</li><li>启动内核模块</li><li>执行不同运行级别的脚本程序</li><li>执行/etc/rc.d/rc.local(留给用户个性化的地方)</li><li>执行/bin/login程序，进入登录状态</li></ol><h1 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h1><table><thead><tr><th></th><th>第一类虚拟机</th><th>第二类虚拟机</th></tr></thead><tbody><tr><td>对物理资源的控制</td><td>直接运行在硬件之上，能直接控制和分配物理资源</td><td>运行在Host OS上，依赖于Host OS为其分配物理资源</td></tr><tr><td>资源分配方式</td><td>在安装Guest OS时，VMM要在原本的硬件上自行分配存储空间，类似于“外核”的分配方式，分配未经抽象的物理硬件</td><td>Guest OS拥有自己的虚拟磁盘，该盘实际上是Host OS文件系统中的一个大文件，Guest OS分配到的内存是虚拟内存</td></tr><tr><td>性能</td><td>好</td><td>差（需要Host OS作为中介）</td></tr><tr><td>可支持的虚拟机数量</td><td>更多（不需要和Host OS竞争资源，相同的硬件资源可以支持更多的虚拟机</td><td>更少（Host OS本身需要使用物理资源，Host OS上运行的其他进程也需要物理资源）</td></tr><tr><td>可迁移性</td><td>差</td><td>好</td></tr><tr><td>运行模式</td><td>运行在最高特权级（Ring 0），可以执行最高特权的指令</td><td>运行在用户态，部分运行在内核态， Guest OS发出的系统调用会被VMM截获，并转化为VMM对Host OS的系统调用</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> OS </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>结构化机器学习项目</title>
      <link href="/2024/02/24/dl/jie-gou-hua-ji-qi-xue-xi-xiang-mu/"/>
      <url>/2024/02/24/dl/jie-gou-hua-ji-qi-xue-xi-xiang-mu/</url>
      
        <content type="html"><![CDATA[<h1 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h1><p>正交化是指每个参数只控制一个维度的效果</p><h1 id="单一数字评估标准"><a href="#单一数字评估标准" class="headerlink" title="单一数字评估标准"></a>单一数字评估标准</h1><p>评估分类器的一个合理方式：观察它的查准率(Precision)和查全率（Recall）</p><ul><li>查准率：衡量预测的准确性。如：分类器标记为猫的例子中，有多少真的是猫</li><li>查全率：衡量发现所有正例的能力。如：对于所有真猫图片，分类器正确识别出了多少百分比</li></ul><table><thead><tr><th></th><th>预测为正例</th><th>预测为负例</th></tr></thead><tbody><tr><td>实际为正例</td><td>TP</td><td>FP</td></tr><tr><td>实际为负例</td><td>FN</td><td>TN</td></tr></tbody></table><p>$$<br>Precision = \frac{TP}{TP + FP} \<br>Recall = \frac{TP}{TP + FN}<br>$$</p><p>两种指标都要顾及到，比较方法是F1分数，公式为$\frac{2}{\frac{1}{P}+\frac{1}{R}}$(越大越好)</p><h1 id="mAP-mean-Average-Precision-评价指标"><a href="#mAP-mean-Average-Precision-评价指标" class="headerlink" title="mAP(mean Average Precision)评价指标"></a>mAP(mean Average Precision)评价指标</h1><p>mAP是目标检测模型中常用的评价指标。</p><p>首先介绍一下交并比IoU(Intersection over union)的概念，它是用来度量预测边框与实际边框的重叠程度的。<br>$$<br>IoU = \frac{预测边框与实际边框的交}{二者的并}<br>$$</p><p>tips:在训练时，常常依据候选区域和标定区域 的IoU值来确定正负样本。</p><ol><li><p>设置IoU阈值：在计算mAP之前，我们会先设置一个IoU（如0.5）阈值来判断预测是真阳性还是假阳性。</p></li><li><p>判断预测框的实际类别：对于一个预测框，与预测框所在图片里的所有标签框作比较，如果二者的IoU大于阈值，则认为该预测框为“实际为正例”，否则为“实际为负例”。</p><p>如果是多分类任务，则找与预测框预测的类别相同的所有标签框比较</p></li><li><p>判断预测框的预测类别：设置不同的置信度阈值，对于每一个置信度阈值，若预测框的置信度大于该阈值，说明预测结果可信，预测结果为正例；小于该阈值，说明预测结构不可信，预测结果为负例。</p></li><li><p>一般会将预测框先按置信度降序排序，所以可以按把每个预测框的置信度都取一遍来获得不同的置信度阈值。对于相同的Recall，Precision取最高的一个值，然后画出P-R曲线。</p></li><li><p>P-R曲线围成的面积就是平均精度（AP）。</p></li><li><p>把每个类别的AP都计算出来之后求它们的平均值，即可得到mAP。</p></li></ol><p>为何要使用AP/mAP呢？因为我们通常希望模型的Precision和Recall值都很高，所有就需要把两个因素综合考虑。一种方法就是上节提到的F1分数，一种就是计算P-R曲线下的面积，越接近1越好。</p><h1 id="满足和优化指标"><a href="#满足和优化指标" class="headerlink" title="满足和优化指标"></a>满足和优化指标</h1><ul><li>优化指标optimizing metric:如准确率，越大越好</li><li>满足指标satisficing metric:如运行时间，只需小于100ms就算足够好，达到之后不在乎这个指标有多好</li></ul><p>一般来说，如果要考虑n个指标，有时候选择其中一个做为优化指标是合理的，然后剩下都是满足指标，只需满足设定的阈值就好。</p><h1 id="贝叶斯最优错误率：理论上限"><a href="#贝叶斯最优错误率：理论上限" class="headerlink" title="贝叶斯最优错误率：理论上限"></a>贝叶斯最优错误率：理论上限</h1><p>对于计算机视觉任务而言，常用人类水平的错误率估计或代替贝叶斯错误率/贝叶斯最优错误率</p><p>$$可避免误差=贝叶斯错误率（的估计）-训练错误率$$<br>如果想要提升机器学习系统的性能，建议关注训练错误率和贝叶斯错误率的距离（可避免误差）以及开发错误率和训练错误率的距离（方差），然后针对不同的问题采用不同的策略</p><p>解决可避免误差的常用方法：</p><ul><li>Train Bigger model</li><li>Train longer/better optimization algorithms:momentum, RMSprop, Adam</li><li>NN archtecture / hyperparameters search</li></ul><p>解决方差的常用方法:</p><ul><li>more data</li><li>rugularization: L2 dropout, data augmentation</li><li>NN archtecture / hyperparameters search</li></ul><h1 id="进行错误分析-error-analysis"><a href="#进行错误分析-error-analysis" class="headerlink" title="进行错误分析(error analysis)"></a>进行错误分析(error analysis)</h1><p>找一组错误样本（可能是开发集或者测试集的），观察错误标记的样本，看看假阳性和假阴性，统计属于不同错误类型的错误数量。如此，可以知道哪些问题有改进的潜力</p><h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>任务A –&gt; 任务B</p><p>预训练    微调</p><p>注：</p><ul><li>任务A与任务B有同样的输入时(例如都是图像、都是音频)，迁移学习是有意义的</li><li>任务A比任务B的数据多得多时，迁移学习意义更大</li></ul><h1 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h1><p>同时开始学习，试图让单个神经网络做几件事情，并且希望这里每个任务都能帮到其他所有任务</p><h1 id="端到端的深度学习"><a href="#端到端的深度学习" class="headerlink" title="端到端的深度学习"></a>端到端的深度学习</h1><p>$$X \xrightarrow{直接映射} Y$$<br>注：需要足够多的数据</p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>改善深层神经网络</title>
      <link href="/2024/02/17/dl/gai-shan-shen-ceng-shen-jing-wang-luo/"/>
      <url>/2024/02/17/dl/gai-shan-shen-ceng-shen-jing-wang-luo/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习的实践方面"><a href="#深度学习的实践方面" class="headerlink" title="深度学习的实践方面"></a>深度学习的实践方面</h1><h2 id="训练集-验证集-测试集-Train-Dev-Test-set"><a href="#训练集-验证集-测试集-Train-Dev-Test-set" class="headerlink" title="训练集/验证集/测试集(Train/Dev/Test set)"></a>训练集/验证集/测试集(Train/Dev/Test set)</h2><table><thead><tr><th align="left"></th><th>Train</th><th>Dev</th><th>Test</th></tr></thead><tbody><tr><td align="left">小数量</td><td>70%</td><td></td><td>30%</td></tr><tr><td align="left">or</td><td>60%</td><td>20%</td><td>20%</td></tr><tr><td align="left">百万级数据样本</td><td>98%</td><td>1%</td><td>1%</td></tr><tr><td align="left">超过百万</td><td>99.5%</td><td>0.25%</td><td>0.25%</td></tr><tr><td align="left">or</td><td>99.5%</td><td>0.4%</td><td>0.1%</td></tr></tbody></table><h2 id="偏见与方差-bias-varience"><a href="#偏见与方差-bias-varience" class="headerlink" title="偏见与方差(bias&amp;varience)"></a>偏见与方差(bias&amp;varience)</h2><table><thead><tr><th align="left"></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td align="left">训练集错误率</td><td>1%</td><td>15%</td><td>15%</td><td>0.5%</td></tr><tr><td align="left">开发集错误率</td><td>11%</td><td>16%</td><td>30%</td><td>1%</td></tr><tr><td align="left"></td><td>high varience</td><td>high bias</td><td>high bias &amp; varience</td><td>low bias &amp; varience</td></tr><tr><td align="left"></td><td>过拟合overfitting</td><td>欠拟合unfitting</td><td>一些地方欠拟合，一些地方过拟合</td><td></td></tr></tbody></table><p>以上分析基于：human=0% (optimal/base error 理想误差/基误差)</p><p>解决办法</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">graph TD;</span><br><span class="line">    A[high bias]--&gt;B{判断};</span><br><span class="line">    B--&gt;|no|C[high varience];</span><br><span class="line">    B--&gt;|yes|D[bigger network\n train longer\n NN architecture search];</span><br><span class="line">    D--&gt;A;</span><br><span class="line">    C--&gt;|no|E[Done];</span><br><span class="line">    C--&gt;|yes|F[more data\n regularization正则化\n NN architecture search];</span><br><span class="line">    F--&gt;A;</span><br></pre></td></tr></tbody></table></figure><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>对较大值的系数进行惩罚</p><h3 id="L2-regularization"><a href="#L2-regularization" class="headerlink" title="L2 regularization"></a>L2 regularization</h3><p>逻辑回归中：$J\left( {w,b} \right) = \frac{1}{m}\sum\limits_{i = 1}^m {L({\hat{y}^{(i)}},\mathop y\nolimits^{(i)} )} + \frac{\lambda}{2m}|w|^2$      $|w|^2=\sum\limits_{j=1}^{n_x}w_j^2=w^Tw$</p><p>神经网络中：$J\left( {w^{[1]},b^{[1]}, …, w^{[l]},b^{[l]}} \right) = \frac{1}{m}\sum\limits_{i = 1}^m {L({\hat{y}^{(i)}},\mathop y\nolimits^{(i)} )} + \frac{\lambda}{2m}|w^{[l]}|^2$      $|w^{[l]}|^2=\sum\limits_{i=1}^{n^{[l-1]}}\sum\limits_{j=1}^{n^{[l]}}{w_{ij}^{[l]}}^2$</p><p>用backprop计算dw的值：${\text{w}}^{[l]}: = {\text{w}}^{[l]} - \partial (dw + \frac{\lambda }{m}{w^{[l]}}) = (1 - \frac{\partial \lambda }{m}){w^{[l]}} - \partial dw$</p><p>正则化减少过拟合的原因：$\lambda$增大，新的${\text{w}}^{[l]}$减小，隐藏单元的影响减小，存在一个$\lambda$的中间值使得接近”just right”的中间状态</p><h3 id="随机失活正则化（Dropout-regularization-设置消除神经网络中节点的概率"><a href="#随机失活正则化（Dropout-regularization-设置消除神经网络中节点的概率" class="headerlink" title="随机失活正则化（Dropout regularization): 设置消除神经网络中节点的概率"></a>随机失活正则化（Dropout regularization): 设置消除神经网络中节点的概率</h3><p>在神经网络的训练过程中，随机地丢弃（屏蔽）一部分神经元的输出，即将它们的权重置为零。通过这种方式，可以防止神经网络过度依赖某些特征，从而提高模型的泛化能力和鲁棒性。Dropout技术通常应用于神经网络的隐藏层上，并按照一定的概率p随机失活部分节点，在后向传播时进行相应的参数更新</p><p>最常用的实施方法：反向随机失活</p><p>e.g.: 以一个三层随即网络为例</p><ol><li><p>定义向量d</p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">d3=np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep-prob</span><br></pre></td></tr></tbody></table></figure><p>keep-prop表示保留某个隐藏单元的概率</p></li></ol><h2 id="归一化输入（normalizing-inputs）-可以加速训练方法"><a href="#归一化输入（normalizing-inputs）-可以加速训练方法" class="headerlink" title="归一化输入（normalizing inputs）:可以加速训练方法"></a>归一化输入（normalizing inputs）:可以加速训练方法</h2><p>将所有图像的像素值缩放到一个特定的范围（通常是 [0, 1] 或 [-1, 1]）</p><p>step：</p><ol><li>零均值化：${M_j} = \frac{1}{m}\sum\limits_{i = 1}^m {x_j^{(i)}}$     $x_j:=x_j-\mu _j$</li><li>归一化方差：${\sigma ^2} = \frac{1}{m}{\sum\limits_{i = 1}^m {({x^{(i)}})} ^2}$       ${x_j}: = \frac{x_j}{\sigma }$</li></ol><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p>梯度爆炸：各层权重w都大于1，层数很大</p><p>梯度消失：各层权重w都小于1，层数很大</p><h2 id="神经网络的权重初始化"><a href="#神经网络的权重初始化" class="headerlink" title="神经网络的权重初始化"></a>神经网络的权重初始化</h2><p>${w^{[l]}} = np.random.randn({n^{[l]}},{n^{[l - 1]}}.*np.sqrt(1/{n^{[l - 1]}}))$</p><p>$n^{[l - 1]}$表示第l-1层神经元的数量；如果用的Relu激活函数，sqrt中的1改为2更好</p><h2 id="梯度检验：确保backprop正确实施"><a href="#梯度检验：确保backprop正确实施" class="headerlink" title="梯度检验：确保backprop正确实施"></a>梯度检验：确保backprop正确实施</h2><p>${\theta _{\text{i}}} = (w_i^{[1]},b_i^{[1]},…,w_i^{[l]},b_i^{[l]})$连接成一个超大向量</p><p>$d{\theta _{approx[i]}} = \frac{J({\theta _1},{\theta _2},…,{\theta _i} + \epsilon ,..) - J({\theta _1},{\theta _2},…,{\theta _i} - \epsilon ,..)}{2\epsilon }$验证是否逼近$d{\theta _{[i]}} = \frac{\partial J}{\partial {\theta _i}}$</p><p>验证方法：计算两个向量的欧式距离$\frac{||d{\theta _{approx[i]} - d{\theta _{[i]}}||{^2}}}{||d{\theta _{approx[i]}}||{^2} + ||d{\theta _{[i]}}||{^2}}$,$\leqslant {10^{ - 7}}$没问题，$\approx {10^{ - 5}}$可能有问题/bug，$\approx {10^{ - 3}}$有问题</p><p>注：</p><ol><li>不要在训练中使用grad check，它只用于调试</li><li>如果算法的梯度检验失败，要检查所有项，并试着找出bug，注意$\theta$的各项与b、w的各项都是一一对应的</li><li>在实施梯度检验时，如果使用正则化，请注意正则项</li><li>梯度检验不与dropout同时使用</li><li>现实中不会在随机初始化时运行梯度检验</li></ol><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="Mini-batch梯度下降法"><a href="#Mini-batch梯度下降法" class="headerlink" title="Mini-batch梯度下降法"></a>Mini-batch梯度下降法</h2><p>将整个训练样本分成若干份（mini-batch），然后迭代。</p><p>假设训练样本m个，分成n份，一次遍历训练集，可以做n次梯度梯度下降</p><ul><li>m&lt;2000时，直接batch gradient descent(就是之前讲的常规梯度下降法)</li><li>m很大时，建议分成$2^n$倍（跟电脑内存存储方式有关）</li></ul><h2 id="指数加权平均（Exponentially-weighted-averages）-统计学中称为指数加权移动平均"><a href="#指数加权平均（Exponentially-weighted-averages）-统计学中称为指数加权移动平均" class="headerlink" title="指数加权平均（Exponentially weighted averages）(统计学中称为指数加权移动平均)"></a>指数加权平均（Exponentially weighted averages）(统计学中称为指数加权移动平均)</h2><p>关键公式：${v_t} = \beta {v_{t - 1}} + (1 - \beta ){\theta _t}$</p><p>其中$\theta_t$是实际值，整个式子可以看成$v_t$是$\frac{1}{1-\beta}$个数据的平均值</p><p>指数加权平均的偏差修正（bias correction）在预估初期，不用$v_t$，而用$\frac{v_t}{1-\beta^t}$</p><h2 id="动量梯度下降法（Gradient-descent-with-momentum）"><a href="#动量梯度下降法（Gradient-descent-with-momentum）" class="headerlink" title="动量梯度下降法（Gradient descent  with momentum）"></a>动量梯度下降法（Gradient descent  with momentum）</h2><p>基本想法：计算梯度的指数加权平均数，并利用该梯度更新权值</p><p>$$\left{ \begin{gathered}<br>  {v_{dw}} = \beta {v_{dw}} + (1 - \beta )dw \hfill \<br>  {v_{db}} = \beta {v_{db}} + (1 - \beta )db \hfill \<br>\end{gathered}  \right.$$</p><p>$$\left{ \begin{gathered}<br>  w: = w - \partial {v_{dw}} \hfill \<br>  b: = b - \partial {v_{db}} \hfill \<br>\end{gathered}  \right.$$</p><h2 id="RMSprop算法（RMS：root-mean-square均方根，标准差）"><a href="#RMSprop算法（RMS：root-mean-square均方根，标准差）" class="headerlink" title="RMSprop算法（RMS：root  mean  square均方根，标准差）"></a>RMSprop算法（RMS：root  mean  square均方根，标准差）</h2><p>$$\left{ \begin{gathered}<br>  {S_{dw}} = \beta {S_{dw}} + (1 - \beta )(dw)^2 \hfill \<br>  {S_{db}} = \beta {S_{db}} + (1 - \beta )(db)^2 \hfill \<br>\end{gathered}  \right.$$</p><p>$$\left{ \begin{gathered}<br>  w: = w - \partial \frac{dw}{\sqrt {S_{dw}} } \hfill \<br>  b: = b - \partial \frac{db}{\sqrt {S_{db}} } \hfill \<br>\end{gathered}  \right.$$</p><p>原理：<br><img src="/../../images/deepLearning/RMSprop%E7%AE%97%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.jpg" alt="RMSprop算法示意图"></p><p>我们希望在横轴（w方向）学习速度快，纵轴（b方向）减缓摆动，所有有了$S_{dw}$,、$S_{db}$，w要除以一个较小的数（$S_{dw}$相对较小），b要除以一个相对较大的数（$S_{db}$较大）。</p><h2 id="Adam优化算法（Momentum和RMSprop的结合）"><a href="#Adam优化算法（Momentum和RMSprop的结合）" class="headerlink" title="Adam优化算法（Momentum和RMSprop的结合）"></a>Adam优化算法（Momentum和RMSprop的结合）</h2><p>步骤：</p><ol><li><p>初始化：$v_{dw}=0$ $S_{dw}=0$ $v_{db}=0$ $S_{db}=0$</p></li><li><p>计算Momentum指数加权平均数：</p><p>$$\left{ \begin{gathered}<br>  {v_{dw}} = \beta_1 {v_{dw}} + (1 - \beta_1 )dw \hfill \<br>  {v_{db}} = \beta_1 {v_{db}} + (1 - \beta_1 )db \hfill \<br>\end{gathered}  \right.$$</p></li><li><p>使用RMSprop进行更新</p><p>$$\left{ \begin{gathered}<br>  {S_{dw}} = \beta_2 {S_{dw}} + (1 - \beta_2 )(dw)^2 \hfill \<br>  {S_{db}} = \beta_2 {S_{db}} + (1 - \beta_2 )(db)^2 \hfill \<br>\end{gathered}  \right.$$</p></li><li><p>使用Adam算法，一般要计算偏差修正</p><p>$v_{dw}^{corrected} = \frac{v_{dw}}{1 - \beta _1^t}$</p><p>$v_{db}^{corrected} = \frac{v_{db}}{1 - \beta _1^t}$</p><p>$S_{dw}^{corrected} = \frac{S_{dw}}{1 - \beta _2^t}$ </p><p>$S_{db}^{corrected} = \frac{S_{db}}{1 - \beta _2^t}$</p></li><li><p>更新权重</p><p>$$\left{ \begin{gathered}<br>  w: = w - \partial \frac{v_{dw}^{corrected}}{\sqrt {S_{dw}^{corrected}} + \epsilon } \hfill \<br>  b: = b - \partial \frac{v_{db}^{corrected}}{\sqrt {S_{db}^{corrected}} + \epsilon } \hfill \<br>\end{gathered}  \right.$$</p></li></ol><h2 id="学习率衰减（learning-rate-decay）"><a href="#学习率衰减（learning-rate-decay）" class="headerlink" title="学习率衰减（learning rate decay）"></a>学习率衰减（learning rate decay）</h2><p>加快学习算法的一个办法是随时间慢慢减少学习率</p><p>$\partial  = \frac{1}{1 + decayrate \cdot epoch - num}{\partial _0}$，epoch-num是训练的代数</p><p>$\partial  = {0.95^{epoch - num}}{\partial _0}$</p><p>$\partial  = \frac{k}{\sqrt {epoch - num} }{\partial _0}$</p><p>$\partial  = \frac{k}{\sqrt t }{\partial _0}$</p><h2 id="局部最优问题"><a href="#局部最优问题" class="headerlink" title="局部最优问题"></a>局部最优问题</h2><p><img src="/../../images/deepLearning/%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E8%A7%A3%E9%87%8A.jpg" alt="局部最优问题"></p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/02/10/dl/shen-jing-wang-luo-ji-chu-zhi-shi/"/>
      <url>/2024/02/10/dl/shen-jing-wang-luo-ji-chu-zhi-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="概念区分"><a href="#概念区分" class="headerlink" title="概念区分"></a>概念区分</h1><p>机器学习&gt;神经网络&gt;深度学习=深度神经网络</p><p>机器学习</p><ul><li>神经网络<ul><li>浅层神经网络</li><li>深层神经网络/深度学习：一种机器学习算法，“深度”指的是神经网络中的层次很深。</li></ul></li><li>逻辑回归：没有隐含层的神经网络</li><li>……</li></ul><h1 id="标注习惯"><a href="#标注习惯" class="headerlink" title="标注习惯"></a>标注习惯</h1><ul><li>上角小括号：训练集里的值</li><li>上角中括号：神经网络的层数</li><li>大括号：不同的mini-batch</li></ul><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h2><p>$z=w^{T}x+b$</p><p>激活函数：$a = \hat{y} = \sigma (z)$</p><p>损失函数(衡量在单个训练样本上的表现)：</p><p>$L(\hat y, y) = -ylog\hat y - (1-y)log(1-\hat{y})$</p><p>成本函数(衡量在全体训练样本上的表现)：$J\left( {w,b} \right) = \frac{1}{m}\sum\limits_{i = 1}^m {L({\hat{y}^{(i)}},\mathop y\nolimits^{(i)} )}$</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>梯度下降算法：</p><p>$$da = \frac{\partial L}{\partial a} =  - \frac{y}{a} + \frac{1 - y}{1 - a}$$</p><p>$$dz = \frac{\partial L}{\partial z} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} = ( - \frac{y}{a} + \frac{1 - y}{1 - a}) \cdot a \cdot (1 - a) = a - y$$</p><p>$$\left{ {\begin{array}{l}<br>{d\mathop w\nolimits_1  = \frac{\partial L}{\partial \mathop w\nolimits_1 } = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial \mathop w\nolimits_1 } = \mathop x\nolimits_1  \cdot (a - y)}\<br>{d\mathop w\nolimits_2  = \frac{\partial L}{\partial \mathop w\nolimits_2 } = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial \mathop w\nolimits_2 } = \mathop x\nolimits_2  \cdot (a - y)}\<br>{db = \frac{\partial L}{\partial b} = a - y}<br>\end{array}} \right.$$</p><p>更新：<br>$$\left{ {\begin{array}{l}<br>{\mathop w\nolimits_1 : = \mathop w\nolimits_1  - \partial  \cdot d\mathop w\nolimits_1 }\<br>{\mathop w\nolimits_2 : = \mathop w\nolimits_2  - \partial  \cdot d\mathop w\nolimits_2 }\<br>{b: = b - \partial  \cdot db}<br>\end{array}} \right.$$</p><p>m个样本的梯度下降：<br>$$\left{ {\begin{array}{l}<br>{d{\rm{ }}{w_1} = \frac{\rm{1}}{\rm{m}}\sum\limits_{i = 1}^m {x_1^{(i)} \cdot ({a^{(i)}} - {y^{(i)}})} {\rm{ }}}\<br>{d{\rm{ }}{w_2} = \frac{\rm{1}}{\rm{m}}\sum\limits_{i = 1}^m {x_2^{(i)} \cdot ({a^{(i)}} - {y^{(i)}})} {\rm{ }}}\<br>{db = \frac{\partial L}{\partial b} = \frac{\rm{1}}{\rm{m}}\sum\limits_{i = 1}^m {({a^{(i)}} - {y^{(i)}})} }<br>\end{array}} \right.$$</p><p>深层网络的向前传播：<br>$$z^{[l]}=w^{[l]}a^{[l]-1}+b^{[l]}$$<br>$$a^{[l]} =  \sigma (z^{[l]})$$</p><h2 id="常见激活函数"><a href="#常见激活函数" class="headerlink" title="常见激活函数"></a>常见激活函数</h2><h3 id="sigmiod函数"><a href="#sigmiod函数" class="headerlink" title="sigmiod函数"></a>sigmiod函数</h3><p>$$<br>f(x) = \frac{1}{1 + e^{-x}}<br>$$</p><p>使用范围：常用于二分类问题。被广泛地使用在输出单元/最后一层上，在隐藏层使用较少。</p><p>输出特点：值在0和1之间。</p><p>具有平滑性、可导性、单调性等良好的性质，使用sigmod激活函数的神经网络可以学习到非线性的特征，提高模型的拟合能力。</p><p>存在输出值偏置的问题，当输入值很大或很小时，函数的斜率会趋近于0，导致梯度消失，使得神经网络难以进行有效的学习。</p><h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><p>$$<br>S_i = \frac{e^i}{\sum_j e^j}<br>$$<br>使用范围：多分类问题。用在输出单元。</p><p>输出特点：每一项的区间范围是(0, 1)，所有项相加的和为1。</p><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>$$<br>tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}<br>$$</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>$$<br>ReLU(x) = max(x, 0)<br>$$<br>使用范围：当搭建的神经网络层数较多时，此时不宜选择sigmoid、tanh函数，应选用ReLU。</p><p>ReLU函数的求导表现得很好，要么让参数消失，要么让参数通过。ReLU减轻了神经网络的梯度消失问题。</p><h2 id="常见损失函数"><a href="#常见损失函数" class="headerlink" title="常见损失函数"></a>常见损失函数</h2><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>$$<br>L(\hat y, y) = -ylog\hat y - (1-y)log(1-\hat{y})<br>$$</p><h3 id="均方差"><a href="#均方差" class="headerlink" title="均方差"></a>均方差</h3><p>$$<br>MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> deepLearning </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
